import os
from io import BytesIO  # ç§»åˆ°æœ€ä¸Šé¢
import logging
from dotenv import load_dotenv
from utils.path_helper import PathHelper
import re
import asyncio
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page
from typing import Optional, List, Dict, Literal, Any
import random
from PIL import Image
import pytesseract
import shutil
import requests
import json
import fitz  # PyMuPDF for PDF
import tempfile
from pptx import Presentation
from prompt_manager import GoogleSheetPromptManager

# Load environment variables
load_dotenv(override=True)

# Pytesseract Path - æ”¯æ´å¤šç¨®å¯èƒ½çš„è·¯å¾‘
tesseract_cmd = os.getenv('TESSERACT_CMD') or os.getenv('TESSERACT') 
tesseract_found = False

if tesseract_cmd:
    # ä½¿ç”¨ç’°å¢ƒè®Šæ•¸æŒ‡å®šçš„è·¯å¾‘
    if os.path.exists(tesseract_cmd) or shutil.which(tesseract_cmd):
        pytesseract.pytesseract.tesseract_cmd = tesseract_cmd
        tesseract_found = True
    else:
        # ç’°å¢ƒè®Šæ•¸è·¯å¾‘ç„¡æ•ˆï¼Œå˜—è©¦å…¶ä»–è·¯å¾‘
        tesseract_cmd = None

if not tesseract_found:
    # å˜—è©¦å¸¸è¦‹è·¯å¾‘
    possible_paths = [
        '/root/.nix-profile/bin/tesseract',  # Railway/Nix ç’°å¢ƒ
        '/usr/bin/tesseract',  # Ubuntu/Debian
        '/usr/local/bin/tesseract',  # æ¨™æº– Unix ä½ç½®
        '/bin/tesseract',  # æŸäº› Linux ç™¼è¡Œç‰ˆ
        '/opt/homebrew/bin/tesseract',  # macOS Homebrew
        'tesseract'  # PATH ä¸­ï¼ˆæœ€å¾Œå˜—è©¦ï¼‰
    ]
    
    for path in possible_paths:
        if shutil.which(path) or os.path.exists(path):
            pytesseract.pytesseract.tesseract_cmd = path
            tesseract_found = True
            break
    
    if not tesseract_found:
        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨é è¨­å€¼ä½†æœƒåœ¨å¾ŒçºŒè™•ç†ä¸­å„ªé›…é™ç´š
        pytesseract.pytesseract.tesseract_cmd = 'tesseract'

# é…ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)
if tesseract_found:
    logger.info(f"âœ… Tesseract OCR æ‰¾åˆ°: {pytesseract.pytesseract.tesseract_cmd}")
else:
    logger.warning(f"âš ï¸ Tesseract OCR æœªæ‰¾åˆ°ï¼ŒOCR åŠŸèƒ½å°‡è¢«åœç”¨")

logger.setLevel(logging.INFO)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    handlers=[logging.StreamHandler()]
)

# å»¶é²åˆå§‹åŒ– prompt_manager
prompt_manager = None

class DeckBrowser:
    
    def __init__(self, prompt_manager: GoogleSheetPromptManager = None):
        """Initialize the DeckBrowser."""
        # å»¶é²åˆå§‹åŒ– prompt_managerï¼Œé¿å…å•Ÿå‹•æ™‚ç¶²è·¯å•é¡Œ
        self.prompt_manager = prompt_manager
        
        # è¨­ç½®æ—¥èªŒ
        self.logger = logging.getLogger(__name__)
        self.browser = None
        self.email = os.getenv("DOCSEND_EMAIL")  # æ›¿æ›ç‚ºæ‚¨çš„é›»å­éƒµä»¶
        self.path_helper = PathHelper()
        self.docsend_password = None  # æ–°å¢ï¼šå„²å­˜ DocSend å¯†ç¢¼

    #æ±ºå®šæµç¨‹
    SourceType = Literal["docsend", "attachment", "gdrive", "website", "unknown"]
    
    @staticmethod
    def detect_source_type(message: str, attachments: Optional[list] = None) -> "DeckBrowser.SourceType":
        if "docsend.com" in message.lower():
            return "docsend"
        elif attachments and any(
            att.get("name", "").lower().endswith(('.pdf', '.pptx', '.ppt'))
            for att in attachments if isinstance(att, dict)
        ):
            return "attachment"
        elif re.search(r"https://(?:drive|docs)\.google\.com/(?:file/d/|presentation/)[\w\-/]+", message):
            return "gdrive"
        elif re.search(r"https?://[^\s\)]+", message):  # åŒ¹é…ä»»ä½•ç¶²å€
            return "website"
        else:
            return "unknown"
    
    def extract_password_from_message(self, message: str) -> str:
        """å¾ message ä¸­æ“·å– password: xxx æˆ– å¯†ç¢¼: xxx"""
        import re
        patterns = [
            r'password[:ï¼š]?\s*([\w\-!@#$%^&*()_+=]+)',
            r'passwords[:ï¼š]?\s*([\w\-!@#$%^&*()_+=]+)',
            r'pw[:ï¼š]?\s*([\w\-!@#$%^&*()_+=]+)',
            r'å¯†ç¢¼[:ï¼š]?\s*([\w\-!@#$%^&*()_+=]+)'
        ]
        for pattern in patterns:
            match = re.search(pattern, message, re.IGNORECASE)
            if match:
                return match.group(1)
        return None

    async def process_input(self, message: str, attachments: Optional[list] = None):
        # å…ˆæ“·å–å¯†ç¢¼
        self.docsend_password = self.extract_password_from_message(message)
        self.logger.info(f"å·²æ“·å–å¯†ç¢¼: {self.docsend_password}" )
        results = []  # ç”¨æ–¼å­˜å„²æ‰€æœ‰çµæœ
        processed_urls = set()

        # 1. DocSend
        if "docsend.com" in message.lower():
            self.logger.info(f"é–‹å§‹è™•ç† Docsend")
            docsend_results = await self.run_docsend_analysis(message)
            if docsend_results:
                results.extend(docsend_results)
            # æ”¶é›†å·²è™•ç†éçš„ DocSend é€£çµ
            docsend_urls = await self.extract_docsend_links(message)
            processed_urls.update(docsend_urls)

        # 2. Attachment
        if attachments and any(
            att.get("name", "").lower().endswith((".pdf", ".pptx", ".ppt"))
            for att in attachments if isinstance(att, dict)
        ):
            self.logger.info(f"é–‹å§‹è™•ç† Attachment")
            attachment_results = await self.run_file_analysis(attachments)
            if attachment_results:
                results.extend(attachment_results)
            # é™„ä»¶é€šå¸¸æ²’æœ‰ç¶²å€ï¼Œé€™è£¡ç•¥é

        # 3. GDrive
        gdrive_urls = []
        if re.search(r"https://(?:drive|docs)\.google\.com/(?:file/d/|presentation/)[\w\-/]+", message):
            self.logger.info(f"é–‹å§‹è™•ç† Google Drive")
            gdrive_results = await self.run_gdrive_analysis(message)
            if gdrive_results:
                results.extend(gdrive_results)
            # æ”¶é›†å·²è™•ç†éçš„ GDrive é€£çµ
            gdrive_urls = re.findall(r'https://drive\.google\.com/file/d/[\w-]+|https://docs\.google\.com/presentation/d/[\w-]+', message)
            processed_urls.update(gdrive_urls)

        # 4. Generic Website
        if re.search(r"https?://[^\s\)]+", message):
            self.logger.info("ğŸ”— åµæ¸¬ç‚ºä¸€èˆ¬ç¶²ç«™ï¼Œé–‹å§‹æ“·å–ç¶²é å…§å®¹é€²è¡Œåˆ†æ")
            generic_results = await self.run_generic_link_analysis(message, exclude_urls=processed_urls)
            if generic_results:
                results.extend(generic_results)

        # 5. ç´”æ–‡å­—
        if not results:
            self.logger.info("æœªåµæ¸¬åˆ°é€£çµæˆ–é™„ä»¶ï¼Œç›´æ¥åˆ†æç´”æ–‡å­—å…§å®¹")
            summary = await summarize_pitch_deck(message, message)
            return [summary] if summary else [{"error": "âŒ ç´”æ–‡å­—åˆ†æå¤±æ•—"}]

        return results if results else [{"error": "âŒ æ²’æœ‰æˆåŠŸæ“·å–ä»»ä½•å…§å®¹"}]

    async def run_docsend_analysis(self, message: str) -> List[Dict[str, Any]]:
        """
        å°è£å®Œæ•´æµç¨‹ï¼šåˆå§‹åŒ– -> æ“·å– URL -> æŠ½å–å…§å®¹ -> é—œé–‰ç€è¦½å™¨ -> å›å‚³çµæœ
        """
        await self.initialize()
        
        results = []  # List to store JSON results
        urls = await self.extract_docsend_links(message)
        
        for url in urls:
            self.logger.info(f"è™•ç† DocSend é€£çµ: {url}")
            content = await self.read_docsend_document(url)
            if isinstance(content, dict):
                results.append(content)
            elif isinstance(content, str):
                summarized = await summarize_pitch_deck(content, message)
                if summarized:
                    results.append(summarized)

        await self.close()
        
        return results if results else [{"error": "âŒ æ²’æœ‰æˆåŠŸæ“·å–ä»»ä½• DocSend æ–‡æª”å…§å®¹"}]

    async def run_gdrive_analysis(self, message: str) -> List[Dict[str, Any]]:
        self.logger.info(f"ğŸ“¥ é–‹å§‹è™•ç† Google Drive é€£çµ")
        results = []

        # æå–æ‰€æœ‰ Google Drive é€£çµ
        gdrive_urls = re.findall(r'https://drive\.google\.com/file/d/([\w-]+)|id=([\w-]+)', message)
        
        for match in gdrive_urls:
            file_id = match[0] or match[1]  # ä½¿ç”¨éç©ºçš„åŒ¹é…çµæœ
            export_url = f"https://drive.google.com/uc?export=download&id={file_id}"
            
            try:
                response = requests.get(export_url)
                response.raise_for_status()

                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                    tmp.write(response.content)
                    tmp_path = tmp.name

                self.logger.info(f"ğŸ“„ æˆåŠŸä¸‹è¼‰ PDF æª”æ¡ˆè‡³ {tmp_path}ï¼Œé–‹å§‹åŸ·è¡Œåˆ†æ...")

                # ä½¿ç”¨ Playwright æ‰“é–‹ PDF æ–‡ä»¶
                await self.initialize()
                page = await self._get_page(f"file://{tmp_path}")
                
                try:
                    # ç­‰å¾… PDF åŠ è¼‰
                    await page.wait_for_load_state('networkidle', timeout=30000)
                    
                    # æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰å…§å®¹
                    self.logger.info("é–‹å§‹æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰å…§å®¹")
                    last_height = await page.evaluate('document.body.scrollHeight')
                    while True:
                        await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                        await page.wait_for_timeout(2000)
                        new_height = await page.evaluate('document.body.scrollHeight')
                        if new_height == last_height:
                            break
                        last_height = new_height
                        self.logger.info("ç¹¼çºŒæ»¾å‹•ä»¥åŠ è¼‰æ›´å¤šå…§å®¹")
                    
                    # æ»¾å‹•å›é ‚éƒ¨
                    await page.evaluate('window.scrollTo(0, 0)')
                    await page.wait_for_timeout(1000)
                    
                    # æå–å…§å®¹
                    content = await page.content()
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # æå–æ–‡æœ¬
                    text_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div'])
                    extracted_text = []
                    
                    for elem in text_elements:
                        text = elem.get_text().strip()
                        if text and len(text) > 10:
                            extracted_text.append(text)
                    
                    if not extracted_text:
                        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ–‡æœ¬ï¼Œå˜—è©¦ OCR
                        self.logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°æ–‡æœ¬å…§å®¹ï¼Œå˜—è©¦ OCR")
                        img_tags = soup.find_all('img')
                        image_urls = [img['src'] for img in img_tags if img.get('src')]
                        
                        if image_urls:
                            ocr_text = await ocr_images_from_urls(image_urls)
                            if ocr_text:
                                summary = await summarize_pitch_deck(ocr_text, message)
                                results.append(summary)
                                continue
                    
                    # æ ¼å¼åŒ–å…§å®¹
                    formatted_content = "\n\n".join(extracted_text)
                    summary = await summarize_pitch_deck(formatted_content, message)
                    if summary:
                        results.append(summary)
                    
                except Exception as e:
                    self.logger.error(f"è™•ç† PDF æ™‚å‡ºéŒ¯: {e}")
                    results.append({"error": f"âŒ è™•ç† PDF å¤±æ•—: {str(e)}"})
                finally:
                    await page.close()
                    await self.close()
                
            except Exception as e:
                self.logger.error(f"âŒ ä¸‹è¼‰æˆ–è™•ç† Google Drive æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
                results.append({"error": f"âŒ Google Drive æª”æ¡ˆè™•ç†å¤±æ•—ï¼š{str(e)}"})
        
        return results if results else [{"error": "âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½• Google Drive æª”æ¡ˆ"}]

    async def run_file_analysis(self, attachments: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """
        è™•ç† PDF/PPTX é™„ä»¶ï¼šåŸ·è¡Œ OCR æˆ–çµæ§‹åŒ–æ‘˜è¦
        """
        results = []

        for file in attachments:
            path = file.get("path")
            name = file.get("name", "unnamed")
            suffix = self.path_helper.get(name).suffix.lower()

            self.logger.info(f"ğŸ“‚ é–‹å§‹åˆ†æé™„ä»¶: {name}")

            extracted_text = ""

            try:
                # Check file size before processing
                file_path_obj = self.path_helper.get(path)
                if not file_path_obj.exists() or file_path_obj.stat().st_size < 1024:
                    self.logger.error(f"âŒ æª”æ¡ˆ {name} ({path}) å¤ªå°æˆ–ä¸å­˜åœ¨ï¼Œå¯èƒ½ä¸‹è¼‰å¤±æ•—ã€‚")
                    results.append({"error": f"âŒ æª”æ¡ˆ {name} ä¸‹è¼‰å¤±æ•—æˆ–ä¸æ˜¯æœ‰æ•ˆçš„æª”æ¡ˆã€‚"})
                    continue

                if suffix == ".pdf":
                    doc = fitz.open(str(file_path_obj))
                    for page in doc:
                        extracted_text += page.get_text("text") + "\n"
                    doc.close()

                elif suffix == ".pptx":
                    try:
                        prs = Presentation(str(file_path_obj))
                    except Exception as e:
                        self.logger.error(f"âŒ ç„¡æ³•é–‹å•Ÿ PPTX æª”æ¡ˆ {name}: {type(e).__name__}: {e}")
                        results.append({"error": f"âŒ ç„¡æ³•é–‹å•Ÿ PPTX æª”æ¡ˆ {name}: {type(e).__name__}: {e}"})
                        continue
                    for i, slide in enumerate(prs.slides):
                        for shape in slide.shapes:
                            if hasattr(shape, "text"):
                                extracted_text += f"[Slide {i+1}]\n{shape.text}\n"

                if not extracted_text.strip():
                    self.logger.warning(f"âš ï¸ {name} æ²’æœ‰æ“·å–åˆ°æ–‡å­—ï¼Œé–‹å§‹åŸ·è¡Œ OCR åœ–ç‰‡è¾¨è­˜")

                    image_urls = []
                    if suffix == ".pdf":
                        doc = fitz.open(str(file_path_obj))
                        for page_num, page in enumerate(doc):
                            pix = page.get_pixmap(dpi=200)
                            img_path_obj = self.path_helper.get(f"{file_path_obj}_page_{page_num}.png")
                            pix.save(str(img_path_obj))
                            image_urls.append(f"file://{img_path_obj}")
                        doc.close()

                    elif suffix == ".pptx":
                        self.logger.warning("âŒ å°šæœªå¯¦ä½œ PPTX é é¢è½‰åœ–ç‰‡çš„ OCR fallback")
                        continue

                    if image_urls:
                        ocr_text = await ocr_images_from_urls(image_urls)
                        if ocr_text:
                            summary = await summarize_pitch_deck(ocr_text, name)
                            if summary:
                                results.append(summary)
                                continue

                else:
                    summary = await summarize_pitch_deck(extracted_text, name)
                    if summary:
                        results.append(summary)

            except Exception as e:
                self.logger.error(f"âŒ åˆ†ææª”æ¡ˆ {name} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                results.append({"error": f"âŒ åˆ†æå¤±æ•—: {name}"})

        return results if results else [{"error": "âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•é™„ä»¶å…§å®¹"}]

    async def initialize(self):
        """Initialize the browser and browser context asynchronously."""
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=[
                    "--disable-gpu",
                    "--no-sandbox",
                    "--disable-dev-shm-usage",
                    "--disable-setuid-sandbox",
                    "--disable-extensions",
                    "--disable-background-networking",
                    "--disable-sync",
                    "--metrics-recording-only",
                    "--disable-default-apps",
                    "--mute-audio",
                    "--no-first-run",
                    "--hide-scrollbars",
                    "--ignore-certificate-errors",
                    "--window-size=1280,800",
                    "--single-process",
                    "--disable-blink-features=AutomationControlled"
                ]
            )

            # âœ… å»ºè­°åœ¨é€™è£¡å»ºç«‹ contextï¼Œè€Œä¸æ˜¯å»¶å¾Œåˆ° _get_page æ‰å»º
            self.context = await self.browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
                viewport={"width": 1280, "height": 800},
                locale="en-US",
                timezone_id="America/Los_Angeles",
                permissions=["geolocation"],
                extra_http_headers={
                    "Accept-Language": "en-US,en;q=0.9",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
                    "sec-ch-ua": '"Chromium";v="123", "Not:A-Brand";v="99", "Google Chrome";v="123"',
                    "sec-ch-ua-mobile": "?0",
                    "sec-ch-ua-platform": '"Windows"',
                }
            )

            self.logger.info("âœ… Browser + context initialized")
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize browser: {str(e)}")
            raise



    async def close(self):
        """é—œé–‰ç€è¦½å™¨"""
        try:
            if self.browser:
                await self.browser.close()
                self.browser = None
            if hasattr(self, 'playwright'):
                await self.playwright.stop()
        except Exception as e:
            self.logger.error(f"Error closing browser: {e}")
        finally:
            # ç¢ºä¿æ‰€æœ‰è³‡æºéƒ½è¢«é‡‹æ”¾
            import gc
            gc.collect()

    async def extract_docsend_links(self, text: str) -> List[str]:
        """å¾æ–‡æœ¬ä¸­æå– DocSend é€£çµ"""
        docsend_pattern = r'https?://(?:www\.)?docsend\.com/[^\s)"}]+'
        return re.findall(docsend_pattern, text)
    
    async def _get_page(self, url: str) -> Page:
        if not self.context:
            raise RuntimeError("Browser context not initialized")
        page = await self.context.new_page()
        page.set_default_timeout(30000)
        return page


    async def read_docsend_document(self, url: str) -> Optional[str]:
        """è®€å– DocSend æ–‡æª”çš„å…§å®¹"""
        try:
            self.logger.info(f"æ­£åœ¨å¾ DocSend é€£çµè®€å–å…§å®¹: {url}")
            
            # å‰µå»ºæ–°é é¢
            page = await self._get_page(url)
            
            # è¨ªå• DocSend é é¢
            response = await page.goto(url, wait_until='networkidle', timeout=30000)
            
            self.logger.info(f"è¨ªå•ç‹€æ…‹ç¢¼: {response.status}")
            
            if response.status == 403:
                logger.error("è¨ªå•è¢«æ‹’çµ• (403 Forbidden)")
                await page.close()
                return None
            
            # éš¨æ©Ÿç­‰å¾… 2-5 ç§’
            await asyncio.sleep(random.uniform(2, 5))
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦å¡«å¯«é›»å­éƒµä»¶
            self.logger.info("[DocSend] æº–å‚™æª¢æŸ¥æ˜¯å¦éœ€è¦å¡«å¯« email")
            email_input = await page.query_selector('input[type="email"]')
            if email_input:
                self.logger.info("[DocSend] åµæ¸¬åˆ° email è¼¸å…¥æ¡†ï¼Œæº–å‚™å¡«å¯« email")
                await page.type('input[type="email"]', self.email, delay=random.uniform(100, 200))
                self.logger.info(f"[DocSend] å·²è¼¸å…¥ email: {self.email}")
                # å¡«å®Œ email å¾Œæ‰¾å¯†ç¢¼ inputï¼Œæ”¯æ´å¤šç¨® selector
                selectors = [
                    'input.js-auth-form_passcode',
                    'input[name="link_auth_form[passcode]"]',
                    'input[id="link_auth_form_passcode"]',
                    'input[type="password"]'
                ]
                password_input = None
                used_selector = None
                for sel in selectors:
                    try:
                        password_input = await page.query_selector(sel)
                        if password_input:
                            self.logger.info(f"[DocSend] ç”¨ selector {sel} æ‰¾åˆ°å¯†ç¢¼æ¬„ä½")
                            used_selector = sel
                            break
                    except Exception as e:
                        self.logger.warning(f"[DocSend] ç”¨ selector {sel} æ‰¾å¯†ç¢¼æ¬„ä½å¤±æ•—: {e}")
                if password_input and self.docsend_password:
                    await page.type(used_selector, self.docsend_password, delay=random.uniform(100, 200))
                    self.logger.info(f"[DocSend] å·²ç”¨ {used_selector} å¡«å…¥å¯†ç¢¼: {self.docsend_password}")
                else:
                    self.logger.warning("[DocSend] æ²’æœ‰æ‰¾åˆ°å¯å¡«å¯«çš„å¯†ç¢¼æ¬„ä½")
                # å¡«å®Œå¯†ç¢¼å†æŒ‰ Continue
                try:
                    btn = page.locator('button:has-text("Continue")')
                    await btn.click()
                    self.logger.info("[DocSend] å·²é»æ“Š Continue (email+password)")
                    try:
                        await page.wait_for_load_state('networkidle', timeout=8000)
                    except Exception as e:
                        self.logger.warning(f"[DocSend] é»æ“Š Continue å¾Œç­‰å¾… networkidle è¶…æ™‚: {e}ï¼Œæ”¹ç”¨ sleep 3 ç§’")
                        await page.wait_for_timeout(3000)
                except Exception as e:
                    self.logger.warning(f"[DocSend] æäº¤ email+password æŒ‰éˆ•é»æ“Šå¤±æ•—: {e}")
                self.logger.info("[DocSend] email+password æµç¨‹çµæŸï¼Œé€²å…¥ä¸‹ä¸€æ­¥")
            else:
                # æ²’æœ‰ password inputï¼Œæ‰æŒ‰ Continue
                try:
                    self.logger.info("[DocSend] æ²’æœ‰åµæ¸¬åˆ° password è¼¸å…¥æ¡†ï¼Œæº–å‚™é»æ“Š Continue æŒ‰éˆ• (email only)")
                    await page.locator('button:has-text("Continue")').wait_for(state='visible', timeout=1000)
                    await page.locator('button:has-text("Continue")').click(timeout=1000)
                    self.logger.info("[DocSend] å·²é»æ“Š Continue (email only)")
                except Exception as e:
                    self.logger.warning(f"[DocSend] æäº¤ email æŒ‰éˆ•é»æ“Šå¤±æ•—: {e}")
                await page.wait_for_load_state('networkidle', timeout=10000)
                self.logger.info("[DocSend] email only æµç¨‹çµæŸï¼Œé€²å…¥ä¸‹ä¸€æ­¥")
            # --- å¯†ç¢¼è‡ªå‹•å¡«å¯«çµæŸ ---

            # æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰å…§å®¹
            self.logger.info("é–‹å§‹æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰å…§å®¹")
            last_height = await page.evaluate('document.body.scrollHeight')
            while True:
                # æ»¾å‹•åˆ°åº•éƒ¨
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                # ç­‰å¾…æ–°å…§å®¹åŠ è¼‰
                await page.wait_for_timeout(2000)
                # è¨ˆç®—æ–°çš„æ»¾å‹•é«˜åº¦
                new_height = await page.evaluate('document.body.scrollHeight')
                if new_height == last_height:
                    break
                last_height = new_height
                self.logger.info("ç¹¼çºŒæ»¾å‹•ä»¥åŠ è¼‰æ›´å¤šå…§å®¹")
            
            # æ»¾å‹•å›é ‚éƒ¨
            await page.evaluate('window.scrollTo(0, 0)')
            await page.wait_for_timeout(1000)
            
            # å˜—è©¦é»æ“Šä¸‹ä¸€é æŒ‰éˆ•åŠ è¼‰æ›´å¤šå…§å®¹
            self.logger.info("å˜—è©¦é€šéé»æ“Šä¸‹ä¸€é æŒ‰éˆ•åŠ è¼‰æ›´å¤šå…§å®¹")
            try:
                next_buttons = await page.query_selector_all('button[aria-label*="next"], button[aria-label*="Next"], button[title*="next"], button[title*="Next"]')
                for button in next_buttons:
                    for _ in range(10):  # æœ€å¤šé»æ“Š10æ¬¡
                        try:
                            await button.click()
                            await page.wait_for_timeout(1000)  # ç­‰å¾…æ–°é é¢åŠ è¼‰
                        except:
                            break
            except Exception as e:
                self.logger.warning(f"é»æ“Šä¸‹ä¸€é æŒ‰éˆ•æ™‚å‡ºéŒ¯: {e}")
            
            # å†æ¬¡æ»¾å‹•ä»¥ç¢ºä¿æ‰€æœ‰å…§å®¹éƒ½å·²åŠ è¼‰
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            await page.wait_for_timeout(2000)
            
            # æˆªåœ–ä»¥ä¾¿èª¿è©¦
            debug_screenshot_name = f"docsend_debug_{random.randint(1000, 9999)}.png"
            debug_screenshot_path = str(self.path_helper.get("tmp", debug_screenshot_name))
            self.path_helper.ensure_dir("tmp")
            await page.screenshot(path=debug_screenshot_path)
            self.logger.info(f"ä¿å­˜é é¢æˆªåœ–è‡³: {debug_screenshot_path}")
            
            # Debug å°‡æ•´é  HTML å„²å­˜ä¸‹ä¾†
            html = await page.content()
            debug_html_path = self.path_helper.get("debug_docsend.html")
            with debug_html_path.open("w", encoding="utf-8") as f:
                f.write(html)

            # ç­‰å¾…æ–‡æª”å…§å®¹åŠ è¼‰
            target_frame = None
            content_selectors = ['.document-content', '.page', '.viewer-content', '.ds-viewer-container']

            for frame in page.frames:
                frame_url = frame.url or ""
                if "docsend.com/view" in frame_url and "marketing.docsend.com" not in frame_url:
                    for selector in content_selectors:
                        try:
                            await frame.wait_for_selector(selector, timeout=5000)
                            self.logger.info(f"åœ¨ iframe ä¸­æ‰¾åˆ°å…§å®¹: {selector}")
                            target_frame = frame
                            break
                        except:
                            continue
                if target_frame:
                    break

            if not target_frame:
                self.logger.warning("âš ï¸ æœªèƒ½æ‰¾åˆ°å«æ–‡å­— selectorï¼Œé–‹å§‹ debug æ‰€æœ‰ iframe...")
                await debug_all_iframes(page)

                # å˜—è©¦å¾æ‰€æœ‰ iframe ä¸­æ“·å–åœ–ç‰‡åš OCR
                for frame in page.frames:
                    try:
                        frame_html = await frame.content()
                        soup = BeautifulSoup(frame_html, "html.parser")
                        img_tags = soup.find_all("img")
                        image_urls = [img["src"] for img in img_tags if img.get("src")]
                        
                        if image_urls:
                            self.logger.info(f"âœ… åœ¨ iframe ä¸­æ‰¾åˆ°åœ–ç‰‡ï¼šå…± {len(image_urls)} å¼µï¼Œæº–å‚™åŸ·è¡Œ OCR")
                            ocr_raw_text = await ocr_images_from_urls(image_urls)
                            if ocr_raw_text:
                                summarized_text = await summarize_pitch_deck(ocr_raw_text, url)
                                await page.close()
                                return summarized_text
                    except Exception as e:
                        self.logger.warning(f"è®€å– iframe å…§å®¹å¤±æ•—ï¼š{e}")

                self.logger.error("âŒ æ‰€æœ‰ iframe å‡æœªèƒ½æä¾›å…§å®¹æˆ–åœ–ç‰‡")
                await page.close()
                return None
            
            # å¾ iframe æŠ“ HTML
            html = await target_frame.content()
            self.logger.info(f"ç²å–çš„HTMLå…§å®¹é•·åº¦: {len(html)} å­—ç¬¦")
            
            # è§£æ HTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # æå–æ–‡æª”æ¨™é¡Œ
            title_elem = soup.find('title')
            if not title_elem:
                self.logger.info("iframe ä¸­æœªæ‰¾åˆ° <title>ï¼Œå˜—è©¦å¾ä¸»é æŠ“å– title")
                main_html = await page.content()
                main_soup = BeautifulSoup(main_html, 'html.parser')
                title_elem = main_soup.find('title')

            title = title_elem.text.strip() if title_elem else "DocSend Document"
            self.logger.info(f"æå–çš„æ–‡æª”æ¨™é¡Œ: {title}")
            
            # æå–æ–‡æª”å…§å®¹
            text_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div'])
            self.logger.info(f"æ‰¾åˆ° {len(text_elements)} å€‹æ–‡æœ¬å…ƒç´ ")
            
            extracted_text = []
            
            for elem in text_elements:
                text = elem.get_text().strip()
                if text and len(text) > 10:  # åªä¿ç•™æœ‰æ„ç¾©çš„æ–‡æœ¬
                    extracted_text.append(text)
            
            self.logger.info(f"ç¯©é¸å¾Œæå–äº† {len(extracted_text)} å€‹æœ‰æ„ç¾©çš„æ–‡æœ¬æ®µè½")
            extracted_text = "\n\n".join(extracted_text)
            
            # é—œé–‰é é¢
            await page.close()
            
            if not extracted_text or len(extracted_text) < 100:
                self.logger.warning("âš ï¸ æ–‡å­—å…§å®¹éå°‘ï¼Œå˜—è©¦ OCR åœ–ç‰‡ä¸¦ç”¨ GPT æ‘˜è¦")
                img_tags = soup.find_all('img')
                image_urls = [img['src'] for img in img_tags if img.get('src')]

                if image_urls:
                    ocr_raw_text = await ocr_images_from_urls(image_urls)
                    if ocr_raw_text:
                        summarized_text = await summarize_pitch_deck(ocr_raw_text, url)
                        extracted_text = summarized_text
            
            # è¨˜éŒ„å…§å®¹æ‘˜è¦
            content_preview = extracted_text[:100] + "..." if len(extracted_text) > 100 else extracted_text
            self.logger.info(f"æå–æ–‡æœ¬é è¦½: {content_preview}")
            self.logger.info(f"ç¸½æå–æ–‡æœ¬é•·åº¦: {len(extracted_text)} å­—ç¬¦")
            
            # æ ¼å¼åŒ–æå–çš„å…§å®¹
            formatted_content = f"--- DocSend æ–‡æª”: {title} ---\n\n{extracted_text}\n\n--- DocSend æ–‡æª”çµæŸ ---"
            
            self.logger.info(f"æˆåŠŸå¾ DocSend é€£çµæå–å…§å®¹: {url}")
            return formatted_content
        
        except Exception as e:
            self.logger.error(f"è®€å– DocSend æ–‡æª”æ™‚å‡ºéŒ¯: {str(e)}", exc_info=True)
            if 'page' in locals():
                await page.close()
            return None

    async def is_pitch_deck(self, page) -> bool:
        """åˆ¤æ–·ç¶²é æ˜¯å¦ç‚º Pitch Deck"""
        try:
            # 0. é¦–å…ˆæª¢æŸ¥æ˜¯å¦ç‚ºç¤¾ç¾¤ç¶²ç«™
            url = page.url.lower()
            social_media_domains = ['x.com', 'twitter.com', 'facebook.com', 'linkedin.com', 'instagram.com']
            if any(domain in url for domain in social_media_domains):
                return False

            # 1. æª¢æŸ¥ URL é—œéµå­—
            deck_keywords = ['pitch', 'deck', 'presentation', 'slides', 'investor', 'fundraising']
            if any(keyword in url for keyword in deck_keywords):
                return True

            # 2. æª¢æŸ¥é é¢æ¨™é¡Œ
            title = await page.title()
            if title and any(keyword in title.lower() for keyword in deck_keywords):
                return True

            # 3. æª¢æŸ¥é é¢å…§å®¹ç‰¹å¾µ
            content = await page.content()
            soup = BeautifulSoup(content, "html.parser")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡ç›¸é—œçš„å…ƒç´ 
            slide_indicators = [
                # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡å®¹å™¨
                bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['slide', 'deck', 'presentation']))),
                # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡å°èˆª
                bool(soup.find('nav', class_=lambda x: x and any(word in str(x).lower() for word in ['slide', 'deck', 'presentation']))),
                # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡è¨ˆæ•¸å™¨
                bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['counter', 'progress', 'slide-number']))),
                # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡æ§åˆ¶æŒ‰éˆ•
                bool(soup.find('button', class_=lambda x: x and any(word in str(x).lower() for word in ['next', 'prev', 'slide']))),
            ]
            
            if any(slide_indicators):
                return True

            # 4. æª¢æŸ¥é é¢çµæ§‹
            # è¨ˆç®—é é¢ä¸­çš„åœ–ç‰‡æ•¸é‡
            images = await page.query_selector_all('img')
            # è¨ˆç®—é é¢ä¸­çš„æ–‡æœ¬å¡Šæ•¸é‡
            text_blocks = await page.query_selector_all('p, h1, h2, h3, h4, h5, h6')
            
            # å¦‚æœåœ–ç‰‡æ•¸é‡è¼ƒå¤šä¸”æ–‡æœ¬å¡Šè¼ƒå°‘ï¼Œå¯èƒ½æ˜¯æŠ•å½±ç‰‡
            # ä½†éœ€è¦ç¢ºä¿ä¸æ˜¯ç¤¾ç¾¤ç¶²ç«™çš„çµæ§‹
            if len(images) > 5 and len(text_blocks) < 10:
                # æª¢æŸ¥æ˜¯å¦æœ‰ç¤¾ç¾¤ç¶²ç«™ç‰¹æœ‰çš„å…ƒç´ 
                social_indicators = [
                    bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['tweet', 'post', 'status', 'feed', 'timeline']))),
                    bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['profile', 'avatar', 'user-info']))),
                    bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['like', 'share', 'comment', 'retweet']))),
                ]
                if not any(social_indicators):
                    return True

            # 5. æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡ç›¸é—œçš„ JavaScript
            scripts = await page.query_selector_all('script')
            for script in scripts:
                content = await script.text_content()
                if any(keyword in content.lower() for keyword in ['slideshow', 'presentation', 'deck']):
                    return True

            return False

        except Exception as e:
            self.logger.error(f"Error checking if page is pitch deck: {e}")
            return False

    async def run_generic_link_analysis(self, message: str, exclude_urls: set = None) -> List[Dict[str, Any]]:
        """åˆ†æä¸€èˆ¬ç¶²å€ï¼ˆåŒ…æ‹¬å…¬å¸å®˜ç¶²ï¼‰ï¼Œæ¯å€‹ç¶²å€å–®ç¨å›å‚³ summaryï¼Œä¸åˆä½µçµ±æ•´"""
        urls = re.findall(r'https?://[^\s\)]+', message)
        if exclude_urls:
            # æ¨™æº–åŒ–ç¶²å€ï¼ˆå»é™¤æœ«å°¾æ–œç·šï¼‰
            def norm(u):
                return u.rstrip('/')
            exclude_set = set(map(norm, exclude_urls))
            urls = [u for u in urls if norm(u) not in exclude_set]
        results = []
        
        if not urls:
            return [{"error": "âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ç¶²å€"}]
        
        self.logger.info(f"æ‰¾åˆ° {len(urls)} å€‹ç¶²å€éœ€è¦è™•ç†")
        
        for url in urls:
            try:
                self.logger.info(f"ğŸŒ é–‹å§‹åˆ†æç¶²å€: {url}")
                content = await self.extract_content(url)
                if content:
                    self.logger.info(f"æˆåŠŸæå–å…§å®¹: {len(content)} å­—ç¬¦")
                    results.append({"url": url, "summary": content})
                else:
                    self.logger.warning("âŒ æ²’æœ‰æå–åˆ°å…§å®¹")
                    results.append({"url": url, "error": "âŒ ç„¡æ³•æå–å…§å®¹"})
            except Exception as e:
                self.logger.error(f"âŒ åˆ†æ {url} å¤±æ•—ï¼š{e}")
                results.append({"url": url, "error": f"âŒ åˆ†æå¤±æ•—: {e}"})
        return results if results else [{"error": "âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•ç¶²å€"}]

    async def process_pitch_deck_page(self, page) -> Optional[str]:
        """è™•ç† Pitch Deck é é¢"""
        try:
            self.logger.info("é–‹å§‹è™•ç† Pitch Deck é é¢")
            
            # æª¢æŸ¥æ˜¯å¦ç‚º Journey.io ç¶²ç«™
            if "journey.io" in page.url:
                self.logger.info("æª¢æ¸¬åˆ° Journey.io ç¶²ç«™ï¼Œä½¿ç”¨ç‰¹æ®Šè™•ç†æ–¹å¼")
                return await self.process_journey_page(page)
            
            # ç­‰å¾…é é¢åŠ è¼‰å®Œæˆ
            await page.wait_for_load_state('networkidle', timeout=30000)
            
            # æˆªåœ–ä»¥ä¾¿èª¿è©¦
            debug_screenshot_name = f"pitch_deck_debug_{random.randint(1000, 9999)}.png"
            debug_screenshot_path = str(self.path_helper.get("tmp", debug_screenshot_name))
            self.path_helper.ensure_dir("tmp")
            await page.screenshot(path=debug_screenshot_path)
            self.logger.info(f"ä¿å­˜é é¢æˆªåœ–è‡³: {debug_screenshot_path}")
            
            # Debug å°‡æ•´é  HTML å„²å­˜ä¸‹ä¾†
            html = await page.content()
            debug_html_path = self.path_helper.get("debug_pitch_deck.html")
            with debug_html_path.open("w", encoding="utf-8") as f:
                f.write(html)
            
            # æå–é é¢æ¨™é¡Œ
            title = await page.title()
            self.logger.info(f"æå–çš„é é¢æ¨™é¡Œ: {title}")
            
            # 1. å˜—è©¦æå–æŠ•å½±ç‰‡å…§å®¹
            slides = []
            
            # ç­‰å¾…æŠ•å½±ç‰‡å®¹å™¨åŠ è¼‰
            try:
                # æ“´å±•é¸æ“‡å™¨ä»¥åŒ¹é…æ›´å¤šå¯èƒ½çš„æŠ•å½±ç‰‡å®¹å™¨
                await page.wait_for_selector('div[class*="slide"], div[class*="deck"], div[class*="presentation"], div[class*="page"], div[class*="slide-container"], div[class*="slide-wrapper"]', timeout=5000)
            except Exception as e:
                self.logger.warning(f"ç­‰å¾…æŠ•å½±ç‰‡å®¹å™¨è¶…æ™‚: {e}")
            
            # æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰æŠ•å½±ç‰‡
            self.logger.info("é–‹å§‹æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰æŠ•å½±ç‰‡")
            last_height = await page.evaluate('document.body.scrollHeight')
            while True:
                # æ»¾å‹•åˆ°åº•éƒ¨
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                # ç­‰å¾…æ–°å…§å®¹åŠ è¼‰
                await page.wait_for_timeout(2000)
                # è¨ˆç®—æ–°çš„æ»¾å‹•é«˜åº¦
                new_height = await page.evaluate('document.body.scrollHeight')
                if new_height == last_height:
                    break
                last_height = new_height
                self.logger.info("ç¹¼çºŒæ»¾å‹•ä»¥åŠ è¼‰æ›´å¤šæŠ•å½±ç‰‡")
            
            # æ»¾å‹•å›é ‚éƒ¨
            await page.evaluate('window.scrollTo(0, 0)')
            await page.wait_for_timeout(1000)
            
            # ç²å–æ‰€æœ‰æŠ•å½±ç‰‡
            slide_elements = await page.query_selector_all('div[class*="slide"], div[class*="deck"], div[class*="presentation"], div[class*="page"], div[class*="slide-container"], div[class*="slide-wrapper"]')
            self.logger.info(f"æ‰¾åˆ° {len(slide_elements)} å€‹æŠ•å½±ç‰‡å…ƒç´ ")
            
            # å¦‚æœæ‰¾ä¸åˆ°æŠ•å½±ç‰‡ï¼Œå˜—è©¦å…¶ä»–é¸æ“‡å™¨
            if not slide_elements:
                self.logger.info("å˜—è©¦ä½¿ç”¨å‚™ç”¨é¸æ“‡å™¨")
                # å˜—è©¦æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æŠ•å½±ç‰‡å®¹å™¨
                slide_elements = await page.query_selector_all('div[role="slide"], div[data-slide], div[data-page], div[data-index]')
                self.logger.info(f"ä½¿ç”¨å‚™ç”¨é¸æ“‡å™¨æ‰¾åˆ° {len(slide_elements)} å€‹æŠ•å½±ç‰‡å…ƒç´ ")
            
            # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œå˜—è©¦æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æŠ•å½±ç‰‡å…§å®¹çš„ div
            if not slide_elements:
                self.logger.info("å˜—è©¦æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æŠ•å½±ç‰‡å…§å®¹çš„ div")
                # ç²å–é é¢ä¸­çš„æ‰€æœ‰ div
                all_divs = await page.query_selector_all('div')
                # éæ¿¾å‡ºå¯èƒ½åŒ…å«æŠ•å½±ç‰‡å…§å®¹çš„ div
                slide_elements = []
                for div in all_divs:
                    try:
                        # æª¢æŸ¥ div æ˜¯å¦åŒ…å«æŠ•å½±ç‰‡ç›¸é—œçš„å…§å®¹
                        text = await div.text_content()
                        if text and len(text.strip()) > 50:  # å‡è¨­æŠ•å½±ç‰‡å…§å®¹é€šå¸¸è¼ƒé•·
                            slide_elements.append(div)
                    except:
                        continue
                self.logger.info(f"é€šéå…§å®¹åˆ†ææ‰¾åˆ° {len(slide_elements)} å€‹å¯èƒ½çš„æŠ•å½±ç‰‡")
            
            # å¦‚æœæ‰¾åˆ°çš„æŠ•å½±ç‰‡æ•¸é‡æ˜é¡¯å°‘æ–¼é æœŸï¼Œå˜—è©¦é»æ“Šä¸‹ä¸€é æŒ‰éˆ•
            if len(slide_elements) < 10:  # å‡è¨­è‡³å°‘æœ‰10é 
                self.logger.info("å˜—è©¦é€šéé»æ“Šä¸‹ä¸€é æŒ‰éˆ•åŠ è¼‰æ›´å¤šæŠ•å½±ç‰‡")
                try:
                    # å˜—è©¦é»æ“Šä¸‹ä¸€é æŒ‰éˆ•
                    next_buttons = await page.query_selector_all('button[aria-label*="next"], button[aria-label*="Next"], button[title*="next"], button[title*="Next"]')
                    for button in next_buttons:
                        for _ in range(10):  # æœ€å¤šé»æ“Š10æ¬¡
                            try:
                                await button.click()
                                await page.wait_for_timeout(1000)  # ç­‰å¾…æ–°é é¢åŠ è¼‰
                            except:
                                break
                except Exception as e:
                    self.logger.warning(f"é»æ“Šä¸‹ä¸€é æŒ‰éˆ•æ™‚å‡ºéŒ¯: {e}")
                
                # é‡æ–°ç²å–æŠ•å½±ç‰‡
                slide_elements = await page.query_selector_all('div[class*="slide"], div[class*="deck"], div[class*="presentation"], div[class*="page"], div[class*="slide-container"], div[class*="slide-wrapper"]')
                self.logger.info(f"é»æ“Šä¸‹ä¸€é å¾Œæ‰¾åˆ° {len(slide_elements)} å€‹æŠ•å½±ç‰‡å…ƒç´ ")
            
            for i, slide in enumerate(slide_elements):
                try:
                    # æå–æŠ•å½±ç‰‡æ–‡æœ¬
                    text = await slide.text_content()
                    if text.strip():
                        slides.append(f"[Slide {i+1}]\n{text.strip()}")
                        self.logger.info(f"æˆåŠŸæå–ç¬¬ {i+1} å¼µæŠ•å½±ç‰‡çš„æ–‡æœ¬")
                    
                    # æå–æŠ•å½±ç‰‡åœ–ç‰‡
                    images = await slide.query_selector_all('img')
                    self.logger.info(f"ç¬¬ {i+1} å¼µæŠ•å½±ç‰‡ä¸­æ‰¾åˆ° {len(images)} å¼µåœ–ç‰‡")
                    
                    for img in images:
                        src = await img.get_attribute('src')
                        if src:
                            try:
                                # ä¸‹è¼‰åœ–ç‰‡ä¸¦é€²è¡Œ OCR
                                response = requests.get(src)
                                img = Image.open(BytesIO(response.content))
                                
                                try:
                                    ocr_text = pytesseract.image_to_string(img)
                                    if ocr_text.strip():
                                        slides.append(f"[Slide {i+1} Image]\n{ocr_text.strip()}")
                                        self.logger.info(f"æˆåŠŸå°ç¬¬ {i+1} å¼µæŠ•å½±ç‰‡çš„åœ–ç‰‡é€²è¡Œ OCR")
                                except Exception as ocr_e:
                                    # è™•ç† Tesseract ç›¸é—œéŒ¯èª¤
                                    if "TesseractNotFoundError" in str(type(ocr_e)) or "tesseract" in str(ocr_e).lower():
                                        self.logger.warning(f"âš ï¸ Tesseract OCR ä¸å¯ç”¨ï¼Œè·³éç¬¬ {i+1} å¼µæŠ•å½±ç‰‡åœ–ç‰‡çš„æ–‡å­—æå–")
                                        slides.append(f"[Slide {i+1} Image]\n[OCRä¸å¯ç”¨ - ç„¡æ³•æå–æ–‡å­—å…§å®¹]")
                                    else:
                                        self.logger.warning(f"âŒ OCR è™•ç†å¤±æ•— (ç¬¬ {i+1} å¼µ): {str(ocr_e)}")
                                        slides.append(f"[Slide {i+1} Image]\n[æ–‡å­—æå–å¤±æ•—]")
                                
                            except Exception as e:
                                self.logger.warning(f"OCR å¤±æ•—: {e}")
                except Exception as e:
                    self.logger.warning(f"è™•ç†ç¬¬ {i+1} å¼µæŠ•å½±ç‰‡æ™‚å‡ºéŒ¯: {e}")
            
            if slides:
                formatted_content = f"--- Pitch Deck: {title} ---\n\n" + "\n\n".join(slides) + "\n\n--- Pitch Deck çµæŸ ---"
                self.logger.info("æˆåŠŸæå–æŠ•å½±ç‰‡å…§å®¹")
                return formatted_content
            
            # 2. å¦‚æœç„¡æ³•æå–æŠ•å½±ç‰‡ï¼Œå˜—è©¦æå–æ•´å€‹é é¢å…§å®¹
            self.logger.info("ç„¡æ³•æå–æŠ•å½±ç‰‡å…§å®¹ï¼Œå˜—è©¦æå–æ•´å€‹é é¢")
            content = await page.content()
            soup = BeautifulSoup(content, "html.parser")
            
            # æå–æ‰€æœ‰æ–‡æœ¬å…§å®¹
            texts = []
            for element in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div']):
                text = element.get_text(strip=True)
                if text and len(text) > 10:
                    texts.append(text)
            
            if texts:
                formatted_content = f"--- Pitch Deck: {title} ---\n\n" + "\n\n".join(texts) + "\n\n--- Pitch Deck çµæŸ ---"
                self.logger.info("æˆåŠŸæå–é é¢æ–‡æœ¬å…§å®¹")
                return formatted_content
            
            # 3. å¦‚æœæ–‡å­—æå–å¤±æ•—ï¼Œå˜—è©¦ OCR æ‰€æœ‰åœ–ç‰‡
            self.logger.warning("æ–‡å­—æå–å¤±æ•—ï¼Œå˜—è©¦ OCR æ‰€æœ‰åœ–ç‰‡")
            img_tags = soup.find_all("img")
            image_urls = [img.get("src") for img in img_tags if img.get("src")]
            
            if image_urls:
                self.logger.info(f"æ‰¾åˆ° {len(image_urls)} å¼µåœ–ç‰‡ï¼Œé–‹å§‹ OCR")
                ocr_text = await ocr_images_from_urls(image_urls)
                if ocr_text.strip():
                    formatted_content = f"--- Pitch Deck: {title} ---\n\n{ocr_text}\n\n--- Pitch Deck çµæŸ ---"
                    self.logger.info("æˆåŠŸé€šé OCR æå–å…§å®¹")
                    return formatted_content
            
            self.logger.error("âŒ ç„¡æ³•æå–ä»»ä½•æœ‰æ•ˆå…§å®¹")
            return None
            
        except Exception as e:
            self.logger.error(f"è™•ç† Pitch Deck é é¢æ™‚å‡ºéŒ¯: {str(e)}", exc_info=True)
            return None

    async def process_journey_page(self, page) -> Optional[str]:
        """è™•ç† Journey.io é é¢"""
        try:
            self.logger.info("é–‹å§‹è™•ç† Journey.io é é¢")
            
            # ç­‰å¾…é é¢åŠ è¼‰
            await page.wait_for_load_state('networkidle', timeout=30000)
            
            # ä¿å­˜é é¢çµæ§‹ä»¥ä¾›èª¿è©¦
            debug_html_path = self.path_helper.get("debug_journey.html")
            html_content = await page.content()
            with debug_html_path.open("w", encoding="utf-8") as f:
                f.write(html_content)
            self.logger.info(f"å·²ä¿å­˜é é¢çµæ§‹åˆ° {debug_html_path}")
            
            # ä¿å­˜é é¢æˆªåœ–
            debug_screenshot_path = self.path_helper.get("debug_journey.png")
            await page.screenshot(path=str(debug_screenshot_path))
            self.logger.info(f"å·²ä¿å­˜é é¢æˆªåœ–åˆ° {debug_screenshot_path}")
            
            # æå–é é¢æ¨™é¡Œ
            title = await page.title()
            self.logger.info(f"æå–çš„é é¢æ¨™é¡Œ: {title}")
            
            # æ”¹é€²çš„æ»¾å‹•é‚è¼¯
            self.logger.info("é–‹å§‹æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰å…§å®¹")
            
            # 1. å…ˆæ»¾å‹•åˆ°é ‚éƒ¨
            await page.evaluate('window.scrollTo(0, 0)')
            await page.wait_for_timeout(1000)
            
            # 2. ç²å–åˆå§‹é é¢é«˜åº¦
            initial_height = await page.evaluate('document.body.scrollHeight')
            self.logger.info(f"åˆå§‹é é¢é«˜åº¦: {initial_height}")
            
            # 3. ä½¿ç”¨æ›´å°çš„æ»¾å‹•æ­¥é•·å’Œæ›´é•·çš„ç­‰å¾…æ™‚é–“
            scroll_step = 300  # æ¯æ¬¡æ»¾å‹•300åƒç´ 
            max_attempts = 50  # æœ€å¤§å˜—è©¦æ¬¡æ•¸
            no_change_count = 0  # è¨˜éŒ„é«˜åº¦æœªè®ŠåŒ–çš„æ¬¡æ•¸
            
            for attempt in range(max_attempts):
                # ç²å–ç•¶å‰æ»¾å‹•ä½ç½®
                current_position = await page.evaluate('window.pageYOffset')
                
                # è¨ˆç®—æ–°çš„æ»¾å‹•ä½ç½®
                new_position = current_position + scroll_step
                
                # æ»¾å‹•åˆ°æ–°ä½ç½®
                await page.evaluate(f'window.scrollTo(0, {new_position})')
                await page.wait_for_timeout(2000)  # ç­‰å¾…2ç§’è®“å…§å®¹åŠ è¼‰
                
                # ç²å–æ–°çš„é é¢é«˜åº¦
                new_height = await page.evaluate('document.body.scrollHeight')
                
                # æª¢æŸ¥æ˜¯å¦åˆ°é”åº•éƒ¨
                if new_position >= new_height:
                    self.logger.info("å·²åˆ°é”é é¢åº•éƒ¨")
                    break
                
                # æª¢æŸ¥é é¢é«˜åº¦æ˜¯å¦è®ŠåŒ–
                if new_height == initial_height:
                    no_change_count += 1
                    if no_change_count >= 3:  # å¦‚æœé€£çºŒ3æ¬¡é«˜åº¦æœªè®ŠåŒ–ï¼Œå¯èƒ½å·²ç¶“åˆ°åº•
                        self.logger.info("é é¢é«˜åº¦é€£çºŒæœªè®ŠåŒ–ï¼Œå¯èƒ½å·²åˆ°åº•éƒ¨")
                        break
                else:
                    no_change_count = 0
                    initial_height = new_height
                
                self.logger.info(f"æ»¾å‹•é€²åº¦: {new_position}/{new_height} (å˜—è©¦ {attempt + 1}/{max_attempts})")
            
            # 4. æœ€å¾Œå†æ»¾å‹•å›é ‚éƒ¨
            await page.evaluate('window.scrollTo(0, 0)')
            await page.wait_for_timeout(1000)
            
            # ä½¿ç”¨ JavaScript æå–æ‰€æœ‰å…§å®¹
            content = await page.evaluate('''() => {
                const sections = [];
                
                // éæ­·æ‰€æœ‰ä¸»è¦å€å¡Š
                document.querySelectorAll('div[class*="page"], div[class*="section"], div[class*="content"], div[class*="block"], div[class*="slide"]').forEach(section => {
                    const sectionData = {
                        title: '',
                        content: [],
                        teamMembers: []
                    };
                    
                    // æå–æ¨™é¡Œ
                    const titleElem = section.querySelector('h1, h2, h3, h4, h5, h6, [class*="title"], [class*="heading"], [class*="header"]');
                    if (titleElem) {
                        sectionData.title = titleElem.textContent.trim();
                    }
                    
                    // æª¢æŸ¥æ˜¯å¦ç‚ºåœ˜éšŠç›¸é—œéƒ¨åˆ†
                    const isTeamSection = sectionData.title.toLowerCase().includes('team') || 
                                        sectionData.title.toLowerCase().includes('about us') ||
                                        sectionData.title.toLowerCase().includes('founder') ||
                                        sectionData.title.toLowerCase().includes('leadership');
                    
                    if (isTeamSection) {
                        console.log('Found team section:', sectionData.title);
                        
                        // æ“´å±•åœ˜éšŠæˆå“¡é¸æ“‡å™¨
                        const teamElements = section.querySelectorAll(`
                            div[class*="member"], 
                            div[class*="person"], 
                            div[class*="profile"], 
                            div[class*="team"], 
                            div[class*="founder"], 
                            div[class*="investor"], 
                            div[class*="funding"],
                            div[class*="card"],
                            div[class*="bio"],
                            div[class*="staff"],
                            div[class*="leadership"],
                            div[class*="executive"]
                        `);
                        
                        console.log('Found team elements:', teamElements.length);
                        
                        teamElements.forEach(teamElem => {
                            const memberData = {
                                name: '',
                                role: '',
                                description: ''
                            };
                            
                            // æ“´å±•åç¨±é¸æ“‡å™¨
                            const nameElem = teamElem.querySelector(`
                                h1, h2, h3, h4, h5, h6, 
                                [class*="name"], 
                                [class*="title"],
                                [class*="heading"],
                                [class*="header"],
                                strong,
                                b
                            `);
                            if (nameElem) {
                                memberData.name = nameElem.textContent.trim();
                                console.log('Found team member name:', memberData.name);
                            }
                            
                            // æ“´å±•è§’è‰²é¸æ“‡å™¨
                            const roleElem = teamElem.querySelector(`
                                [class*="role"], 
                                [class*="position"], 
                                [class*="title"],
                                [class*="job"],
                                [class*="position"],
                                [class*="designation"],
                                em,
                                i
                            `);
                            if (roleElem) {
                                memberData.role = roleElem.textContent.trim();
                                console.log('Found team member role:', memberData.role);
                            }
                            
                            // æ“´å±•æè¿°é¸æ“‡å™¨
                            const descElem = teamElem.querySelector(`
                                p, 
                                div[class*="text"], 
                                div[class*="content"], 
                                div[class*="description"],
                                div[class*="bio"],
                                div[class*="about"],
                                div[class*="info"]
                            `);
                            if (descElem) {
                                memberData.description = descElem.textContent.trim();
                                console.log('Found team member description:', memberData.description);
                            }
                            
                            // å¦‚æœæ‰¾åˆ°ä»»ä½•ä¿¡æ¯ï¼Œå°±æ·»åŠ åˆ°åœ˜éšŠæˆå“¡åˆ—è¡¨
                            if (memberData.name || memberData.role || memberData.description) {
                                sectionData.teamMembers.push(memberData);
                                console.log('Added team member:', memberData);
                            }
                        });
                    }
                    
                    // æå–æ‰€æœ‰æ–‡æœ¬å…§å®¹
                    const textElements = section.querySelectorAll(`
                        p, 
                        li, 
                        div[class*="text"], 
                        div[class*="content"], 
                        div[class*="description"],
                        div[class*="info"],
                        div[class*="detail"]
                    `);
                    textElements.forEach(elem => {
                        const text = elem.textContent.trim();
                        if (text) {
                            sectionData.content.push(text);
                        }
                    });
                    
                    if (sectionData.title || sectionData.content.length > 0 || sectionData.teamMembers.length > 0) {
                        sections.push(sectionData);
                    }
                });
                
                return sections;
            }''')
            
            # æ ¼å¼åŒ–æå–çš„å…§å®¹
            formatted_sections = []
            for section in content:
                section_text = []
                
                if section['title']:
                    section_text.append(f"\n## {section['title']}")
                    self.logger.info(f"Found section: {section['title']}")
                
                if section['content']:
                    section_text.extend(section['content'])
                    self.logger.info(f"Found {len(section['content'])} content items")
                
                if section['teamMembers']:
                    section_text.append("\n### Team Members:")
                    self.logger.info(f"Found {len(section['teamMembers'])} team members")
                    for member in section['teamMembers']:
                        member_text = []
                        if member['name']:
                            member_text.append(f"Name: {member['name']}")
                            self.logger.info(f"Found team member: {member['name']}")
                        if member['role']:
                            member_text.append(f"Role: {member['role']}")
                            self.logger.info(f"Member role: {member['role']}")
                        if member['description']:
                            member_text.append(f"Description: {member['description']}")
                            self.logger.info(f"Member description: {member['description']}")
                        if member_text:
                            section_text.append("\n" + "\n".join(member_text))
                
                if section_text:
                    formatted_sections.append('\n'.join(section_text))
            
            if formatted_sections:
                formatted_content = f"--- Journey.io Document: {title} ---\n\n" + "\n\n".join(formatted_sections) + "\n\n--- Document End ---"
                self.logger.info("æˆåŠŸæå– Journey.io é é¢å…§å®¹")
                return formatted_content
            
            self.logger.error("âŒ ç„¡æ³•æå–ä»»ä½•æœ‰æ•ˆå…§å®¹")
            return None
            
        except Exception as e:
            self.logger.error(f"è™•ç† Journey.io é é¢æ™‚å‡ºéŒ¯: {str(e)}", exc_info=True)
            return None

    async def extract_content(self, url: str) -> Optional[str]:
        """ç”¨ Playwright å–å¾—æ¸²æŸ“å¾Œå…§å®¹ï¼ŒåªæŠ“ä¸»è¦æ–‡å­—å…§å®¹ï¼Œä¸å‘¼å« GPT"""
        try:
            self.logger.info(f"ç”¨ Playwright å–å¾—æ¸²æŸ“å¾Œå…§å®¹: {url}")
            async with async_playwright() as pw:
                browser = await pw.chromium.launch(headless=True)
                page = await browser.new_page()
                try:
                    await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                except Exception as e:
                    self.logger.warning(f"ç¬¬ä¸€æ¬¡ goto domcontentloaded å¤±æ•—: {e}")
                    try:
                        await page.goto(url, wait_until="load", timeout=60000)
                    except Exception as e2:
                        self.logger.error(f"ç¬¬äºŒæ¬¡ goto load ä¹Ÿå¤±æ•—: {e2}")
                        await browser.close()
                        return None

                # åˆ¤æ–·æ˜¯å¦ç‚º GitBook/Notion/Docs é€™é¡æœ‰ç›®éŒ„çš„ç¶²ç«™
                is_multi_page = False
                sidebar_selectors = [
                    'nav.toc a',           # GitBook
                    'nav[aria-label="Table of contents"] a',
                    'aside a',             # Notion/Docs
                    '.sidebar a',
                    '.menu a',
                    '.toc a',
                    'nav a',
                ]
                all_links = set()
                for sel in sidebar_selectors:
                    try:
                        links = await page.query_selector_all(sel)
                        for link in links:
                            href = await link.get_attribute('href')
                            if href and not href.startswith('#') and not href.startswith('javascript:'):
                                # çµ±ä¸€è£œå…¨ç›¸å°è·¯å¾‘
                                if href.startswith('/') and url.startswith('http'):
                                    from urllib.parse import urljoin
                                    href = urljoin(url, href)
                                elif href.startswith('http'):
                                    pass
                                else:
                                    continue
                                all_links.add(href)
                        if len(all_links) > 3:
                            is_multi_page = True
                            self.logger.info(f"åµæ¸¬åˆ°å¤šåˆ†é ç›®éŒ„ selector: {sel}ï¼Œå…± {len(all_links)} å€‹åˆ†é ")
                            break
                    except Exception:
                        continue

                all_links = list(all_links)
                if url not in all_links:
                    all_links = [url] + all_links

                all_contents = []
                if is_multi_page:
                    # éæ­·æ‰€æœ‰åˆ†é 
                    for idx, link in enumerate(all_links):
                        try:
                            self.logger.info(f"[å¤šåˆ†é ] æŠ“å–ç¬¬{idx+1}/{len(all_links)}é : {link}")
                            await page.goto(link, wait_until="domcontentloaded", timeout=30000)
                            await page.wait_for_timeout(1000)
                            # æ»¾å‹•åˆ°åº•éƒ¨
                            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                            await page.wait_for_timeout(1000)
                            await page.evaluate('window.scrollTo(0, 0)')
                            await page.wait_for_timeout(500)
                            html = await page.content()
                            soup = BeautifulSoup(html, "html.parser")
                            title = soup.title.string.strip() if soup.title else ""
                            main = soup.find("main") or soup.find("article") or soup.body
                            text_blocks = []
                            if main:
                                for tag in main.find_all(["h1", "h2", "h3", "h4", "h5", "h6", "p", "li"]):
                                    txt = tag.get_text(strip=True)
                                    if txt and len(txt) > 10:
                                        text_blocks.append(txt)
                            if len(text_blocks) < 5:
                                for tag in soup.find_all(["h1", "h2", "h3", "h4", "h5", "h6", "p", "li", "span", "div"]):
                                    txt = tag.get_text(strip=True)
                                    if txt and len(txt) > 20:
                                        text_blocks.append(txt)
                            content = f"[åˆ†é : {title or link}]:\n" + "\n".join(text_blocks)
                            if content.strip():
                                all_contents.append(content)
                        except Exception as e:
                            self.logger.warning(f"[å¤šåˆ†é ] æŠ“å– {link} å¤±æ•—: {e}")
                    await browser.close()
                    merged_content = "\n\n".join(all_contents)[:12000]  # é™åˆ¶é•·åº¦
                    if not merged_content or len(merged_content) < 100:
                        self.logger.warning("å¤šåˆ†é æŠ“å–å¾Œä»ç„¡æœ‰æ•ˆå…§å®¹")
                        return None
                    return merged_content
                else:
                    # å–®é ç¶²ç«™ç¶­æŒåŸæœ¬ç­–ç•¥
                    # 1. å˜—è©¦ç­‰å¾…å¸¸è¦‹å…§å®¹ selector
                    selectors = [
                        'main', 'article', 'section', '.content', '.main', '.article', '#content', '#main', '#app', '#root'
                    ]
                    found = False
                    for sel in selectors:
                        try:
                            await page.wait_for_selector(sel, timeout=3000)
                            found = True
                            self.logger.info(f"ç­‰å¾… selector æˆåŠŸ: {sel}")
                            break
                        except Exception:
                            continue
                    # 2. è‡ªå‹•é»æ“Šå±•é–‹/æ›´å¤šæŒ‰éˆ•
                    expand_selectors = [
                        'button:has-text("å±•é–‹")', 'button:has-text("æ›´å¤š")', 'button:has-text("Read more")',
                        'button:has-text("Show more")', 'a:has-text("å±•é–‹")', 'a:has-text("æ›´å¤š")',
                        'a:has-text("Read more")', 'a:has-text("Show more")'
                    ]
                    for sel in expand_selectors:
                        try:
                            btns = await page.query_selector_all(sel)
                            for btn in btns:
                                await btn.click()
                                self.logger.info(f"è‡ªå‹•é»æ“Šå±•é–‹/æ›´å¤šæŒ‰éˆ•: {sel}")
                                await page.wait_for_timeout(500)
                        except Exception:
                            continue
                    # 3. æ»¾å‹•åˆ°åº•éƒ¨ï¼Œç¢ºä¿å‹•æ…‹å…§å®¹è¼‰å…¥
                    await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                    await page.wait_for_timeout(2000)
                    await page.evaluate('window.scrollTo(0, 0)')
                    await page.wait_for_timeout(1000)
                    # 4. å˜—è©¦å¤šç¨®æ–¹å¼æå–å…§å®¹
                    html = await page.content()
                    soup = BeautifulSoup(html, "html.parser")
                    title = soup.title.string.strip() if soup.title else ""
                    meta_desc = soup.find("meta", attrs={"name": "description"})
                    desc = meta_desc["content"].strip() if meta_desc and meta_desc.get("content") else ""
                    text_blocks = []
                    main = soup.find("main") or soup.find("article") or soup.body
                    if main:
                        for tag in main.find_all(["h1", "h2", "h3", "h4", "h5", "h6", "p", "li"]):
                            txt = tag.get_text(strip=True)
                            if txt and len(txt) > 10:
                                text_blocks.append(txt)
                    if len(text_blocks) < 5:
                        for tag in soup.find_all(["h1", "h2", "h3", "h4", "h5", "h6", "p", "li", "span", "div"]):
                            txt = tag.get_text(strip=True)
                            if txt and len(txt) > 20:
                                text_blocks.append(txt)
                    if len(text_blocks) < 3:
                        try:
                            inner_text = await page.evaluate('document.body.innerText')
                            if inner_text:
                                for line in inner_text.splitlines():
                                    line = line.strip()
                                    if len(line) > 20:
                                        text_blocks.append(line)
                                self.logger.info("å·²æŠ“å– body.innerText")
                        except Exception as e:
                            self.logger.warning(f"æŠ“å– body.innerText å¤±æ•—: {e}")
                    if len(text_blocks) < 3:
                        try:
                            text_content = await page.evaluate('document.body.textContent')
                            if text_content:
                                for line in text_content.splitlines():
                                    line = line.strip()
                                    if len(line) > 20:
                                        text_blocks.append(line)
                                self.logger.info("å·²æŠ“å– body.textContent")
                        except Exception as e:
                            self.logger.warning(f"æŠ“å– body.textContent å¤±æ•—: {e}")
                    await browser.close()
                    content = "\n".join(text_blocks)[:6000]  # é™åˆ¶é•·åº¦
                    if not content or len(content) < 100:
                        self.logger.warning("Playwright æ²’æœ‰æŠ“åˆ°æœ‰æ•ˆå…§å®¹ï¼Œè©²ç¶²ç«™å¯èƒ½éœ€ç™»å…¥æˆ–æœ‰é˜²çˆ¬èŸ²æªæ–½")
                        return None
                    return content
        except Exception as e:
            self.logger.error(f"Playwrightå…§å®¹æ“·å–æµç¨‹å¤±æ•—: {str(e)}")
            return None


async def ocr_images_from_urls(image_urls: List[str]) -> str:
    """ä¸‹è¼‰åœ–ç‰‡ä¸¦åŸ·è¡Œ OCR"""
    from pytesseract import pytesseract
    results = []

    for i, url in enumerate(image_urls):
        try:
            # è™•ç† base64 åœ–ç‰‡
            if url.startswith('data:image/'):
                try:
                    import base64
                    # å–å¾— base64 ç·¨ç¢¼éƒ¨åˆ†
                    header, encoded = url.split(",", 1)
                    img_data = base64.b64decode(encoded)
                    img = Image.open(BytesIO(img_data))
                except Exception as e:
                    logger.warning(f"âŒ ç„¡æ³•è§£æ base64 åœ–ç‰‡: {str(e)}")
                    continue
            # è™•ç†ä¸€èˆ¬ URL
            elif url.startswith("http"):
                response = requests.get(url)
                img = Image.open(BytesIO(response.content))
            else:
                logger.warning(f"âŒ ä¸æ”¯æ´çš„åœ–ç‰‡ URL æ ¼å¼: {url[:100]}...")
                continue

            # ä½¿ç”¨ UTF-8 ç·¨ç¢¼è™•ç†æ–‡å­—ï¼Œæ·»åŠ  Tesseract éŒ¯èª¤è™•ç†
            try:
                text = pytesseract.image_to_string(img, lang='eng')
                if text and text.strip():
                    # ç¢ºä¿æ–‡å­—ä½¿ç”¨ UTF-8 ç·¨ç¢¼
                    text_encoded = text.encode('utf-8', errors='ignore').decode('utf-8')
                    results.append(f"[Slide {i+1}]\n{text_encoded}")
            except Exception as ocr_e:
                # è™•ç† Tesseract ç›¸é—œéŒ¯èª¤
                if "TesseractNotFoundError" in str(type(ocr_e)) or "tesseract" in str(ocr_e).lower():
                    logger.warning(f"âš ï¸ Tesseract OCR ä¸å¯ç”¨ï¼Œè·³éç¬¬ {i+1} å¼µåœ–ç‰‡çš„æ–‡å­—æå–: {str(ocr_e)}")
                    results.append(f"[Slide {i+1}]\n[OCRä¸å¯ç”¨ - ç„¡æ³•æå–æ–‡å­—å…§å®¹]")
                else:
                    # å…¶ä»– OCR éŒ¯èª¤
                    logger.warning(f"âŒ OCR è™•ç†å¤±æ•— (ç¬¬ {i+1} å¼µ): {str(ocr_e)}")
                    results.append(f"[Slide {i+1}]\n[æ–‡å­—æå–å¤±æ•—]")
                
        except Exception as e:
            logger.warning(f"âŒ è®€å–åœ–ç‰‡å¤±æ•—: {str(e)}", exc_info=True)

    return "\n\n".join(results)

async def extract_company_name_from_message(message: str) -> Optional[str]:
    """å¾æ¶ˆæ¯ä¸­æå–å…¬å¸åç¨±"""
    try:
        # 1. æª¢æŸ¥æ˜¯å¦æœ‰æ˜ç¢ºçš„å…¬å¸åç¨±æ¨™è¨˜
        company_patterns = [
            r'(?:^|\n|\s)(?:company|project|startup):\s*([^\n]+)',
            r'(?:^|\n|\s)(?:company|project|startup)\s*name:\s*([^\n]+)',
            r'(?:^|\n|\s)(?:company|project|startup)\s*name\s*is\s*([^\n]+)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:a|an)\s*(?:company|startup|project)',
            r'(?:^|\n|\s)(?:about|introducing|presenting)\s+([A-Z][A-Za-z0-9\s&]+)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:also|working|building)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:governance|related)',
        ]
        
        for pattern in company_patterns:
            match = re.search(pattern, message, re.IGNORECASE | re.MULTILINE)
            if match:
                company_name = match.group(1).strip()
                # æ¸…ç†å…¬å¸åç¨±
                company_name = re.sub(r'\s+', ' ', company_name)  # ç§»é™¤å¤šé¤˜ç©ºæ ¼
                company_name = re.sub(r'[^\w\s&-]', '', company_name)  # åªä¿ç•™å­—æ¯ã€æ•¸å­—ã€ç©ºæ ¼ã€&å’Œ-
                # å¦‚æœå…¬å¸åç¨±å¤ªé•·ï¼Œå¯èƒ½ä¸æ˜¯æ­£ç¢ºçš„åŒ¹é…
                if len(company_name.split()) <= 3:  # å‡è¨­å…¬å¸åç¨±ä¸è¶…é3å€‹å–®è©
                    return company_name
        
        # 2. æª¢æŸ¥æ˜¯å¦æœ‰ "about" æˆ– "introducing" å¾Œé¢çš„åç¨±
        about_patterns = [
            r'(?:^|\n|\s)(?:about|introducing|presenting)\s+([A-Z][A-Za-z0-9\s&]+)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:a|an)\s*(?:company|startup|project)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:also|working|building)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:governance|related)',
        ]
        
        for pattern in about_patterns:
            match = re.search(pattern, message, re.IGNORECASE | re.MULTILINE)
            if match:
                company_name = match.group(1).strip()
                company_name = re.sub(r'\s+', ' ', company_name)
                company_name = re.sub(r'[^\w\s&-]', '', company_name)
                if len(company_name.split()) <= 3:
                    return company_name
        
        # 3. æª¢æŸ¥æ˜¯å¦æœ‰ "Blurb" æˆ– "Description" å¾Œé¢çš„åç¨±
        blurb_patterns = [
            r'(?:^|\n|\s)(?:blurb|description):\s*([^\n]+)',
            r'(?:^|\n|\s)(?:blurb|description)\s*is\s*([^\n]+)',
        ]
        
        for pattern in blurb_patterns:
            match = re.search(pattern, message, re.IGNORECASE | re.MULTILINE)
            if match:
                text = match.group(1).strip()
                # å˜—è©¦å¾æè¿°ä¸­æå–å…¬å¸åç¨±
                company_match = re.search(r'([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:a|an)\s*(?:company|startup|project)', text)
                if company_match:
                    company_name = company_match.group(1).strip()
                    company_name = re.sub(r'\s+', ' ', company_name)
                    company_name = re.sub(r'[^\w\s&-]', '', company_name)
                    if len(company_name.split()) <= 3:
                        return company_name
        
        # 4. ç‰¹æ®Šæƒ…æ³ï¼šç›´æ¥æŸ¥æ‰¾å¤§å¯«é–‹é ­çš„å–®è©
        words = message.split()
        for i, word in enumerate(words):
            if word[0].isupper() and len(word) > 1 and word.isalpha():
                # æª¢æŸ¥é€™å€‹è©æ˜¯å¦å¯èƒ½æ˜¯å…¬å¸åç¨±
                if i > 0 and words[i-1].lower() in ['about', 'introducing', 'presenting', 'company', 'project', 'startup']:
                    return word
                if i < len(words)-1 and words[i+1].lower() in ['is', 'are', 'working', 'building']:
                    return word
        
        return None
    except Exception as e:
        logger.error(f"æå–å…¬å¸åç¨±æ™‚å‡ºéŒ¯: {e}")
        return None

async def summarize_pitch_deck(ocr_text: str, message: str = "") -> Dict:
    """ç›´æ¥è¿”å› OCR æ–‡å­—çµæœ"""
    # ç¢ºä¿è¼¸å…¥æ–‡å­—æ˜¯ UTF-8 ç·¨ç¢¼
    if isinstance(ocr_text, str):
        ocr_text = ocr_text.encode('utf-8', errors='ignore').decode('utf-8')

    # ç›´æ¥è¿”å› OCR çµæœ
    return {
        "raw_content": ocr_text,
        "url": "",  # æ·»åŠ ç©ºçš„ URL å­—æ®µä»¥é¿å…åºåˆ—åŒ–å•é¡Œ
        "error": None  # æ·»åŠ ç©ºçš„ error å­—æ®µä»¥é¿å…åºåˆ—åŒ–å•é¡Œ
    }

async def debug_all_iframes(page):
    """
    å„²å­˜æ‰€æœ‰ iframe çš„ HTML ä¸¦åˆ—å‡ºé—œéµè³‡è¨Šï¼ˆæ–‡å­—ã€åœ–ç‰‡ã€canvas æ•¸é‡ï¼‰ã€‚
    """
    frames = page.frames
    print(f"\nğŸ” æª¢æ¸¬åˆ° {len(frames)} å€‹ iframeã€‚")

    for idx, frame in enumerate(frames):
        print(f"\nğŸ“¦ è§£æ iframe [{idx}] URL: {frame.url}")

        try:
            # å„²å­˜ HTML
            html = await frame.content()
            from utils.path_helper import PathHelper as _PH
            _ph = _PH()
            file_path = _ph.get(f"debug_frame_{idx}.html")
            with file_path.open("w", encoding="utf-8") as f:
                f.write(html)
            print(f"ğŸ“ å·²å„²å­˜ HTML åˆ° {file_path}")

            # æŠ“ä¸»è¦å…ƒç´ æ•¸é‡
            text_count = await frame.eval_on_selector_all('p', 'els => els.length')
            div_count = await frame.eval_on_selector_all('div', 'els => els.length')
            img_count = await frame.eval_on_selector_all('img', 'els => els.length')
            canvas_count = await frame.eval_on_selector_all('canvas', 'els => els.length')

            print(f"ğŸ“Š å…§å®¹çµ±è¨ˆï¼š")
            print(f" - <p> æ¨™ç±¤ï¼š{text_count}")
            print(f" - <div> æ¨™ç±¤ï¼š{div_count}")
            print(f" - <img> åœ–ç‰‡ï¼š{img_count}")
            print(f" - <canvas> å…ƒç´ ï¼š{canvas_count}")

            # é¡¯ç¤ºå¯èƒ½æ€§åˆ†æ
            if canvas_count > 0:
                print("âš ï¸ ç™¼ç¾ canvasï¼Œå¯èƒ½æ˜¯åœ–åƒæ¸²æŸ“çš„ slideã€‚")
            if text_count > 5 or div_count > 20:
                print("âœ… å¯èƒ½æœ‰æ–‡å­—å…§å®¹ã€‚")
            if img_count > 5 and text_count < 2:
                print("ğŸ–¼ï¸ å¾ˆå¯èƒ½æ˜¯ç´”åœ–ç‰‡å‹æŠ•å½±ç‰‡ã€‚")

        except Exception as e:
            print(f"âŒ è®€å– iframe {idx} ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
    

#æ¸¬è©¦ç”¨é©…å‹•å™¨
if __name__ == "__main__":

    async def main():
        message = """
        TrueNorth
        Crypto's first AI discovery engine that uses agentic technology to unlock a symbiotic user journey - from intent straight to outcome

        Co-founders

        Willy: Serial entrepreneur with a successful M&A exit (Series-B SaaS startup), Forbes 30 Under 30 China, ex-COO and acting CEO of WOO.

        Alex: PhD in AI & Domain-Specific Computing, ex-McKinsey, ex-Temasek, Head of Product, Strategy and Capital Market at Enflame (~USD3b pre-IPO AI chip startup), and the Tech Founding Partner of Iluvatar (~USD2b pre-IPO AI chip startup).

        Backed by
        Cyber Fund, Delphi Labs and founders, GPs from Layerzero, Virtuals, Selini, SEI, Merlin, Presto, LTP, Initial, Generative and more.

        Deck
        https://docsend.com/v/w7g7p/truenorth-pitch-deck-seed

        Website
        https://true-north.xyz/\
        """
        
        reader = DeckBrowser()
        try:
            await reader.initialize()
            results = await reader.process_input(message)
            if results:
                print(json.dumps(results, ensure_ascii=False, indent=2))
            else:
                print("âš ï¸ æ²’æœ‰æ“·å–åˆ°ä»»ä½•çµæœ")
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            await reader.close()
            # ç¢ºä¿æ‰€æœ‰è³‡æºéƒ½è¢«é‡‹æ”¾
            import gc
            gc.collect()

    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("Bot stopped by user.")
    except Exception as e:
        print(f"Fatal error: {e}")
    finally:
        # ç¢ºä¿æ‰€æœ‰è³‡æºéƒ½è¢«é‡‹æ”¾
        import gc
        gc.collect()