import os
from io import BytesIO  # ç§»åˆ°æœ€ä¸Šé¢
import logging
from dotenv import load_dotenv
from utils.path_helper import PathHelper
import re
import asyncio
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page
from typing import Optional, List, Dict, Literal, Any
import random
from PIL import Image
import pytesseract
import requests
from openai import AsyncOpenAI
import json
import fitz  # PyMuPDF for PDF
import tempfile
from pptx import Presentation

# Load environment variables
load_dotenv()

# Pytesseract Path
pytesseract.pytesseract.tesseract_cmd = os.getenv('TESSERACT')


# é…ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)
print("OPENAI_API_KEY:", os.getenv("OPENAI_API_KEY"))
# 5. é…ç½® Tesseract è·¯å¾‘
tesseract_path = os.getenv('TESSERACT')

logger.info(f"Final Tesseract path: {pytesseract.pytesseract.tesseract_cmd}")

class DeckBrowser:
    """DocSend æ–‡æª”è®€å–å™¨"""
    
    def __init__(self):
        """Initialize the DeckBrowser."""
        self.browser = None
        self.logger = logging.getLogger(__name__)
        self.email = os.getenv("DOCSEND_EMAIL")  # æ›¿æ›ç‚ºæ‚¨çš„é›»å­éƒµä»¶
        self.path_helper = PathHelper()

    #æ±ºå®šæµç¨‹
    SourceType = Literal["docsend", "attachment", "gdrive", "unknown"]
    
    @staticmethod
    def detect_source_type(message: str, attachments: Optional[list] = None) -> "DeckBrowser.SourceType":
        if "docsend.com" in message.lower():
            return "docsend"
        elif attachments and any(
            att.get("name", "").lower().endswith(('.pdf', '.pptx', '.ppt'))
            for att in attachments if isinstance(att, dict)
        ):
            return "attachment"
        elif re.search(r"https://(?:drive|docs)\.google\.com/(?:file/d/|presentation/)[\w\-/]+", message):
            return "gdrive"
        else:
            return "unknown"
    
    async def process_input(self, message: str, attachments: Optional[list] = None):
        source_type = self.detect_source_type(message, attachments)

        if source_type == "docsend":
            self.logger.info(f"é–‹å§‹è™•ç† Docsend")
            return await self.run_docsend_analysis(message)
        elif source_type == "attachment":
            self.logger.info(f"é–‹å§‹è™•ç† Attachment")
            return await self.run_file_analysis(attachments)
        elif source_type == "gdrive":
            self.logger.info(f"é–‹å§‹è™•ç† Google Drive")
            return await self.run_gdrive_analysis(message)
        else:
            # æ–°å¢ï¼šå…è¨±ç´”æ–‡å­—ç›´æ¥ä¸Ÿçµ¦ GPT
            self.logger.info("æœªåµæ¸¬åˆ°é€£çµæˆ–é™„ä»¶ï¼Œç›´æ¥åˆ†æç´”æ–‡å­—å…§å®¹")
            summary = await summarize_pitch_deck(message)
            return [summary] if summary else [{"error": "âŒ ç´”æ–‡å­—åˆ†æå¤±æ•—"}]

    async def run_gdrive_analysis(self, message: str) -> Dict[str, Any]:
        self.logger.info(f"ğŸ“¥ é–‹å§‹è™•ç† Google Drive é€£çµ: {message}")

        gdrive_match = re.search(r"https://drive\.google\.com/file/d/([\w-]+)", message)
        if not gdrive_match:
            gdrive_match = re.search(r"id=([\w-]+)", message)

        if not gdrive_match:
            return {"error": "âŒ ç„¡æ³•å¾è¨Šæ¯ä¸­æ“·å– Google Drive æª”æ¡ˆ IDã€‚è«‹ç¢ºèªé€£çµæ ¼å¼ã€‚"}

        file_id = gdrive_match.group(1)
        export_url = f"https://drive.google.com/uc?export=download&id={file_id}"

        try:
            response = requests.get(export_url)
            response.raise_for_status()

            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(response.content)
                tmp_path = tmp.name

            self.logger.info(f"ğŸ“„ æˆåŠŸä¸‹è¼‰ PDF æª”æ¡ˆè‡³ {tmp_path}ï¼Œé–‹å§‹åŸ·è¡Œåˆ†æ...")

            return await self.run_file_analysis([{"path": tmp_path, "name": "gdrive.pdf"}])

        except Exception as e:
            self.logger.error(f"âŒ ä¸‹è¼‰æˆ–è™•ç† Google Drive æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            return {"error": f"âŒ Google Drive æª”æ¡ˆè™•ç†å¤±æ•—ï¼š{str(e)}"}
            
    async def run_file_analysis(self, attachments: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """
        è™•ç† PDF/PPTX é™„ä»¶ï¼šåŸ·è¡Œ OCR æˆ–çµæ§‹åŒ–æ‘˜è¦
        """
        results = []

        for file in attachments:
            path = file.get("path")
            name = file.get("name", "unnamed")
            suffix = self.path_helper.get(name).suffix.lower()

            self.logger.info(f"ğŸ“‚ é–‹å§‹åˆ†æé™„ä»¶: {name}")

            extracted_text = ""

            try:
                # Check file size before processing
                file_path_obj = self.path_helper.get(path)
                if not file_path_obj.exists() or file_path_obj.stat().st_size < 1024:
                    self.logger.error(f"âŒ æª”æ¡ˆ {name} ({path}) å¤ªå°æˆ–ä¸å­˜åœ¨ï¼Œå¯èƒ½ä¸‹è¼‰å¤±æ•—ã€‚")
                    results.append({"error": f"âŒ æª”æ¡ˆ {name} ä¸‹è¼‰å¤±æ•—æˆ–ä¸æ˜¯æœ‰æ•ˆçš„æª”æ¡ˆã€‚"})
                    continue

                if suffix == ".pdf":
                    doc = fitz.open(str(file_path_obj))
                    for page in doc:
                        extracted_text += page.get_text("text") + "\n"
                    doc.close()

                elif suffix == ".pptx":
                    try:
                        prs = Presentation(str(file_path_obj))
                    except Exception as e:
                        self.logger.error(f"âŒ ç„¡æ³•é–‹å•Ÿ PPTX æª”æ¡ˆ {name}: {type(e).__name__}: {e}")
                        results.append({"error": f"âŒ ç„¡æ³•é–‹å•Ÿ PPTX æª”æ¡ˆ {name}: {type(e).__name__}: {e}"})
                        continue
                    for i, slide in enumerate(prs.slides):
                        for shape in slide.shapes:
                            if hasattr(shape, "text"):
                                extracted_text += f"[Slide {i+1}]\n{shape.text}\n"

                if not extracted_text.strip():
                    self.logger.warning(f"âš ï¸ {name} æ²’æœ‰æ“·å–åˆ°æ–‡å­—ï¼Œé–‹å§‹åŸ·è¡Œ OCR åœ–ç‰‡è¾¨è­˜")

                    image_urls = []
                    if suffix == ".pdf":
                        doc = fitz.open(str(file_path_obj))
                        for page_num, page in enumerate(doc):
                            pix = page.get_pixmap(dpi=200)
                            img_path_obj = self.path_helper.get(f"{file_path_obj}_page_{page_num}.png")
                            pix.save(str(img_path_obj))
                            image_urls.append(f"file://{img_path_obj}")
                        doc.close()

                    elif suffix == ".pptx":
                        self.logger.warning("âŒ å°šæœªå¯¦ä½œ PPTX é é¢è½‰åœ–ç‰‡çš„ OCR fallback")
                        continue

                    if image_urls:
                        ocr_text = await ocr_images_from_urls(image_urls)
                        if ocr_text:
                            summary = await summarize_pitch_deck(ocr_text)
                            if summary:
                                results.append(summary)
                                continue

                else:
                    summary = await summarize_pitch_deck(extracted_text)
                    if summary:
                        results.append(summary)

            except Exception as e:
                self.logger.error(f"âŒ åˆ†ææª”æ¡ˆ {name} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                results.append({"error": f"âŒ åˆ†æå¤±æ•—: {name}"})

        return results if results else [{"error": "âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•é™„ä»¶å…§å®¹"}]

    #ä¸»è¦ Docsend é©…å‹•å¼
    async def run_docsend_analysis(self, message: str) -> Dict[str, Any]:
        """
        å°è£å®Œæ•´æµç¨‹ï¼šåˆå§‹åŒ– -> æ“·å– URL -> æŠ½å–å…§å®¹ -> é—œé–‰ç€è¦½å™¨ -> å›å‚³çµæœ
        """
        await self.initialize()
        
        results = []  # List to store JSON results
        urls = await self.extract_docsend_links(message)
        for url in urls:
            content = await self.read_docsend_document(url)
            if isinstance(content, dict):
                results.append(content)
            elif isinstance(content, str):
                summarized = await summarize_pitch_deck(content)
                if summarized:
                    results.append(summarized)

        await self.close()
        
        return results if results else [{"error": "âŒ æ²’æœ‰æˆåŠŸæ“·å–ä»»ä½• DocSend æ–‡æª”å…§å®¹"}]

    async def initialize(self):
        """Initialize the browser instance asynchronously."""
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(headless=False)
            self.logger.info("Browser initialized successfully")
        except Exception as e:
            self.logger.error(f"Failed to initialize browser: {str(e)}")
            raise

    async def close(self):
        """é—œé–‰ç€è¦½å™¨"""
        if self.browser:
            await self.browser.close()
            self.browser = None
        if hasattr(self, 'playwright'):
            await self.playwright.stop()
    
    async def extract_docsend_links(self, text: str) -> List[str]:
        """å¾æ–‡æœ¬ä¸­æå– DocSend é€£çµ"""
        docsend_pattern = r'https?://(?:www\.)?docsend\.com/[^\s)"}]+'
        return re.findall(docsend_pattern, text)
    
    async def _get_page(self, url: str) -> Page:
        """Get a new page with configured User-Agent and other settings."""
        page = await self.browser.new_page()
        await page.set_viewport_size({'width': 1920, 'height': 1080})
        await page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'sec-ch-ua': '"Chromium";v="122", "Not(A:Brand";v="24", "Google Chrome";v="122"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"'
        })
        
        return page

    async def read_docsend_document(self, url: str) -> Optional[str]:
        """è®€å– DocSend æ–‡æª”çš„å…§å®¹"""
        try:
            self.logger.info(f"æ­£åœ¨å¾ DocSend é€£çµè®€å–å…§å®¹: {url}")
            
            # å‰µå»ºæ–°é é¢
            page = await self._get_page(url)
            
            # è¨ªå• DocSend é é¢
            response = await page.goto(url, wait_until='networkidle', timeout=30000)
            
            self.logger.info(f"è¨ªå•ç‹€æ…‹ç¢¼: {response.status}")
            
            if response.status == 403:
                self.logger.error("è¨ªå•è¢«æ‹’çµ• (403 Forbidden)")
                await page.close()
                return None
            
            # éš¨æ©Ÿç­‰å¾… 2-5 ç§’
            await asyncio.sleep(random.uniform(2, 5))
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦å¡«å¯«é›»å­éƒµä»¶
            email_input = await page.query_selector('input[type="email"]')
            if email_input:
                self.logger.info("éœ€è¦å¡«å¯«é›»å­éƒµä»¶æ‰èƒ½è¨ªå•æ–‡æª”")
                await page.type('input[type="email"]', self.email, delay=random.uniform(100, 200))
                try:
                    self.logger.info("å˜—è©¦é»æ“Šæäº¤æŒ‰éˆ•")
                    await page.locator('button:has-text("Continue")').wait_for(state='visible', timeout=1000)
                    await page.locator('button:has-text("Continue")').click(timeout=1000)
                except Exception as e:
                    self.logger.warning(f"æäº¤æŒ‰éˆ•é»æ“Šå¤±æ•—: {e}")
                await page.wait_for_load_state('networkidle', timeout=30000)
                
            # å˜—è©¦åˆ¤æ–·æ˜¯å¦æˆåŠŸç™»å…¥ DocSend æ–‡ä»¶ï¼ˆåŠ å…¥æ›´å¤š selector fallbackï¼‰
            success_selectors = [
                '.document-content',
                '.viewer-content',
                '.page',
                '.ds-viewer-container',
            ]

            successfully_entered = False
            for selector in success_selectors:
                try:
                    if await page.query_selector(selector):
                        self.logger.info(f"âœ… æˆåŠŸé€²å…¥ DocSend æ–‡ä»¶ï¼Œåµæ¸¬åˆ° selector: {selector}")
                        successfully_entered = True
                        break
                except Exception as e:
                    self.logger.warning(f"âŒ æª¢æŸ¥ selector {selector} ç™¼ç”ŸéŒ¯èª¤: {e}")

            if not successfully_entered:
                self.logger.warning("âš ï¸ æ²’æœ‰åµæ¸¬åˆ°ä»»ä½• DocSend æ–‡ä»¶å…§å®¹çš„ selectorï¼Œå¯èƒ½ç™»å…¥å¤±æ•—ã€‚")

            # å†æ¬¡éš¨æ©Ÿç­‰å¾…ï¼ˆè®“ç¶²é æœ‰æ©Ÿæœƒç¹¼çºŒè¼‰å…¥ï¼‰
            await asyncio.sleep(random.uniform(2, 5))
            
            # æˆªåœ–ä»¥ä¾¿èª¿è©¦
            debug_screenshot_name = f"docsend_debug_{random.randint(1000, 9999)}.png"
            debug_screenshot_path = str(self.path_helper.get("tmp", debug_screenshot_name))
            self.path_helper.ensure_dir("tmp")
            await page.screenshot(path=debug_screenshot_path)
            self.logger.info(f"ä¿å­˜é é¢æˆªåœ–è‡³: {debug_screenshot_path}")
            
            # Debug å°‡æ•´é  HTML å„²å­˜ä¸‹ä¾†
            html = await page.content()
            debug_html_path = self.path_helper.get("debug_docsend.html")
            with debug_html_path.open("w", encoding="utf-8") as f:
                f.write(html)

            # Debug é¡¯ç¤ºç›®å‰é é¢ä¸Šçš„æ‰€æœ‰ iframeï¼ˆè‹¥æœ‰ï¼‰
            for frame in page.frames:
                print(f"[iframe] name: {frame.name}, url: {frame.url}")
            
            # ç­‰å¾…æ–‡æª”å…§å®¹åŠ è¼‰
            # å˜—è©¦å¾æ‰€æœ‰ iframe ä¸­æ‰¾åˆ°å…§å®¹
            target_frame = None
            content_selectors = ['.document-content', '.page', '.viewer-content', '.ds-viewer-container']

            for frame in page.frames:
                frame_url = frame.url or ""
                # æ ¹æ“š URL åˆ¤æ–·æ˜¯ä¸» DocSend iframeï¼Œè€Œä¸æ˜¯ dropbox æˆ– intercom ç­‰é›œè¨Š
                if "docsend.com/view" in frame_url and "marketing.docsend.com" not in frame_url:
                    for selector in content_selectors:
                        try:
                            await frame.wait_for_selector(selector, timeout=5000)
                            self.logger.info(f"åœ¨ iframe ä¸­æ‰¾åˆ°å…§å®¹: {selector}")
                            target_frame = frame
                            break
                        except:
                            continue
                if target_frame:
                    break

                if not target_frame:
                    self.logger.warning("âš ï¸ æœªèƒ½æ‰¾åˆ°å«æ–‡å­— selectorï¼Œé–‹å§‹ debug æ‰€æœ‰ iframe...")
                    await debug_all_iframes(page)

                    # å˜—è©¦å¾æ‰€æœ‰ iframe ä¸­æ“·å–åœ–ç‰‡åš OCR
                    for frame in page.frames:
                        try:
                            frame_html = await frame.content()
                            soup = BeautifulSoup(frame_html, "html.parser")
                            img_tags = soup.find_all("img")
                            image_urls = [img["src"] for img in img_tags if img.get("src")]
                            
                            if image_urls:
                                self.logger.info(f"âœ… åœ¨ iframe ä¸­æ‰¾åˆ°åœ–ç‰‡ï¼šå…± {len(image_urls)} å¼µï¼Œæº–å‚™åŸ·è¡Œ OCR")
                                ocr_raw_text = await ocr_images_from_urls(image_urls)
                                if ocr_raw_text:
                                    summarized_text = await summarize_pitch_deck(ocr_raw_text)
                                    await page.close()
                                    return summarized_text
                        except Exception as e:
                            self.logger.warning(f"è®€å– iframe å…§å®¹å¤±æ•—ï¼š{e}")

                    self.logger.error("âŒ æ‰€æœ‰ iframe å‡æœªèƒ½æä¾›å…§å®¹æˆ–åœ–ç‰‡")
                    await page.close()
                    return None
                
            # å¾ iframe æŠ“ HTML
            html = await target_frame.content()
            self.logger.info(f"ç²å–çš„HTMLå…§å®¹é•·åº¦: {len(html)} å­—ç¬¦")
            
            # è§£æ HTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # æå–æ–‡æª”æ¨™é¡Œ
            title_elem = soup.find('title')

            # fallbackï¼šå¦‚æœ iframe æ²’æœ‰ titleï¼Œå˜—è©¦å¾ä¸»é  page æ‹¿
            if not title_elem:
                self.logger.info("iframe ä¸­æœªæ‰¾åˆ° <title>ï¼Œå˜—è©¦å¾ä¸»é æŠ“å– title")
                main_html = await page.content()
                main_soup = BeautifulSoup(main_html, 'html.parser')
                title_elem = main_soup.find('title')

            # æœ€çµ‚å–å¾— title
            title = title_elem.text.strip() if title_elem else "DocSend Document"
            self.logger.info(f"æå–çš„æ–‡æª”æ¨™é¡Œ: {title}")
            
            # æå–æ–‡æª”å…§å®¹
            text_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div'])
            self.logger.info(f"æ‰¾åˆ° {len(text_elements)} å€‹æ–‡æœ¬å…ƒç´ ")
            
            extracted_text = []
            
            for elem in text_elements:
                text = elem.get_text().strip()
                if text and len(text) > 10:  # åªä¿ç•™æœ‰æ„ç¾©çš„æ–‡æœ¬
                    extracted_text.append(text)
            
            self.logger.info(f"ç¯©é¸å¾Œæå–äº† {len(extracted_text)} å€‹æœ‰æ„ç¾©çš„æ–‡æœ¬æ®µè½")
            extracted_text = "\n\n".join(extracted_text)
            
            # é—œé–‰é é¢
            await page.close()
            
            if not extracted_text or len(extracted_text) < 100:
                self.logger.warning("âš ï¸ æ–‡å­—å…§å®¹éå°‘ï¼Œå˜—è©¦ OCR åœ–ç‰‡ä¸¦ç”¨ GPT æ‘˜è¦")
                img_tags = soup.find_all('img')
                image_urls = [img['src'] for img in img_tags if img.get('src')]

                if image_urls:
                    ocr_raw_text = await ocr_images_from_urls(image_urls)
                    if ocr_raw_text:
                        summarized_text = await summarize_pitch_deck(ocr_raw_text)
                        extracted_text = summarized_text
            
            # è¨˜éŒ„å…§å®¹æ‘˜è¦
            content_preview = extracted_text[:100] + "..." if len(extracted_text) > 100 else extracted_text
            self.logger.info(f"æå–æ–‡æœ¬é è¦½: {content_preview}")
            self.logger.info(f"ç¸½æå–æ–‡æœ¬é•·åº¦: {len(extracted_text)} å­—ç¬¦")
            
            # æ ¼å¼åŒ–æå–çš„å…§å®¹
            formatted_content = f"--- DocSend æ–‡æª”: {title} ---\n\n{extracted_text}\n\n--- DocSend æ–‡æª”çµæŸ ---"
            
            self.logger.info(f"æˆåŠŸå¾ DocSend é€£çµæå–å…§å®¹: {url}")
            return formatted_content
        
        except Exception as e:
            self.logger.error(f"è®€å– DocSend æ–‡æª”æ™‚å‡ºéŒ¯: {str(e)}", exc_info=True)
            if 'page' in locals():
                await page.close()
            return None

        

#GPT ç¸½çµ        
openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

async def ocr_images_from_urls(image_urls: List[str]) -> str:
    """ä¸‹è¼‰åœ–ç‰‡ä¸¦åŸ·è¡Œ OCR"""
    from pytesseract import pytesseract
    results = []

    for i, url in enumerate(image_urls):
        try:
            # è™•ç† base64 åœ–ç‰‡
            if url.startswith('data:image/'):
                try:
                    import base64
                    # å–å¾— base64 ç·¨ç¢¼éƒ¨åˆ†
                    header, encoded = url.split(",", 1)
                    img_data = base64.b64decode(encoded)
                    img = Image.open(BytesIO(img_data))
                except Exception as e:
                    logger.warning(f"âŒ ç„¡æ³•è§£æ base64 åœ–ç‰‡: {str(e)}")
                    continue
            # è™•ç†ä¸€èˆ¬ URL
            elif url.startswith("http"):
                response = requests.get(url)
                img = Image.open(BytesIO(response.content))
            else:
                logger.warning(f"âŒ ä¸æ”¯æ´çš„åœ–ç‰‡ URL æ ¼å¼: {url[:100]}...")
                continue

            # ä½¿ç”¨ UTF-8 ç·¨ç¢¼è™•ç†æ–‡å­—
            text = pytesseract.image_to_string(img, lang='eng')
            if text and text.strip():
                # ç¢ºä¿æ–‡å­—ä½¿ç”¨ UTF-8 ç·¨ç¢¼
                text_encoded = text.encode('utf-8', errors='ignore').decode('utf-8')
                results.append(f"[Slide {i+1}]\n{text_encoded}")
                
        except Exception as e:
            logger.warning(f"âŒ è®€å–åœ–ç‰‡å¤±æ•—: {str(e)}", exc_info=True)

    return "\n\n".join(results)

async def summarize_pitch_deck(ocr_text: str) -> Dict:
    """ç”¨ GPT æ‘˜è¦ Pitch Deck OCR æ–‡å­—ç‚ºçµæ§‹åŒ–å¤§ç¶±ä¸¦å›å‚³ dict"""
    # ç¢ºä¿è¼¸å…¥æ–‡å­—æ˜¯ UTF-8 ç·¨ç¢¼
    if isinstance(ocr_text, str):
        ocr_text = ocr_text.encode('utf-8', errors='ignore').decode('utf-8')

    # ä½¿ç”¨è‹±æ–‡ prompt é¿å…ç·¨ç¢¼å•é¡Œ
    prompt = f"""
Based on the following Pitch Deck content, create a structured summary.
Return in the following JSON format:
{{
  "company": "company_name",
  "problem": "problem_statement",
  "solution": "solution_statement",
  "business_model": "how they do business",
  "financials": "financials_summary",
  "market": "what's the target market and it's description",
  "funding_team": "founding_team and their background",
}}

Pitch Deck Content:
{ocr_text}
"""

    try:
        completion = await openai_client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": "You are a professional analyst helping investors organize Pitch Deck information."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"}
        )
        raw_output = json.loads(completion.choices[0].message.content)
        logger.info(f"æˆåŠŸæå–Deckä¿¡æ¯")       
        return raw_output
    except Exception as e:
        logger.error(f"GPT è™•ç†å¤±æ•—ï¼š{str(e)}")
        return {
            "error": f"åˆ†æå¤±æ•—: {str(e)}",
            "company": "æœªçŸ¥å…¬å¸",
            "summary": "åˆ†æå¤±æ•—"
        }
    

async def debug_all_iframes(page):
    """
    å„²å­˜æ‰€æœ‰ iframe çš„ HTML ä¸¦åˆ—å‡ºé—œéµè³‡è¨Šï¼ˆæ–‡å­—ã€åœ–ç‰‡ã€canvas æ•¸é‡ï¼‰ã€‚
    """
    frames = page.frames
    print(f"\nğŸ” æª¢æ¸¬åˆ° {len(frames)} å€‹ iframeã€‚")

    for idx, frame in enumerate(frames):
        print(f"\nğŸ“¦ è§£æ iframe [{idx}] URL: {frame.url}")

        try:
            # å„²å­˜ HTML
            html = await frame.content()
            from utils.path_helper import PathHelper as _PH
            _ph = _PH()
            file_path = _ph.get(f"debug_frame_{idx}.html")
            with file_path.open("w", encoding="utf-8") as f:
                f.write(html)
            print(f"ğŸ“ å·²å„²å­˜ HTML åˆ° {file_path}")

            # æŠ“ä¸»è¦å…ƒç´ æ•¸é‡
            text_count = await frame.eval_on_selector_all('p', 'els => els.length')
            div_count = await frame.eval_on_selector_all('div', 'els => els.length')
            img_count = await frame.eval_on_selector_all('img', 'els => els.length')
            canvas_count = await frame.eval_on_selector_all('canvas', 'els => els.length')

            print(f"ğŸ“Š å…§å®¹çµ±è¨ˆï¼š")
            print(f" - <p> æ¨™ç±¤ï¼š{text_count}")
            print(f" - <div> æ¨™ç±¤ï¼š{div_count}")
            print(f" - <img> åœ–ç‰‡ï¼š{img_count}")
            print(f" - <canvas> å…ƒç´ ï¼š{canvas_count}")

            # é¡¯ç¤ºå¯èƒ½æ€§åˆ†æ
            if canvas_count > 0:
                print("âš ï¸ ç™¼ç¾ canvasï¼Œå¯èƒ½æ˜¯åœ–åƒæ¸²æŸ“çš„ slideã€‚")
            if text_count > 5 or div_count > 20:
                print("âœ… å¯èƒ½æœ‰æ–‡å­—å…§å®¹ã€‚")
            if img_count > 5 and text_count < 2:
                print("ğŸ–¼ï¸ å¾ˆå¯èƒ½æ˜¯ç´”åœ–ç‰‡å‹æŠ•å½±ç‰‡ã€‚")

        except Exception as e:
            print(f"âŒ è®€å– iframe {idx} ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
    

#æ¸¬è©¦ç”¨é©…å‹•å™¨
if __name__ == "__main__":

    async def main():
        message = """
        Normie Tech lets your customers pay you in stablecoins without an onramp
        Before this I managed a million dollar grant program for Vitalik and saw the number 1 issue repeatedly holding back web3: sending customers to exchanges where >3/4 give up. We built a solution for our own platform and other projects asked to hire us to do the same.
        Profitable from set up fees by month 4, raising a pre-seed to move faster.
        Here is the deck:
        https://docsend.com/view/sikphsrjbwpz8h82

        """
        
        reader = DeckBrowser()
        await reader.initialize()
        try:
            results = await reader.process_input(message)
            await reader.close()

            if results:
                print(json.dumps(results[0], ensure_ascii=False, indent=2))
            else:
                print("âš ï¸ æ²’æœ‰æ“·å–åˆ°ä»»ä½•çµæœ")

        except Exception as e:
            print(results)
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
            await reader.close()

    asyncio.run(main())
