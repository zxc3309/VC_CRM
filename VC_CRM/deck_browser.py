import os
from io import BytesIO  # ç§»åˆ°æœ€ä¸Šé¢
import logging
from dotenv import load_dotenv
from utils.path_helper import PathHelper
import re
import asyncio
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page
from typing import Optional, List, Dict, Literal, Any
import random
from PIL import Image
import pytesseract
import shutil
import requests
from openai import AsyncOpenAI
import json
import fitz  # PyMuPDF for PDF
import tempfile
from pptx import Presentation
from prompt_manager import GoogleSheetPromptManager

# Load environment variables
load_dotenv(override=True)
print("ğŸ” pytesseract default cmd:", pytesseract.pytesseract.tesseract_cmd)
print("ğŸ” shutil.which('tesseract'):", shutil.which("tesseract"))
# Pytesseract Path
pytesseract.pytesseract.tesseract_cmd = os.getenv('TESSERACT', '/usr/bin/tesseract')

# é…ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)
# 5. é…ç½® Tesseract è·¯å¾‘
tesseract_path = os.getenv('TESSERACT', '/usr/bin/tesseract')

# ç¢ºä¿ logger æœ‰ handler
if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

logger.info(f"Final Tesseract path: {pytesseract.pytesseract.tesseract_cmd}")

logger.setLevel(logging.INFO)

# åœ¨æª”æ¡ˆé–‹é ­åˆå§‹åŒ– prompt_manager
prompt_manager = GoogleSheetPromptManager()

class DeckBrowser:
    
    def __init__(self):
        """Initialize the DeckBrowser."""
        self.browser = None
        self.logger = logging.getLogger(__name__)
        self.email = os.getenv("DOCSEND_EMAIL")  # æ›¿æ›ç‚ºæ‚¨çš„é›»å­éƒµä»¶
        self.path_helper = PathHelper()

    #æ±ºå®šæµç¨‹
    SourceType = Literal["docsend", "attachment", "gdrive", "website", "unknown"]
    
    @staticmethod
    def detect_source_type(message: str, attachments: Optional[list] = None) -> "DeckBrowser.SourceType":
        if "docsend.com" in message.lower():
            return "docsend"
        elif attachments and any(
            att.get("name", "").lower().endswith(('.pdf', '.pptx', '.ppt'))
            for att in attachments if isinstance(att, dict)
        ):
            return "attachment"
        elif re.search(r"https://(?:drive|docs)\.google\.com/(?:file/d/|presentation/)[\w\-/]+", message):
            return "gdrive"
        elif re.search(r"https?://[^\s\)\"]+", message):  # åŒ¹é…ä»»ä½•ç¶²å€
            return "website"
        else:
            return "unknown"
    
    async def process_input(self, message: str, attachments: Optional[list] = None):
        source_type = self.detect_source_type(message, attachments)
        results = []  # ç”¨æ–¼å­˜å„²æ‰€æœ‰çµæœ

        if source_type == "docsend":
            self.logger.info(f"é–‹å§‹è™•ç† Docsend")
            return await self.run_docsend_analysis(message)
        elif source_type == "attachment":
            self.logger.info(f"é–‹å§‹è™•ç† Attachment")
            return await self.run_file_analysis(attachments)
        elif source_type == "gdrive":
            self.logger.info(f"é–‹å§‹è™•ç† Google Drive")
            return await self.run_gdrive_analysis(message)
        elif source_type == "website":
            self.logger.info("ğŸ”— åµæ¸¬ç‚ºä¸€èˆ¬ç¶²ç«™ï¼Œé–‹å§‹æ“·å–ç¶²é å…§å®¹é€²è¡Œåˆ†æ")
            return await self.run_generic_link_analysis(message)
        else:
            self.logger.info(f"Unknown")
            # æ–°å¢ï¼šå…è¨±ç´”æ–‡å­—ç›´æ¥ä¸Ÿçµ¦ GPT
            self.logger.info("æœªåµæ¸¬åˆ°é€£çµæˆ–é™„ä»¶ï¼Œç›´æ¥åˆ†æç´”æ–‡å­—å…§å®¹")
            summary = await summarize_pitch_deck(message, message)  # å‚³å…¥ message å…©æ¬¡ï¼Œä¸€æ¬¡ä½œç‚ºå…§å®¹ï¼Œä¸€æ¬¡ç”¨æ–¼æå–å…¬å¸åç¨±
            return [summary] if summary else [{"error": "âŒ ç´”æ–‡å­—åˆ†æå¤±æ•—"}]

    async def run_docsend_analysis(self, message: str) -> List[Dict[str, Any]]:
        """
        å°è£å®Œæ•´æµç¨‹ï¼šåˆå§‹åŒ– -> æ“·å– URL -> æŠ½å–å…§å®¹ -> é—œé–‰ç€è¦½å™¨ -> å›å‚³çµæœ
        """
        await self.initialize()
        
        results = []  # List to store JSON results
        urls = await self.extract_docsend_links(message)
        
        for url in urls:
            self.logger.info(f"è™•ç† DocSend é€£çµ: {url}")
            content = await self.read_docsend_document(url)
            if isinstance(content, dict):
                results.append(content)
            elif isinstance(content, str):
                summarized = await summarize_pitch_deck(content, message)
                if summarized:
                    results.append(summarized)

        await self.close()
        
        return results if results else [{"error": "âŒ æ²’æœ‰æˆåŠŸæ“·å–ä»»ä½• DocSend æ–‡æª”å…§å®¹"}]

    async def run_gdrive_analysis(self, message: str) -> List[Dict[str, Any]]:
        self.logger.info(f"ğŸ“¥ é–‹å§‹è™•ç† Google Drive é€£çµ")
        results = []

        # æå–æ‰€æœ‰ Google Drive é€£çµ
        gdrive_urls = re.findall(r'https://drive\.google\.com/file/d/([\w-]+)|id=([\w-]+)', message)
        
        for match in gdrive_urls:
            file_id = match[0] or match[1]  # ä½¿ç”¨éç©ºçš„åŒ¹é…çµæœ
            export_url = f"https://drive.google.com/uc?export=download&id={file_id}"
            
            try:
                response = requests.get(export_url)
                response.raise_for_status()

                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                    tmp.write(response.content)
                    tmp_path = tmp.name

                self.logger.info(f"ğŸ“„ æˆåŠŸä¸‹è¼‰ PDF æª”æ¡ˆè‡³ {tmp_path}ï¼Œé–‹å§‹åŸ·è¡Œåˆ†æ...")

                # ä½¿ç”¨ Playwright æ‰“é–‹ PDF æ–‡ä»¶
                await self.initialize()
                page = await self._get_page(f"file://{tmp_path}")
                
                try:
                    # ç­‰å¾… PDF åŠ è¼‰
                    await page.wait_for_load_state('networkidle', timeout=30000)
                    
                    # æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰å…§å®¹
                    self.logger.info("é–‹å§‹æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰å…§å®¹")
                    last_height = await page.evaluate('document.body.scrollHeight')
                    while True:
                        await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                        await page.wait_for_timeout(2000)
                        new_height = await page.evaluate('document.body.scrollHeight')
                        if new_height == last_height:
                            break
                        last_height = new_height
                        self.logger.info("ç¹¼çºŒæ»¾å‹•ä»¥åŠ è¼‰æ›´å¤šå…§å®¹")
                    
                    # æ»¾å‹•å›é ‚éƒ¨
                    await page.evaluate('window.scrollTo(0, 0)')
                    await page.wait_for_timeout(1000)
                    
                    # æå–å…§å®¹
                    content = await page.content()
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # æå–æ–‡æœ¬
                    text_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div'])
                    extracted_text = []
                    
                    for elem in text_elements:
                        text = elem.get_text().strip()
                        if text and len(text) > 10:
                            extracted_text.append(text)
                    
                    if not extracted_text:
                        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ–‡æœ¬ï¼Œå˜—è©¦ OCR
                        self.logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°æ–‡æœ¬å…§å®¹ï¼Œå˜—è©¦ OCR")
                        img_tags = soup.find_all('img')
                        image_urls = [img['src'] for img in img_tags if img.get('src')]
                        
                        if image_urls:
                            ocr_text = await ocr_images_from_urls(image_urls)
                            if ocr_text:
                                summary = await summarize_pitch_deck(ocr_text, message)
                                results.append(summary)
                                continue
                    
                    # æ ¼å¼åŒ–å…§å®¹
                    formatted_content = "\n\n".join(extracted_text)
                    summary = await summarize_pitch_deck(formatted_content, message)
                    if summary:
                        results.append(summary)
                    
                except Exception as e:
                    self.logger.error(f"è™•ç† PDF æ™‚å‡ºéŒ¯: {e}")
                    results.append({"error": f"âŒ è™•ç† PDF å¤±æ•—: {str(e)}"})
                finally:
                    await page.close()
                    await self.close()
                
            except Exception as e:
                self.logger.error(f"âŒ ä¸‹è¼‰æˆ–è™•ç† Google Drive æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
                results.append({"error": f"âŒ Google Drive æª”æ¡ˆè™•ç†å¤±æ•—ï¼š{str(e)}"})
        
        return results if results else [{"error": "âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½• Google Drive æª”æ¡ˆ"}]

    async def run_file_analysis(self, attachments: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """
        è™•ç† PDF/PPTX é™„ä»¶ï¼šåŸ·è¡Œ OCR æˆ–çµæ§‹åŒ–æ‘˜è¦
        """
        results = []

        for file in attachments:
            path = file.get("path")
            name = file.get("name", "unnamed")
            suffix = self.path_helper.get(name).suffix.lower()

            self.logger.info(f"ğŸ“‚ é–‹å§‹åˆ†æé™„ä»¶: {name}")

            extracted_text = ""

            try:
                # Check file size before processing
                file_path_obj = self.path_helper.get(path)
                if not file_path_obj.exists() or file_path_obj.stat().st_size < 1024:
                    self.logger.error(f"âŒ æª”æ¡ˆ {name} ({path}) å¤ªå°æˆ–ä¸å­˜åœ¨ï¼Œå¯èƒ½ä¸‹è¼‰å¤±æ•—ã€‚")
                    results.append({"error": f"âŒ æª”æ¡ˆ {name} ä¸‹è¼‰å¤±æ•—æˆ–ä¸æ˜¯æœ‰æ•ˆçš„æª”æ¡ˆã€‚"})
                    continue

                if suffix == ".pdf":
                    doc = fitz.open(str(file_path_obj))
                    for page in doc:
                        extracted_text += page.get_text("text") + "\n"
                    doc.close()

                elif suffix == ".pptx":
                    try:
                        prs = Presentation(str(file_path_obj))
                    except Exception as e:
                        self.logger.error(f"âŒ ç„¡æ³•é–‹å•Ÿ PPTX æª”æ¡ˆ {name}: {type(e).__name__}: {e}")
                        results.append({"error": f"âŒ ç„¡æ³•é–‹å•Ÿ PPTX æª”æ¡ˆ {name}: {type(e).__name__}: {e}"})
                        continue
                    for i, slide in enumerate(prs.slides):
                        for shape in slide.shapes:
                            if hasattr(shape, "text"):
                                extracted_text += f"[Slide {i+1}]\n{shape.text}\n"

                if not extracted_text.strip():
                    self.logger.warning(f"âš ï¸ {name} æ²’æœ‰æ“·å–åˆ°æ–‡å­—ï¼Œé–‹å§‹åŸ·è¡Œ OCR åœ–ç‰‡è¾¨è­˜")

                    image_urls = []
                    if suffix == ".pdf":
                        doc = fitz.open(str(file_path_obj))
                        for page_num, page in enumerate(doc):
                            pix = page.get_pixmap(dpi=200)
                            img_path_obj = self.path_helper.get(f"{file_path_obj}_page_{page_num}.png")
                            pix.save(str(img_path_obj))
                            image_urls.append(f"file://{img_path_obj}")
                        doc.close()

                    elif suffix == ".pptx":
                        self.logger.warning("âŒ å°šæœªå¯¦ä½œ PPTX é é¢è½‰åœ–ç‰‡çš„ OCR fallback")
                        continue

                    if image_urls:
                        ocr_text = await ocr_images_from_urls(image_urls)
                        if ocr_text:
                            summary = await summarize_pitch_deck(ocr_text, name)
                            if summary:
                                results.append(summary)
                                continue

                else:
                    summary = await summarize_pitch_deck(extracted_text, name)
                    if summary:
                        results.append(summary)

            except Exception as e:
                self.logger.error(f"âŒ åˆ†ææª”æ¡ˆ {name} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                results.append({"error": f"âŒ åˆ†æå¤±æ•—: {name}"})

        return results if results else [{"error": "âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•é™„ä»¶å…§å®¹"}]

    async def initialize(self):
        """Initialize the browser instance asynchronously."""
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=[
                    "--disable-gpu",
                    "--no-sandbox",
                    "--disable-dev-shm-usage",
                    "--disable-setuid-sandbox",
                    "--disable-extensions",
                    "--disable-background-networking",
                    "--disable-sync",
                    "--metrics-recording-only",
                    "--disable-default-apps",
                    "--mute-audio",
                    "--no-first-run",
                    "--hide-scrollbars",
                    "--ignore-certificate-errors",
                    "--window-size=1920,1080",
                    "--single-process",
                    "--disable-blink-features=AutomationControlled"
                ]
            )
            self.logger.info("Browser initialized successfully")
        except Exception as e:
            self.logger.error(f"Failed to initialize browser: {str(e)}")
            raise

    async def close(self):
        """é—œé–‰ç€è¦½å™¨"""
        try:
            if self.browser:
                await self.browser.close()
                self.browser = None
            if hasattr(self, 'playwright'):
                await self.playwright.stop()
        except Exception as e:
            self.logger.error(f"Error closing browser: {e}")
        finally:
            # ç¢ºä¿æ‰€æœ‰è³‡æºéƒ½è¢«é‡‹æ”¾
            import gc
            gc.collect()

    async def extract_docsend_links(self, text: str) -> List[str]:
        """å¾æ–‡æœ¬ä¸­æå– DocSend é€£çµ"""
        docsend_pattern = r'https?://(?:www\.)?docsend\.com/[^\s)"}]+'
        return re.findall(docsend_pattern, text)
    
    async def _get_page(self, url: str) -> Page:
        """ä½¿ç”¨åŠ å¼·é˜²åµæ¸¬è¨­å®šå»ºç«‹æ–°çš„ page"""
        if not hasattr(self, 'context') or self.context is None:
            self.context = await self.browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
                viewport={"width": 1280, "height": 800},
                locale="en-US",
                timezone_id="America/Los_Angeles",
                permissions=["geolocation"],
                extra_http_headers={
                    "Accept-Language": "en-US,en;q=0.9",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
                    "sec-ch-ua": '"Chromium";v="123", "Not:A-Brand";v="99", "Google Chrome";v="123"',
                    "sec-ch-ua-mobile": "?0",
                    "sec-ch-ua-platform": '"Windows"',
                }
            )

        page = await self.context.new_page()
        page.set_default_timeout(30000)  # è¨­å®š 30 ç§’è¶…æ™‚ï¼Œé¿å… hang æ­»
        return page


    async def read_docsend_document(self, url: str) -> Optional[str]:
        """è®€å– DocSend æ–‡æª”çš„å…§å®¹"""
        try:
            self.logger.info(f"æ­£åœ¨å¾ DocSend é€£çµè®€å–å…§å®¹: {url}")
            
            # å‰µå»ºæ–°é é¢
            page = await self._get_page(url)
            
            # è¨ªå• DocSend é é¢
            response = await page.goto(url, wait_until='networkidle', timeout=30000)
            
            self.logger.info(f"è¨ªå•ç‹€æ…‹ç¢¼: {response.status}")
            
            if response.status == 403:
                self.logger.error("è¨ªå•è¢«æ‹’çµ• (403 Forbidden)")
                await page.close()
                return None
            
            # éš¨æ©Ÿç­‰å¾… 2-5 ç§’
            await asyncio.sleep(random.uniform(2, 5))
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦å¡«å¯«é›»å­éƒµä»¶
            email_input = await page.query_selector('input[type="email"]')
            if email_input:
                self.logger.info("éœ€è¦å¡«å¯«é›»å­éƒµä»¶æ‰èƒ½è¨ªå•æ–‡æª”")
                await page.type('input[type="email"]', self.email, delay=random.uniform(100, 200))
                try:
                    self.logger.info("å˜—è©¦é»æ“Šæäº¤æŒ‰éˆ•")
                    await page.locator('button:has-text("Continue")').wait_for(state='visible', timeout=1000)
                    await page.locator('button:has-text("Continue")').click(timeout=1000)
                except Exception as e:
                    self.logger.warning(f"æäº¤æŒ‰éˆ•é»æ“Šå¤±æ•—: {e}")
                await page.wait_for_load_state('networkidle', timeout=30000)

            # æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰å…§å®¹
            self.logger.info("é–‹å§‹æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰å…§å®¹")
            last_height = await page.evaluate('document.body.scrollHeight')
            while True:
                # æ»¾å‹•åˆ°åº•éƒ¨
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                # ç­‰å¾…æ–°å…§å®¹åŠ è¼‰
                await page.wait_for_timeout(2000)
                # è¨ˆç®—æ–°çš„æ»¾å‹•é«˜åº¦
                new_height = await page.evaluate('document.body.scrollHeight')
                if new_height == last_height:
                    break
                last_height = new_height
                self.logger.info("ç¹¼çºŒæ»¾å‹•ä»¥åŠ è¼‰æ›´å¤šå…§å®¹")
            
            # æ»¾å‹•å›é ‚éƒ¨
            await page.evaluate('window.scrollTo(0, 0)')
            await page.wait_for_timeout(1000)
            
            # å˜—è©¦é»æ“Šä¸‹ä¸€é æŒ‰éˆ•åŠ è¼‰æ›´å¤šå…§å®¹
            self.logger.info("å˜—è©¦é€šéé»æ“Šä¸‹ä¸€é æŒ‰éˆ•åŠ è¼‰æ›´å¤šå…§å®¹")
            try:
                next_buttons = await page.query_selector_all('button[aria-label*="next"], button[aria-label*="Next"], button[title*="next"], button[title*="Next"]')
                for button in next_buttons:
                    for _ in range(10):  # æœ€å¤šé»æ“Š10æ¬¡
                        try:
                            await button.click()
                            await page.wait_for_timeout(1000)  # ç­‰å¾…æ–°é é¢åŠ è¼‰
                        except:
                            break
            except Exception as e:
                self.logger.warning(f"é»æ“Šä¸‹ä¸€é æŒ‰éˆ•æ™‚å‡ºéŒ¯: {e}")
            
            # å†æ¬¡æ»¾å‹•ä»¥ç¢ºä¿æ‰€æœ‰å…§å®¹éƒ½å·²åŠ è¼‰
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            await page.wait_for_timeout(2000)
            
            # æˆªåœ–ä»¥ä¾¿èª¿è©¦
            debug_screenshot_name = f"docsend_debug_{random.randint(1000, 9999)}.png"
            debug_screenshot_path = str(self.path_helper.get("tmp", debug_screenshot_name))
            self.path_helper.ensure_dir("tmp")
            await page.screenshot(path=debug_screenshot_path)
            self.logger.info(f"ä¿å­˜é é¢æˆªåœ–è‡³: {debug_screenshot_path}")
            
            # Debug å°‡æ•´é  HTML å„²å­˜ä¸‹ä¾†
            html = await page.content()
            debug_html_path = self.path_helper.get("debug_docsend.html")
            with debug_html_path.open("w", encoding="utf-8") as f:
                f.write(html)

            # ç­‰å¾…æ–‡æª”å…§å®¹åŠ è¼‰
            target_frame = None
            content_selectors = ['.document-content', '.page', '.viewer-content', '.ds-viewer-container']

            for frame in page.frames:
                frame_url = frame.url or ""
                if "docsend.com/view" in frame_url and "marketing.docsend.com" not in frame_url:
                    for selector in content_selectors:
                        try:
                            await frame.wait_for_selector(selector, timeout=5000)
                            self.logger.info(f"åœ¨ iframe ä¸­æ‰¾åˆ°å…§å®¹: {selector}")
                            target_frame = frame
                            break
                        except:
                            continue
                if target_frame:
                    break

            if not target_frame:
                self.logger.warning("âš ï¸ æœªèƒ½æ‰¾åˆ°å«æ–‡å­— selectorï¼Œé–‹å§‹ debug æ‰€æœ‰ iframe...")
                await debug_all_iframes(page)

                # å˜—è©¦å¾æ‰€æœ‰ iframe ä¸­æ“·å–åœ–ç‰‡åš OCR
                for frame in page.frames:
                    try:
                        frame_html = await frame.content()
                        soup = BeautifulSoup(frame_html, "html.parser")
                        img_tags = soup.find_all("img")
                        image_urls = [img["src"] for img in img_tags if img.get("src")]
                        
                        if image_urls:
                            self.logger.info(f"âœ… åœ¨ iframe ä¸­æ‰¾åˆ°åœ–ç‰‡ï¼šå…± {len(image_urls)} å¼µï¼Œæº–å‚™åŸ·è¡Œ OCR")
                            ocr_raw_text = await ocr_images_from_urls(image_urls)
                            if ocr_raw_text:
                                summarized_text = await summarize_pitch_deck(ocr_raw_text, url)
                                await page.close()
                                return summarized_text
                    except Exception as e:
                        self.logger.warning(f"è®€å– iframe å…§å®¹å¤±æ•—ï¼š{e}")

                self.logger.error("âŒ æ‰€æœ‰ iframe å‡æœªèƒ½æä¾›å…§å®¹æˆ–åœ–ç‰‡")
                await page.close()
                return None
            
            # å¾ iframe æŠ“ HTML
            html = await target_frame.content()
            self.logger.info(f"ç²å–çš„HTMLå…§å®¹é•·åº¦: {len(html)} å­—ç¬¦")
            
            # è§£æ HTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # æå–æ–‡æª”æ¨™é¡Œ
            title_elem = soup.find('title')
            if not title_elem:
                self.logger.info("iframe ä¸­æœªæ‰¾åˆ° <title>ï¼Œå˜—è©¦å¾ä¸»é æŠ“å– title")
                main_html = await page.content()
                main_soup = BeautifulSoup(main_html, 'html.parser')
                title_elem = main_soup.find('title')

            title = title_elem.text.strip() if title_elem else "DocSend Document"
            self.logger.info(f"æå–çš„æ–‡æª”æ¨™é¡Œ: {title}")
            
            # æå–æ–‡æª”å…§å®¹
            text_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div'])
            self.logger.info(f"æ‰¾åˆ° {len(text_elements)} å€‹æ–‡æœ¬å…ƒç´ ")
            
            extracted_text = []
            
            for elem in text_elements:
                text = elem.get_text().strip()
                if text and len(text) > 10:  # åªä¿ç•™æœ‰æ„ç¾©çš„æ–‡æœ¬
                    extracted_text.append(text)
            
            self.logger.info(f"ç¯©é¸å¾Œæå–äº† {len(extracted_text)} å€‹æœ‰æ„ç¾©çš„æ–‡æœ¬æ®µè½")
            extracted_text = "\n\n".join(extracted_text)
            
            # é—œé–‰é é¢
            await page.close()
            
            if not extracted_text or len(extracted_text) < 100:
                self.logger.warning("âš ï¸ æ–‡å­—å…§å®¹éå°‘ï¼Œå˜—è©¦ OCR åœ–ç‰‡ä¸¦ç”¨ GPT æ‘˜è¦")
                img_tags = soup.find_all('img')
                image_urls = [img['src'] for img in img_tags if img.get('src')]

                if image_urls:
                    ocr_raw_text = await ocr_images_from_urls(image_urls)
                    if ocr_raw_text:
                        summarized_text = await summarize_pitch_deck(ocr_raw_text, url)
                        extracted_text = summarized_text
            
            # è¨˜éŒ„å…§å®¹æ‘˜è¦
            content_preview = extracted_text[:100] + "..." if len(extracted_text) > 100 else extracted_text
            self.logger.info(f"æå–æ–‡æœ¬é è¦½: {content_preview}")
            self.logger.info(f"ç¸½æå–æ–‡æœ¬é•·åº¦: {len(extracted_text)} å­—ç¬¦")
            
            # æ ¼å¼åŒ–æå–çš„å…§å®¹
            formatted_content = f"--- DocSend æ–‡æª”: {title} ---\n\n{extracted_text}\n\n--- DocSend æ–‡æª”çµæŸ ---"
            
            self.logger.info(f"æˆåŠŸå¾ DocSend é€£çµæå–å…§å®¹: {url}")
            return formatted_content
        
        except Exception as e:
            self.logger.error(f"è®€å– DocSend æ–‡æª”æ™‚å‡ºéŒ¯: {str(e)}", exc_info=True)
            if 'page' in locals():
                await page.close()
            return None

    async def is_pitch_deck(self, page) -> bool:
        """åˆ¤æ–·ç¶²é æ˜¯å¦ç‚º Pitch Deck"""
        try:
            # 0. é¦–å…ˆæª¢æŸ¥æ˜¯å¦ç‚ºç¤¾ç¾¤ç¶²ç«™
            url = page.url.lower()
            social_media_domains = ['x.com', 'twitter.com', 'facebook.com', 'linkedin.com', 'instagram.com']
            if any(domain in url for domain in social_media_domains):
                return False

            # 1. æª¢æŸ¥ URL é—œéµå­—
            deck_keywords = ['pitch', 'deck', 'presentation', 'slides', 'investor', 'fundraising']
            if any(keyword in url for keyword in deck_keywords):
                return True

            # 2. æª¢æŸ¥é é¢æ¨™é¡Œ
            title = await page.title()
            if title and any(keyword in title.lower() for keyword in deck_keywords):
                return True

            # 3. æª¢æŸ¥é é¢å…§å®¹ç‰¹å¾µ
            content = await page.content()
            soup = BeautifulSoup(content, "html.parser")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡ç›¸é—œçš„å…ƒç´ 
            slide_indicators = [
                # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡å®¹å™¨
                bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['slide', 'deck', 'presentation']))),
                # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡å°èˆª
                bool(soup.find('nav', class_=lambda x: x and any(word in str(x).lower() for word in ['slide', 'deck', 'presentation']))),
                # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡è¨ˆæ•¸å™¨
                bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['counter', 'progress', 'slide-number']))),
                # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡æ§åˆ¶æŒ‰éˆ•
                bool(soup.find('button', class_=lambda x: x and any(word in str(x).lower() for word in ['next', 'prev', 'slide']))),
            ]
            
            if any(slide_indicators):
                return True

            # 4. æª¢æŸ¥é é¢çµæ§‹
            # è¨ˆç®—é é¢ä¸­çš„åœ–ç‰‡æ•¸é‡
            images = await page.query_selector_all('img')
            # è¨ˆç®—é é¢ä¸­çš„æ–‡æœ¬å¡Šæ•¸é‡
            text_blocks = await page.query_selector_all('p, h1, h2, h3, h4, h5, h6')
            
            # å¦‚æœåœ–ç‰‡æ•¸é‡è¼ƒå¤šä¸”æ–‡æœ¬å¡Šè¼ƒå°‘ï¼Œå¯èƒ½æ˜¯æŠ•å½±ç‰‡
            # ä½†éœ€è¦ç¢ºä¿ä¸æ˜¯ç¤¾ç¾¤ç¶²ç«™çš„çµæ§‹
            if len(images) > 5 and len(text_blocks) < 10:
                # æª¢æŸ¥æ˜¯å¦æœ‰ç¤¾ç¾¤ç¶²ç«™ç‰¹æœ‰çš„å…ƒç´ 
                social_indicators = [
                    bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['tweet', 'post', 'status', 'feed', 'timeline']))),
                    bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['profile', 'avatar', 'user-info']))),
                    bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['like', 'share', 'comment', 'retweet']))),
                ]
                if not any(social_indicators):
                    return True

            # 5. æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡ç›¸é—œçš„ JavaScript
            scripts = await page.query_selector_all('script')
            for script in scripts:
                content = await script.text_content()
                if any(keyword in content.lower() for keyword in ['slideshow', 'presentation', 'deck']):
                    return True

            return False

        except Exception as e:
            self.logger.error(f"Error checking if page is pitch deck: {e}")
            return False

    async def run_generic_link_analysis(self, message: str) -> List[Dict[str, Any]]:
        """åˆ†æä¸€èˆ¬ç¶²å€ï¼ˆåŒ…æ‹¬å…¬å¸å®˜ç¶²ï¼‰"""
        urls = re.findall(r'https?://[^\s\)\"]+', message)
        results = []
        
        if not urls:
            return [{"error": "âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ç¶²å€"}]
        
        self.logger.info(f"æ‰¾åˆ° {len(urls)} å€‹ç¶²å€éœ€è¦è™•ç†")
        
        try:
            async with async_playwright() as pw:
                browser = await pw.chromium.launch(headless=True)
                if not browser:
                    self.logger.error("âŒ ç„¡æ³•å‰µå»ºç€è¦½å™¨å¯¦ä¾‹")
                    return [{"error": "âŒ ç„¡æ³•å‰µå»ºç€è¦½å™¨å¯¦ä¾‹"}]
                
                # å‰µå»ºæ–°çš„ç€è¦½å™¨ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ æ›´å¤šé…ç½®
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    viewport={'width': 1920, 'height': 1080},
                    # æ·»åŠ æ›´å¤šç€è¦½å™¨é…ç½®
                    locale='en-US',
                    timezone_id='America/New_York',
                    permissions=['geolocation'],
                    # æ·»åŠ æ›´å¤š HTTP é ­
                    extra_http_headers={
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                        'sec-ch-ua': '"Chromium";v="122", "Not(A:Brand";v="24", "Google Chrome";v="122"',
                        'sec-ch-ua-mobile': '?0',
                        'sec-ch-ua-platform': '"Windows"'
                    }
                )
                
                if not context:
                    self.logger.error("âŒ ç„¡æ³•å‰µå»ºç€è¦½å™¨ä¸Šä¸‹æ–‡")
                    await browser.close()
                    return [{"error": "âŒ ç„¡æ³•å‰µå»ºç€è¦½å™¨ä¸Šä¸‹æ–‡"}]
                
                for url in urls:
                    page = None
                    try:
                        self.logger.info(f"ğŸŒ é–‹å§‹åˆ†æç¶²å€: {url}")
                        
                        # å‰µå»ºæ–°é é¢
                        page = await context.new_page()
                        if not page:
                            self.logger.error(f"âŒ ç„¡æ³•ç‚º {url} å‰µå»ºæ–°é é¢")
                            results.append({"url": url, "error": "âŒ ç„¡æ³•å‰µå»ºæ–°é é¢"})
                            continue
                        
                        # æª¢æŸ¥æ˜¯å¦ç‚º Notion é é¢
                        is_notion = "notion.so" in url
                        if is_notion:
                            self.logger.info("æª¢æ¸¬åˆ° Notion é é¢ï¼Œä½¿ç”¨ç‰¹æ®Šè™•ç†æ–¹å¼")
                            
                            # è¨­ç½®æ›´é•·çš„è¶…æ™‚æ™‚é–“
                            await page.set_default_timeout(60000)  # 60 ç§’
                            
                            # è¨­ç½® Notion ç‰¹å®šçš„ cookie å’Œèªè­‰ä¿¡æ¯
                            await context.add_cookies([
                                {
                                    'name': 'notion_browser_id',
                                    'value': 'random_browser_id',
                                    'domain': '.notion.so',
                                    'path': '/'
                                },
                                {
                                    'name': 'notion_user_id',
                                    'value': 'random_user_id',
                                    'domain': '.notion.so',
                                    'path': '/'
                                },
                                {
                                    'name': 'notion_user_info',
                                    'value': '{"id":"random_user_id","email":"user@example.com"}',
                                    'domain': '.notion.so',
                                    'path': '/'
                                }
                            ])
                            
                            # è¨­ç½® Notion ç‰¹å®šçš„è«‹æ±‚é ­
                            await page.set_extra_http_headers({
                                'X-Notion-Client': 'web',
                                'X-Notion-Client-Version': '23.11.0.0',
                                'X-Notion-Client-Platform': 'web',
                                'X-Notion-Client-Platform-Version': 'Windows',
                                'X-Notion-Client-User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                            })
                        
                        # è¨ªå•ç¶²é 
                        try:
                            if is_notion:
                                # å°æ–¼ Notion é é¢ï¼Œä½¿ç”¨ä¸åŒçš„åŠ è¼‰ç­–ç•¥
                                response = await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                                if not response:
                                    raise Exception("é é¢åŠ è¼‰å¤±æ•—")
                                
                                # ç­‰å¾…é é¢åŠ è¼‰å®Œæˆ
                                await page.wait_for_load_state('networkidle', timeout=30000)
                                
                                # ç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç¾
                                try:
                                    # æ“´å±• Notion é é¢å…§å®¹é¸æ“‡å™¨
                                    selectors = [
                                        'div[class*="notion-page-content"]',
                                        'div[class*="notion-page-block"]',
                                        'div[class*="notion-text-block"]',
                                        'div[class*="notion-collection"]',
                                        'div[class*="notion-page"]',
                                        'div[class*="notion-content"]'
                                    ]
                                    
                                    for selector in selectors:
                                        try:
                                            await page.wait_for_selector(selector, timeout=5000)
                                            self.logger.info(f"æ‰¾åˆ° Notion å…§å®¹å…ƒç´ : {selector}")
                                            break
                                        except:
                                            continue
                                except:
                                    self.logger.warning("æœªæ‰¾åˆ° Notion é é¢å…§å®¹å…ƒç´ ï¼Œå˜—è©¦ç¹¼çºŒè™•ç†")
                                
                                # ç­‰å¾…ä¸€æ®µæ™‚é–“è®“å‹•æ…‹å…§å®¹åŠ è¼‰
                                await page.wait_for_timeout(5000)
                                
                                # å˜—è©¦æ»¾å‹•é é¢ä»¥åŠ è¼‰æ›´å¤šå…§å®¹
                                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                                await page.wait_for_timeout(2000)
                                await page.evaluate('window.scrollTo(0, 0)')
                                await page.wait_for_timeout(1000)
                                
                                # ä½¿ç”¨ JavaScript æå– Notion é é¢å…§å®¹
                                content = await page.evaluate('''() => {
                                    const extractContent = (element) => {
                                        const result = {
                                            text: [],
                                            images: [],
                                            headings: []
                                        };
                                        
                                        // æå–æ–‡æœ¬å…§å®¹
                                        const textElements = element.querySelectorAll(`
                                            div[class*="notion-text-block"],
                                            div[class*="notion-page-block"],
                                            div[class*="notion-paragraph"],
                                            div[class*="notion-list-item"],
                                            div[class*="notion-bulleted-list"],
                                            div[class*="notion-numbered-list"],
                                            div[class*="notion-toggle"],
                                            div[class*="notion-quote"],
                                            div[class*="notion-callout"]
                                        `);
                                        
                                        textElements.forEach(elem => {
                                            const text = elem.textContent.trim();
                                            if (text) {
                                                result.text.push(text);
                                            }
                                        });
                                        
                                        // æå–åœ–ç‰‡
                                        const images = element.querySelectorAll('img');
                                        images.forEach(img => {
                                            const src = img.getAttribute('src');
                                            if (src) {
                                                result.images.push(src);
                                            }
                                        });
                                        
                                        // æå–æ¨™é¡Œ
                                        const headings = element.querySelectorAll(`
                                            div[class*="notion-header-block"],
                                            div[class*="notion-sub-header-block"],
                                            div[class*="notion-sub-sub-header-block"]
                                        `);
                                        
                                        headings.forEach(heading => {
                                            const text = heading.textContent.trim();
                                            if (text) {
                                                result.headings.push(text);
                                            }
                                        });
                                        
                                        return result;
                                    };
                                    
                                    // å˜—è©¦ä¸åŒçš„å…§å®¹å®¹å™¨
                                    const containers = [
                                        document.querySelector('div[class*="notion-page-content"]'),
                                        document.querySelector('div[class*="notion-page-block"]'),
                                        document.querySelector('div[class*="notion-content"]'),
                                        document.body
                                    ];
                                    
                                    for (const container of containers) {
                                        if (container) {
                                            return extractContent(container);
                                        }
                                    }
                                    
                                    return null;
                                }''')
                                
                                if content:
                                    # æ ¼å¼åŒ–æå–çš„å…§å®¹
                                    formatted_content = []
                                    
                                    if content['headings']:
                                        formatted_content.extend(content['headings'])
                                    
                                    if content['text']:
                                        formatted_content.extend(content['text'])
                                    
                                    if content['images']:
                                        formatted_content.append("\nImages found:")
                                        formatted_content.extend(content['images'])
                                    
                                    combined_text = "\n\n".join(formatted_content)
                                    
                                    if combined_text.strip():
                                        self.logger.info("æˆåŠŸæå– Notion é é¢å…§å®¹")
                                        summary = await summarize_pitch_deck(combined_text, message)
                                        if summary:
                                            self.logger.info("æˆåŠŸç”Ÿæˆ Notion é é¢æ‘˜è¦")
                                            results.append(summary)
                                            continue
                                        else:
                                            self.logger.warning("âŒ Notion é é¢æ‘˜è¦ç”Ÿæˆå¤±æ•—")
                                    else:
                                        self.logger.warning("âŒ æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„ Notion é é¢å…§å®¹")
                            else:
                                response = await page.goto(url, wait_until='domcontentloaded', timeout=30000)
                                if not response:
                                    raise Exception("é é¢åŠ è¼‰å¤±æ•—")
                                await page.wait_for_load_state('networkidle', timeout=30000)
                        except Exception as e:
                            self.logger.warning(f"é é¢åŠ è¼‰è¶…æ™‚ï¼Œå˜—è©¦ç¹¼çºŒè™•ç†: {str(e)}")
                            results.append({"url": url, "error": f"âŒ é é¢åŠ è¼‰å¤±æ•—: {str(e)}"})
                            continue
                        
                        # æª¢æŸ¥æ˜¯å¦ç‚º Pitch Deck
                        try:
                            if page:  # ç¢ºä¿ page ä¸ç‚º None
                                is_deck = await self.is_pitch_deck(page)
                                if is_deck:
                                    self.logger.info("âœ… æª¢æ¸¬åˆ° Pitch Deckï¼Œä½¿ç”¨ç‰¹æ®Šè™•ç†æ–¹å¼")
                                    
                                    # ä¿å­˜é é¢æˆªåœ–ä»¥ä¾¿èª¿è©¦
                                    debug_screenshot_name = f"pitch_deck_debug_{random.randint(1000, 9999)}.png"
                                    debug_screenshot_path = str(self.path_helper.get("tmp", debug_screenshot_name))
                                    self.path_helper.ensure_dir("tmp")
                                    await page.screenshot(path=debug_screenshot_path)
                                    self.logger.info(f"ä¿å­˜é é¢æˆªåœ–è‡³: {debug_screenshot_path}")
                                    
                                    # è™•ç† Pitch Deck é é¢
                                    content = await self.process_pitch_deck_page(page)
                                    if content:
                                        self.logger.info("æˆåŠŸæå– Pitch Deck å…§å®¹ï¼Œé–‹å§‹ç”Ÿæˆæ‘˜è¦")
                                        summary = await summarize_pitch_deck(content, message)
                                        if summary:
                                            self.logger.info("æˆåŠŸç”Ÿæˆ Pitch Deck æ‘˜è¦")
                                            results.append(summary)
                                            continue
                                        else:
                                            self.logger.warning("âŒ Pitch Deck æ‘˜è¦ç”Ÿæˆå¤±æ•—")
                                    else:
                                        self.logger.warning("âŒ Pitch Deck å…§å®¹æå–å¤±æ•—")
                                        
                                    # å¦‚æœä¸æ˜¯ Pitch Deck æˆ–è™•ç†å¤±æ•—ï¼Œä½¿ç”¨ä¸€èˆ¬ç¶²é è™•ç†æ–¹å¼
                                    self.logger.info("ä½¿ç”¨ä¸€èˆ¬ç¶²é è™•ç†æ–¹å¼")
                                    html = await page.content()
                                    soup = BeautifulSoup(html, "html.parser")
                                    
                                    # æå–é—œéµä¿¡æ¯
                                    extracted_data = {
                                        "title": soup.title.string if soup.title else "",
                                        "meta_description": "",
                                        "main_content": [],
                                        "company_info": {}
                                    }
                                    
                                    # æå– meta description
                                    meta_desc = soup.find('meta', attrs={'name': 'description'})
                                    if meta_desc:
                                        extracted_data["meta_description"] = meta_desc.get('content', '')
                                    
                                    # å°æ–¼ Notion é é¢ï¼Œä½¿ç”¨ç‰¹å®šçš„é¸æ“‡å™¨
                                    if is_notion:
                                        # å˜—è©¦æå– Notion é é¢å…§å®¹
                                        notion_content = soup.find('div', class_=lambda x: x and 'notion-page-content' in str(x))
                                        if notion_content:
                                            # æå–æ‰€æœ‰æ–‡æœ¬å…§å®¹
                                            for element in notion_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div']):
                                                text = element.get_text(strip=True)
                                                if text and len(text) > 10:
                                                    extracted_data["main_content"].append(text)
                                            
                                            # å°‡æå–çš„å…§å®¹è½‰æ›ç‚ºæ–‡æœ¬
                                            combined_text = f"""
                                            Title: {extracted_data['title']}
                                            Description: {extracted_data['meta_description']}
                                            
                                            Main Content:
                                            {' '.join(extracted_data['main_content'])}
                                            
                                            Company Info:
                                            {json.dumps(extracted_data['company_info'], indent=2)}
                                            """
                                            
                                            # ä½¿ç”¨ GPT åˆ†æå…§å®¹
                                            if combined_text.strip():
                                                self.logger.info("é–‹å§‹ä½¿ç”¨ GPT åˆ†æå…§å®¹")
                                                summary = await summarize_pitch_deck(combined_text, message)
                                                if summary:
                                                    self.logger.info("æˆåŠŸç”Ÿæˆå…§å®¹æ‘˜è¦")
                                                    results.append(summary)
                                                    continue
                                                else:
                                                    self.logger.warning("âŒ GPT åˆ†æå¤±æ•—")
                                    else:
                                        # ä¸€èˆ¬ç¶²é çš„è™•ç†æ–¹å¼
                                        main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=lambda x: x and ('content' in x.lower() or 'main' in x.lower()))
                                        
                                        if main_content:
                                            for element in main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):
                                                text = element.get_text(strip=True)
                                                if text and len(text) > 10:
                                                    extracted_data["main_content"].append(text)
                                        else:
                                            for element in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):
                                                text = element.get_text(strip=True)
                                                if text and len(text) > 10:
                                                    extracted_data["main_content"].append(text)
                                        
                                        # æå–å…¬å¸ä¿¡æ¯
                                        for meta in soup.find_all('meta'):
                                            if meta.get('property') == 'og:site_name':
                                                extracted_data["company_info"]["name"] = meta.get('content', '')
                                        
                                        footer = soup.find('footer')
                                        if footer:
                                            footer_text = footer.get_text(strip=True)
                                            if footer_text:
                                                extracted_data["company_info"]["footer"] = footer_text
                                        
                                        # å°‡æå–çš„å…§å®¹è½‰æ›ç‚ºæ–‡æœ¬
                                        combined_text = f"""
                                        Title: {extracted_data['title']}
                                        Description: {extracted_data['meta_description']}
                                        
                                        Main Content:
                                        {' '.join(extracted_data['main_content'])}
                                        
                                        Company Info:
                                        {json.dumps(extracted_data['company_info'], indent=2)}
                                        """
                                        
                                        # ä½¿ç”¨ GPT åˆ†æå…§å®¹
                                        if combined_text.strip():
                                            self.logger.info("é–‹å§‹ä½¿ç”¨ GPT åˆ†æå…§å®¹")
                                            summary = await summarize_pitch_deck(combined_text, message)
                                            if summary:
                                                self.logger.info("æˆåŠŸç”Ÿæˆå…§å®¹æ‘˜è¦")
                                                results.append(summary)
                                                continue
                                            else:
                                                self.logger.warning("âŒ GPT åˆ†æå¤±æ•—")
                                        
                                        # å¦‚æœæ–‡å­—åˆ†æå¤±æ•—ï¼Œå˜—è©¦ OCR åœ–ç‰‡
                                        self.logger.warning(f"âš ï¸ {url} æ–‡å­—åˆ†æå¤±æ•—ï¼Œå˜—è©¦ OCR åœ–ç‰‡")
                                        img_tags = soup.find_all("img")
                                        image_urls = [img.get("src") for img in img_tags if img.get("src")]
                                        
                                        if image_urls:
                                            self.logger.info(f"æ‰¾åˆ° {len(image_urls)} å¼µåœ–ç‰‡ï¼Œé–‹å§‹ OCR")
                                            ocr_text = await ocr_images_from_urls(image_urls)
                                            if ocr_text.strip():
                                                self.logger.info("OCR æˆåŠŸï¼Œé–‹å§‹ç”Ÿæˆæ‘˜è¦")
                                                summary = await summarize_pitch_deck(ocr_text, message)
                                                if summary:
                                                    self.logger.info("æˆåŠŸç”Ÿæˆ OCR å…§å®¹æ‘˜è¦")
                                                    results.append(summary)
                                                    continue
                                                else:
                                                    self.logger.warning("âŒ OCR å…§å®¹æ‘˜è¦ç”Ÿæˆå¤±æ•—")
                                            else:
                                                self.logger.warning("âŒ OCR å¤±æ•—æˆ–æœªæå–åˆ°æ–‡å­—")
                        except Exception as e:
                            self.logger.error(f"è™•ç† Pitch Deck æ™‚å‡ºéŒ¯: {str(e)}")
                        
                        results.append({"url": url, "error": "âŒ ç„¡æ³•æå–æœ‰æ•ˆå…§å®¹"})
                        
                    except Exception as e:
                        self.logger.error(f"âŒ åˆ†æ {url} å¤±æ•—ï¼š{e}")
                        results.append({"url": url, "error": f"âŒ åˆ†æå¤±æ•—: {e}"})
                    finally:
                        if page:
                            await page.close()
                
                await context.close()
                await browser.close()
        
        except Exception as e:
            self.logger.error(f"âŒ ç€è¦½å™¨æ“ä½œå¤±æ•—ï¼š{e}")
            return [{"error": f"âŒ ç€è¦½å™¨æ“ä½œå¤±æ•—: {e}"}]
        
        return results if results else [{"error": "âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•ç¶²å€"}]

    async def process_pitch_deck_page(self, page) -> Optional[str]:
        """è™•ç† Pitch Deck é é¢"""
        try:
            self.logger.info("é–‹å§‹è™•ç† Pitch Deck é é¢")
            
            # æª¢æŸ¥æ˜¯å¦ç‚º Journey.io ç¶²ç«™
            if "journey.io" in page.url:
                self.logger.info("æª¢æ¸¬åˆ° Journey.io ç¶²ç«™ï¼Œä½¿ç”¨ç‰¹æ®Šè™•ç†æ–¹å¼")
                return await self.process_journey_page(page)
            
            # ç­‰å¾…é é¢åŠ è¼‰å®Œæˆ
            await page.wait_for_load_state('networkidle', timeout=30000)
            
            # æˆªåœ–ä»¥ä¾¿èª¿è©¦
            debug_screenshot_name = f"pitch_deck_debug_{random.randint(1000, 9999)}.png"
            debug_screenshot_path = str(self.path_helper.get("tmp", debug_screenshot_name))
            self.path_helper.ensure_dir("tmp")
            await page.screenshot(path=debug_screenshot_path)
            self.logger.info(f"ä¿å­˜é é¢æˆªåœ–è‡³: {debug_screenshot_path}")
            
            # Debug å°‡æ•´é  HTML å„²å­˜ä¸‹ä¾†
            html = await page.content()
            debug_html_path = self.path_helper.get("debug_pitch_deck.html")
            with debug_html_path.open("w", encoding="utf-8") as f:
                f.write(html)
            
            # æå–é é¢æ¨™é¡Œ
            title = await page.title()
            self.logger.info(f"æå–çš„é é¢æ¨™é¡Œ: {title}")
            
            # 1. å˜—è©¦æå–æŠ•å½±ç‰‡å…§å®¹
            slides = []
            
            # ç­‰å¾…æŠ•å½±ç‰‡å®¹å™¨åŠ è¼‰
            try:
                # æ“´å±•é¸æ“‡å™¨ä»¥åŒ¹é…æ›´å¤šå¯èƒ½çš„æŠ•å½±ç‰‡å®¹å™¨
                await page.wait_for_selector('div[class*="slide"], div[class*="deck"], div[class*="presentation"], div[class*="page"], div[class*="slide-container"], div[class*="slide-wrapper"]', timeout=5000)
            except Exception as e:
                self.logger.warning(f"ç­‰å¾…æŠ•å½±ç‰‡å®¹å™¨è¶…æ™‚: {e}")
            
            # æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰æŠ•å½±ç‰‡
            self.logger.info("é–‹å§‹æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰æŠ•å½±ç‰‡")
            last_height = await page.evaluate('document.body.scrollHeight')
            while True:
                # æ»¾å‹•åˆ°åº•éƒ¨
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                # ç­‰å¾…æ–°å…§å®¹åŠ è¼‰
                await page.wait_for_timeout(2000)
                # è¨ˆç®—æ–°çš„æ»¾å‹•é«˜åº¦
                new_height = await page.evaluate('document.body.scrollHeight')
                if new_height == last_height:
                    break
                last_height = new_height
                self.logger.info("ç¹¼çºŒæ»¾å‹•ä»¥åŠ è¼‰æ›´å¤šæŠ•å½±ç‰‡")
            
            # æ»¾å‹•å›é ‚éƒ¨
            await page.evaluate('window.scrollTo(0, 0)')
            await page.wait_for_timeout(1000)
            
            # ç²å–æ‰€æœ‰æŠ•å½±ç‰‡
            slide_elements = await page.query_selector_all('div[class*="slide"], div[class*="deck"], div[class*="presentation"], div[class*="page"], div[class*="slide-container"], div[class*="slide-wrapper"]')
            self.logger.info(f"æ‰¾åˆ° {len(slide_elements)} å€‹æŠ•å½±ç‰‡å…ƒç´ ")
            
            # å¦‚æœæ‰¾ä¸åˆ°æŠ•å½±ç‰‡ï¼Œå˜—è©¦å…¶ä»–é¸æ“‡å™¨
            if not slide_elements:
                self.logger.info("å˜—è©¦ä½¿ç”¨å‚™ç”¨é¸æ“‡å™¨")
                # å˜—è©¦æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æŠ•å½±ç‰‡å®¹å™¨
                slide_elements = await page.query_selector_all('div[role="slide"], div[data-slide], div[data-page], div[data-index]')
                self.logger.info(f"ä½¿ç”¨å‚™ç”¨é¸æ“‡å™¨æ‰¾åˆ° {len(slide_elements)} å€‹æŠ•å½±ç‰‡å…ƒç´ ")
            
            # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œå˜—è©¦æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æŠ•å½±ç‰‡å…§å®¹çš„ div
            if not slide_elements:
                self.logger.info("å˜—è©¦æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æŠ•å½±ç‰‡å…§å®¹çš„ div")
                # ç²å–é é¢ä¸­çš„æ‰€æœ‰ div
                all_divs = await page.query_selector_all('div')
                # éæ¿¾å‡ºå¯èƒ½åŒ…å«æŠ•å½±ç‰‡å…§å®¹çš„ div
                slide_elements = []
                for div in all_divs:
                    try:
                        # æª¢æŸ¥ div æ˜¯å¦åŒ…å«æŠ•å½±ç‰‡ç›¸é—œçš„å…§å®¹
                        text = await div.text_content()
                        if text and len(text.strip()) > 50:  # å‡è¨­æŠ•å½±ç‰‡å…§å®¹é€šå¸¸è¼ƒé•·
                            slide_elements.append(div)
                    except:
                        continue
                self.logger.info(f"é€šéå…§å®¹åˆ†ææ‰¾åˆ° {len(slide_elements)} å€‹å¯èƒ½çš„æŠ•å½±ç‰‡")
            
            # å¦‚æœæ‰¾åˆ°çš„æŠ•å½±ç‰‡æ•¸é‡æ˜é¡¯å°‘æ–¼é æœŸï¼Œå˜—è©¦é»æ“Šä¸‹ä¸€é æŒ‰éˆ•
            if len(slide_elements) < 10:  # å‡è¨­è‡³å°‘æœ‰10é 
                self.logger.info("å˜—è©¦é€šéé»æ“Šä¸‹ä¸€é æŒ‰éˆ•åŠ è¼‰æ›´å¤šæŠ•å½±ç‰‡")
                try:
                    # å˜—è©¦é»æ“Šä¸‹ä¸€é æŒ‰éˆ•
                    next_buttons = await page.query_selector_all('button[aria-label*="next"], button[aria-label*="Next"], button[title*="next"], button[title*="Next"]')
                    for button in next_buttons:
                        for _ in range(10):  # æœ€å¤šé»æ“Š10æ¬¡
                            try:
                                await button.click()
                                await page.wait_for_timeout(1000)  # ç­‰å¾…æ–°é é¢åŠ è¼‰
                            except:
                                break
                except Exception as e:
                    self.logger.warning(f"é»æ“Šä¸‹ä¸€é æŒ‰éˆ•æ™‚å‡ºéŒ¯: {e}")
                
                # é‡æ–°ç²å–æŠ•å½±ç‰‡
                slide_elements = await page.query_selector_all('div[class*="slide"], div[class*="deck"], div[class*="presentation"], div[class*="page"], div[class*="slide-container"], div[class*="slide-wrapper"]')
                self.logger.info(f"é»æ“Šä¸‹ä¸€é å¾Œæ‰¾åˆ° {len(slide_elements)} å€‹æŠ•å½±ç‰‡å…ƒç´ ")
            
            for i, slide in enumerate(slide_elements):
                try:
                    # æå–æŠ•å½±ç‰‡æ–‡æœ¬
                    text = await slide.text_content()
                    if text.strip():
                        slides.append(f"[Slide {i+1}]\n{text.strip()}")
                        self.logger.info(f"æˆåŠŸæå–ç¬¬ {i+1} å¼µæŠ•å½±ç‰‡çš„æ–‡æœ¬")
                    
                    # æå–æŠ•å½±ç‰‡åœ–ç‰‡
                    images = await slide.query_selector_all('img')
                    self.logger.info(f"ç¬¬ {i+1} å¼µæŠ•å½±ç‰‡ä¸­æ‰¾åˆ° {len(images)} å¼µåœ–ç‰‡")
                    
                    for img in images:
                        src = await img.get_attribute('src')
                        if src:
                            try:
                                # ä¸‹è¼‰åœ–ç‰‡ä¸¦é€²è¡Œ OCR
                                response = requests.get(src)
                                img = Image.open(BytesIO(response.content))
                                ocr_text = pytesseract.image_to_string(img)
                                if ocr_text.strip():
                                    slides.append(f"[Slide {i+1} Image]\n{ocr_text.strip()}")
                                    self.logger.info(f"æˆåŠŸå°ç¬¬ {i+1} å¼µæŠ•å½±ç‰‡çš„åœ–ç‰‡é€²è¡Œ OCR")
                            except Exception as e:
                                self.logger.warning(f"OCR å¤±æ•—: {e}")
                except Exception as e:
                    self.logger.warning(f"è™•ç†ç¬¬ {i+1} å¼µæŠ•å½±ç‰‡æ™‚å‡ºéŒ¯: {e}")
            
            if slides:
                formatted_content = f"--- Pitch Deck: {title} ---\n\n" + "\n\n".join(slides) + "\n\n--- Pitch Deck çµæŸ ---"
                self.logger.info("æˆåŠŸæå–æŠ•å½±ç‰‡å…§å®¹")
                return formatted_content
            
            # 2. å¦‚æœç„¡æ³•æå–æŠ•å½±ç‰‡ï¼Œå˜—è©¦æå–æ•´å€‹é é¢å…§å®¹
            self.logger.info("ç„¡æ³•æå–æŠ•å½±ç‰‡å…§å®¹ï¼Œå˜—è©¦æå–æ•´å€‹é é¢")
            content = await page.content()
            soup = BeautifulSoup(content, "html.parser")
            
            # æå–æ‰€æœ‰æ–‡æœ¬å…§å®¹
            texts = []
            for element in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div']):
                text = element.get_text(strip=True)
                if text and len(text) > 10:
                    texts.append(text)
            
            if texts:
                formatted_content = f"--- Pitch Deck: {title} ---\n\n" + "\n\n".join(texts) + "\n\n--- Pitch Deck çµæŸ ---"
                self.logger.info("æˆåŠŸæå–é é¢æ–‡æœ¬å…§å®¹")
                return formatted_content
            
            # 3. å¦‚æœæ–‡å­—æå–å¤±æ•—ï¼Œå˜—è©¦ OCR æ‰€æœ‰åœ–ç‰‡
            self.logger.warning("æ–‡å­—æå–å¤±æ•—ï¼Œå˜—è©¦ OCR æ‰€æœ‰åœ–ç‰‡")
            img_tags = soup.find_all("img")
            image_urls = [img.get("src") for img in img_tags if img.get("src")]
            
            if image_urls:
                self.logger.info(f"æ‰¾åˆ° {len(image_urls)} å¼µåœ–ç‰‡ï¼Œé–‹å§‹ OCR")
                ocr_text = await ocr_images_from_urls(image_urls)
                if ocr_text.strip():
                    formatted_content = f"--- Pitch Deck: {title} ---\n\n{ocr_text}\n\n--- Pitch Deck çµæŸ ---"
                    self.logger.info("æˆåŠŸé€šé OCR æå–å…§å®¹")
                    return formatted_content
            
            self.logger.error("âŒ ç„¡æ³•æå–ä»»ä½•æœ‰æ•ˆå…§å®¹")
            return None
            
        except Exception as e:
            self.logger.error(f"è™•ç† Pitch Deck é é¢æ™‚å‡ºéŒ¯: {str(e)}", exc_info=True)
            return None

    async def process_journey_page(self, page) -> Optional[str]:
        """è™•ç† Journey.io é é¢"""
        try:
            self.logger.info("é–‹å§‹è™•ç† Journey.io é é¢")
            
            # ç­‰å¾…é é¢åŠ è¼‰
            await page.wait_for_load_state('networkidle', timeout=30000)
            
            # ä¿å­˜é é¢çµæ§‹ä»¥ä¾›èª¿è©¦
            debug_html_path = self.path_helper.get("debug_journey.html")
            html_content = await page.content()
            with debug_html_path.open("w", encoding="utf-8") as f:
                f.write(html_content)
            self.logger.info(f"å·²ä¿å­˜é é¢çµæ§‹åˆ° {debug_html_path}")
            
            # ä¿å­˜é é¢æˆªåœ–
            debug_screenshot_path = self.path_helper.get("debug_journey.png")
            await page.screenshot(path=str(debug_screenshot_path))
            self.logger.info(f"å·²ä¿å­˜é é¢æˆªåœ–åˆ° {debug_screenshot_path}")
            
            # æå–é é¢æ¨™é¡Œ
            title = await page.title()
            self.logger.info(f"æå–çš„é é¢æ¨™é¡Œ: {title}")
            
            # æ”¹é€²çš„æ»¾å‹•é‚è¼¯
            self.logger.info("é–‹å§‹æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰å…§å®¹")
            
            # 1. å…ˆæ»¾å‹•åˆ°é ‚éƒ¨
            await page.evaluate('window.scrollTo(0, 0)')
            await page.wait_for_timeout(1000)
            
            # 2. ç²å–åˆå§‹é é¢é«˜åº¦
            initial_height = await page.evaluate('document.body.scrollHeight')
            self.logger.info(f"åˆå§‹é é¢é«˜åº¦: {initial_height}")
            
            # 3. ä½¿ç”¨æ›´å°çš„æ»¾å‹•æ­¥é•·å’Œæ›´é•·çš„ç­‰å¾…æ™‚é–“
            scroll_step = 300  # æ¯æ¬¡æ»¾å‹•300åƒç´ 
            max_attempts = 50  # æœ€å¤§å˜—è©¦æ¬¡æ•¸
            no_change_count = 0  # è¨˜éŒ„é«˜åº¦æœªè®ŠåŒ–çš„æ¬¡æ•¸
            
            for attempt in range(max_attempts):
                # ç²å–ç•¶å‰æ»¾å‹•ä½ç½®
                current_position = await page.evaluate('window.pageYOffset')
                
                # è¨ˆç®—æ–°çš„æ»¾å‹•ä½ç½®
                new_position = current_position + scroll_step
                
                # æ»¾å‹•åˆ°æ–°ä½ç½®
                await page.evaluate(f'window.scrollTo(0, {new_position})')
                await page.wait_for_timeout(2000)  # ç­‰å¾…2ç§’è®“å…§å®¹åŠ è¼‰
                
                # ç²å–æ–°çš„é é¢é«˜åº¦
                new_height = await page.evaluate('document.body.scrollHeight')
                
                # æª¢æŸ¥æ˜¯å¦åˆ°é”åº•éƒ¨
                if new_position >= new_height:
                    self.logger.info("å·²åˆ°é”é é¢åº•éƒ¨")
                    break
                
                # æª¢æŸ¥é é¢é«˜åº¦æ˜¯å¦è®ŠåŒ–
                if new_height == initial_height:
                    no_change_count += 1
                    if no_change_count >= 3:  # å¦‚æœé€£çºŒ3æ¬¡é«˜åº¦æœªè®ŠåŒ–ï¼Œå¯èƒ½å·²ç¶“åˆ°åº•
                        self.logger.info("é é¢é«˜åº¦é€£çºŒæœªè®ŠåŒ–ï¼Œå¯èƒ½å·²åˆ°åº•éƒ¨")
                        break
                else:
                    no_change_count = 0
                    initial_height = new_height
                
                self.logger.info(f"æ»¾å‹•é€²åº¦: {new_position}/{new_height} (å˜—è©¦ {attempt + 1}/{max_attempts})")
            
            # 4. æœ€å¾Œå†æ»¾å‹•å›é ‚éƒ¨
            await page.evaluate('window.scrollTo(0, 0)')
            await page.wait_for_timeout(1000)
            
            # ä½¿ç”¨ JavaScript æå–æ‰€æœ‰å…§å®¹
            content = await page.evaluate('''() => {
                const sections = [];
                
                // éæ­·æ‰€æœ‰ä¸»è¦å€å¡Š
                document.querySelectorAll('div[class*="page"], div[class*="section"], div[class*="content"], div[class*="block"], div[class*="slide"]').forEach(section => {
                    const sectionData = {
                        title: '',
                        content: [],
                        teamMembers: []
                    };
                    
                    // æå–æ¨™é¡Œ
                    const titleElem = section.querySelector('h1, h2, h3, h4, h5, h6, [class*="title"], [class*="heading"], [class*="header"]');
                    if (titleElem) {
                        sectionData.title = titleElem.textContent.trim();
                    }
                    
                    // æª¢æŸ¥æ˜¯å¦ç‚ºåœ˜éšŠç›¸é—œéƒ¨åˆ†
                    const isTeamSection = sectionData.title.toLowerCase().includes('team') || 
                                        sectionData.title.toLowerCase().includes('about us') ||
                                        sectionData.title.toLowerCase().includes('founder') ||
                                        sectionData.title.toLowerCase().includes('leadership');
                    
                    if (isTeamSection) {
                        console.log('Found team section:', sectionData.title);
                        
                        // æ“´å±•åœ˜éšŠæˆå“¡é¸æ“‡å™¨
                        const teamElements = section.querySelectorAll(`
                            div[class*="member"], 
                            div[class*="person"], 
                            div[class*="profile"], 
                            div[class*="team"], 
                            div[class*="founder"], 
                            div[class*="investor"], 
                            div[class*="funding"],
                            div[class*="card"],
                            div[class*="bio"],
                            div[class*="staff"],
                            div[class*="leadership"],
                            div[class*="executive"]
                        `);
                        
                        console.log('Found team elements:', teamElements.length);
                        
                        teamElements.forEach(teamElem => {
                            const memberData = {
                                name: '',
                                role: '',
                                description: ''
                            };
                            
                            // æ“´å±•åç¨±é¸æ“‡å™¨
                            const nameElem = teamElem.querySelector(`
                                h1, h2, h3, h4, h5, h6, 
                                [class*="name"], 
                                [class*="title"],
                                [class*="heading"],
                                [class*="header"],
                                strong,
                                b
                            `);
                            if (nameElem) {
                                memberData.name = nameElem.textContent.trim();
                                console.log('Found team member name:', memberData.name);
                            }
                            
                            // æ“´å±•è§’è‰²é¸æ“‡å™¨
                            const roleElem = teamElem.querySelector(`
                                [class*="role"], 
                                [class*="position"], 
                                [class*="title"],
                                [class*="job"],
                                [class*="position"],
                                [class*="designation"],
                                em,
                                i
                            `);
                            if (roleElem) {
                                memberData.role = roleElem.textContent.trim();
                                console.log('Found team member role:', memberData.role);
                            }
                            
                            // æ“´å±•æè¿°é¸æ“‡å™¨
                            const descElem = teamElem.querySelector(`
                                p, 
                                div[class*="text"], 
                                div[class*="content"], 
                                div[class*="description"],
                                div[class*="bio"],
                                div[class*="about"],
                                div[class*="info"]
                            `);
                            if (descElem) {
                                memberData.description = descElem.textContent.trim();
                                console.log('Found team member description:', memberData.description);
                            }
                            
                            // å¦‚æœæ‰¾åˆ°ä»»ä½•ä¿¡æ¯ï¼Œå°±æ·»åŠ åˆ°åœ˜éšŠæˆå“¡åˆ—è¡¨
                            if (memberData.name || memberData.role || memberData.description) {
                                sectionData.teamMembers.push(memberData);
                                console.log('Added team member:', memberData);
                            }
                        });
                    }
                    
                    // æå–æ‰€æœ‰æ–‡æœ¬å…§å®¹
                    const textElements = section.querySelectorAll(`
                        p, 
                        li, 
                        div[class*="text"], 
                        div[class*="content"], 
                        div[class*="description"],
                        div[class*="info"],
                        div[class*="detail"]
                    `);
                    textElements.forEach(elem => {
                        const text = elem.textContent.trim();
                        if (text) {
                            sectionData.content.push(text);
                        }
                    });
                    
                    if (sectionData.title || sectionData.content.length > 0 || sectionData.teamMembers.length > 0) {
                        sections.push(sectionData);
                    }
                });
                
                return sections;
            }''')
            
            # æ ¼å¼åŒ–æå–çš„å…§å®¹
            formatted_sections = []
            for section in content:
                section_text = []
                
                if section['title']:
                    section_text.append(f"\n## {section['title']}")
                    self.logger.info(f"Found section: {section['title']}")
                
                if section['content']:
                    section_text.extend(section['content'])
                    self.logger.info(f"Found {len(section['content'])} content items")
                
                if section['teamMembers']:
                    section_text.append("\n### Team Members:")
                    self.logger.info(f"Found {len(section['teamMembers'])} team members")
                    for member in section['teamMembers']:
                        member_text = []
                        if member['name']:
                            member_text.append(f"Name: {member['name']}")
                            self.logger.info(f"Found team member: {member['name']}")
                        if member['role']:
                            member_text.append(f"Role: {member['role']}")
                            self.logger.info(f"Member role: {member['role']}")
                        if member['description']:
                            member_text.append(f"Description: {member['description']}")
                            self.logger.info(f"Member description: {member['description']}")
                        if member_text:
                            section_text.append("\n" + "\n".join(member_text))
                
                if section_text:
                    formatted_sections.append('\n'.join(section_text))
            
            if formatted_sections:
                formatted_content = f"--- Journey.io Document: {title} ---\n\n" + "\n\n".join(formatted_sections) + "\n\n--- Document End ---"
                self.logger.info("æˆåŠŸæå– Journey.io é é¢å…§å®¹")
                return formatted_content
            
            self.logger.error("âŒ ç„¡æ³•æå–ä»»ä½•æœ‰æ•ˆå…§å®¹")
            return None
            
        except Exception as e:
            self.logger.error(f"è™•ç† Journey.io é é¢æ™‚å‡ºéŒ¯: {str(e)}", exc_info=True)
            return None

#GPT ç¸½çµ        
openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

async def ocr_images_from_urls(image_urls: List[str]) -> str:
    """ä¸‹è¼‰åœ–ç‰‡ä¸¦åŸ·è¡Œ OCR"""
    from pytesseract import pytesseract
    results = []

    for i, url in enumerate(image_urls):
        try:
            # è™•ç† base64 åœ–ç‰‡
            if url.startswith('data:image/'):
                try:
                    import base64
                    # å–å¾— base64 ç·¨ç¢¼éƒ¨åˆ†
                    header, encoded = url.split(",", 1)
                    img_data = base64.b64decode(encoded)
                    img = Image.open(BytesIO(img_data))
                except Exception as e:
                    logger.warning(f"âŒ ç„¡æ³•è§£æ base64 åœ–ç‰‡: {str(e)}")
                    continue
            # è™•ç†ä¸€èˆ¬ URL
            elif url.startswith("http"):
                response = requests.get(url)
                img = Image.open(BytesIO(response.content))
            else:
                logger.warning(f"âŒ ä¸æ”¯æ´çš„åœ–ç‰‡ URL æ ¼å¼: {url[:100]}...")
                continue

            # ä½¿ç”¨ UTF-8 ç·¨ç¢¼è™•ç†æ–‡å­—
            text = pytesseract.image_to_string(img, lang='eng')
            if text and text.strip():
                # ç¢ºä¿æ–‡å­—ä½¿ç”¨ UTF-8 ç·¨ç¢¼
                text_encoded = text.encode('utf-8', errors='ignore').decode('utf-8')
                results.append(f"[Slide {i+1}]\n{text_encoded}")
                
        except Exception as e:
            logger.warning(f"âŒ è®€å–åœ–ç‰‡å¤±æ•—: {str(e)}", exc_info=True)

    return "\n\n".join(results)

async def extract_company_name_from_message(message: str) -> Optional[str]:
    """å¾æ¶ˆæ¯ä¸­æå–å…¬å¸åç¨±"""
    try:
        # 1. æª¢æŸ¥æ˜¯å¦æœ‰æ˜ç¢ºçš„å…¬å¸åç¨±æ¨™è¨˜
        company_patterns = [
            r'(?:^|\n|\s)(?:company|project|startup):\s*([^\n]+)',
            r'(?:^|\n|\s)(?:company|project|startup)\s*name:\s*([^\n]+)',
            r'(?:^|\n|\s)(?:company|project|startup)\s*name\s*is\s*([^\n]+)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:a|an)\s*(?:company|startup|project)',
            r'(?:^|\n|\s)(?:about|introducing|presenting)\s+([A-Z][A-Za-z0-9\s&]+)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:also|working|building)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:governance|related)',
        ]
        
        for pattern in company_patterns:
            match = re.search(pattern, message, re.IGNORECASE | re.MULTILINE)
            if match:
                company_name = match.group(1).strip()
                # æ¸…ç†å…¬å¸åç¨±
                company_name = re.sub(r'\s+', ' ', company_name)  # ç§»é™¤å¤šé¤˜ç©ºæ ¼
                company_name = re.sub(r'[^\w\s&-]', '', company_name)  # åªä¿ç•™å­—æ¯ã€æ•¸å­—ã€ç©ºæ ¼ã€&å’Œ-
                # å¦‚æœå…¬å¸åç¨±å¤ªé•·ï¼Œå¯èƒ½ä¸æ˜¯æ­£ç¢ºçš„åŒ¹é…
                if len(company_name.split()) <= 3:  # å‡è¨­å…¬å¸åç¨±ä¸è¶…é3å€‹å–®è©
                    return company_name
        
        # 2. æª¢æŸ¥æ˜¯å¦æœ‰ "about" æˆ– "introducing" å¾Œé¢çš„åç¨±
        about_patterns = [
            r'(?:^|\n|\s)(?:about|introducing|presenting)\s+([A-Z][A-Za-z0-9\s&]+)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:a|an)\s*(?:company|startup|project)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:also|working|building)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:governance|related)',
        ]
        
        for pattern in about_patterns:
            match = re.search(pattern, message, re.IGNORECASE | re.MULTILINE)
            if match:
                company_name = match.group(1).strip()
                company_name = re.sub(r'\s+', ' ', company_name)
                company_name = re.sub(r'[^\w\s&-]', '', company_name)
                if len(company_name.split()) <= 3:
                    return company_name
        
        # 3. æª¢æŸ¥æ˜¯å¦æœ‰ "Blurb" æˆ– "Description" å¾Œé¢çš„åç¨±
        blurb_patterns = [
            r'(?:^|\n|\s)(?:blurb|description):\s*([^\n]+)',
            r'(?:^|\n|\s)(?:blurb|description)\s*is\s*([^\n]+)',
        ]
        
        for pattern in blurb_patterns:
            match = re.search(pattern, message, re.IGNORECASE | re.MULTILINE)
            if match:
                text = match.group(1).strip()
                # å˜—è©¦å¾æè¿°ä¸­æå–å…¬å¸åç¨±
                company_match = re.search(r'([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:a|an)\s*(?:company|startup|project)', text)
                if company_match:
                    company_name = company_match.group(1).strip()
                    company_name = re.sub(r'\s+', ' ', company_name)
                    company_name = re.sub(r'[^\w\s&-]', '', company_name)
                    if len(company_name.split()) <= 3:
                        return company_name
        
        # 4. ç‰¹æ®Šæƒ…æ³ï¼šç›´æ¥æŸ¥æ‰¾å¤§å¯«é–‹é ­çš„å–®è©
        words = message.split()
        for i, word in enumerate(words):
            if word[0].isupper() and len(word) > 1 and word.isalpha():
                # æª¢æŸ¥é€™å€‹è©æ˜¯å¦å¯èƒ½æ˜¯å…¬å¸åç¨±
                if i > 0 and words[i-1].lower() in ['about', 'introducing', 'presenting', 'company', 'project', 'startup']:
                    return word
                if i < len(words)-1 and words[i+1].lower() in ['is', 'are', 'working', 'building']:
                    return word
        
        return None
    except Exception as e:
        logger.error(f"æå–å…¬å¸åç¨±æ™‚å‡ºéŒ¯: {e}")
        return None

async def summarize_pitch_deck(ocr_text: str, message: str = "") -> Dict:
    """ç”¨ GPT æ‘˜è¦ Pitch Deck OCR æ–‡å­—ç‚ºçµæ§‹åŒ–å¤§ç¶±ä¸¦å›å‚³ dict"""
    # ç¢ºä¿è¼¸å…¥æ–‡å­—æ˜¯ UTF-8 ç·¨ç¢¼
    if isinstance(ocr_text, str):
        ocr_text = ocr_text.encode('utf-8', errors='ignore').decode('utf-8')

    # é¦–å…ˆå˜—è©¦å¾æ¶ˆæ¯ä¸­æå–å…¬å¸åç¨±
    company_name = None
    if message:
        company_name = await extract_company_name_from_message(message)
        if company_name:
            logger.info(f"å¾æ¶ˆæ¯ä¸­æå–åˆ°å…¬å¸åç¨±: {company_name}")

    # å¦‚æœæ²’æœ‰å¾æ¶ˆæ¯ä¸­æ‰¾åˆ°å…¬å¸åç¨±ï¼Œå˜—è©¦å¾ Deck å…§å®¹ä¸­æå–
    if not company_name:
        # ä½¿ç”¨ GPT å¾ Deck å…§å®¹ä¸­æå–å…¬å¸åç¨±
        try:
            completion = await openai_client.chat.completions.create(
                model="gpt-4.1",
                messages=[
                    {"role": "system", "content": "You are a professional analyst helping investors extract company names from pitch decks."},
                    {"role": "user", "content": f"Please extract the company name from this pitch deck content. If you find multiple possible names, choose the most likely one. Return only the company name, nothing else.\n\nContent:\n{ocr_text[:2000]}"}  # åªä½¿ç”¨å‰2000å€‹å­—ç¬¦ä¾†æå–å…¬å¸åç¨±
                ]
            )
            extracted_name = completion.choices[0].message.content.strip()
            if extracted_name and len(extracted_name) < 50:  # ç¢ºä¿æå–çš„åç¨±åˆç†
                company_name = extracted_name
                logger.info(f"å¾ Deck å…§å®¹ä¸­æå–åˆ°å…¬å¸åç¨±: {company_name}")
        except Exception as e:
            logger.warning(f"å¾ Deck å…§å®¹æå–å…¬å¸åç¨±å¤±æ•—: {str(e)}")

    # ä½¿ç”¨ GoogleSheetPromptManager ç²å–æç¤ºè©
    prompt = prompt_manager.get_prompt_and_format(
        'summarize_pitch_deck',
        company_name=company_name if company_name else 'Unknown Company',
        ocr_text=ocr_text,  # æ·»åŠ  ocr_text åƒæ•¸
    )

    try:
        completion = await openai_client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": "You are a professional analyst helping investors organize Pitch Deck information."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"}
        )
        raw_output = json.loads(completion.choices[0].message.content)
        
        # å¦‚æœå¾æ¶ˆæ¯æˆ– Deck ä¸­æå–åˆ°äº†å…¬å¸åç¨±ï¼Œç¢ºä¿ä½¿ç”¨é€™å€‹åç¨±
        if company_name:
            raw_output["company"] = company_name
            
        logger.info(f"æˆåŠŸæå–Deckä¿¡æ¯")       
        return raw_output
    except Exception as e:
        logger.error(f"GPT è™•ç†å¤±æ•—ï¼š{str(e)}")
        return {
            "error": f"åˆ†æå¤±æ•—: {str(e)}",
            "company": company_name if company_name else "æœªçŸ¥å…¬å¸",
            "summary": "åˆ†æå¤±æ•—"
        }
    

async def debug_all_iframes(page):
    """
    å„²å­˜æ‰€æœ‰ iframe çš„ HTML ä¸¦åˆ—å‡ºé—œéµè³‡è¨Šï¼ˆæ–‡å­—ã€åœ–ç‰‡ã€canvas æ•¸é‡ï¼‰ã€‚
    """
    frames = page.frames
    print(f"\nğŸ” æª¢æ¸¬åˆ° {len(frames)} å€‹ iframeã€‚")

    for idx, frame in enumerate(frames):
        print(f"\nğŸ“¦ è§£æ iframe [{idx}] URL: {frame.url}")

        try:
            # å„²å­˜ HTML
            html = await frame.content()
            from utils.path_helper import PathHelper as _PH
            _ph = _PH()
            file_path = _ph.get(f"debug_frame_{idx}.html")
            with file_path.open("w", encoding="utf-8") as f:
                f.write(html)
            print(f"ğŸ“ å·²å„²å­˜ HTML åˆ° {file_path}")

            # æŠ“ä¸»è¦å…ƒç´ æ•¸é‡
            text_count = await frame.eval_on_selector_all('p', 'els => els.length')
            div_count = await frame.eval_on_selector_all('div', 'els => els.length')
            img_count = await frame.eval_on_selector_all('img', 'els => els.length')
            canvas_count = await frame.eval_on_selector_all('canvas', 'els => els.length')

            print(f"ğŸ“Š å…§å®¹çµ±è¨ˆï¼š")
            print(f" - <p> æ¨™ç±¤ï¼š{text_count}")
            print(f" - <div> æ¨™ç±¤ï¼š{div_count}")
            print(f" - <img> åœ–ç‰‡ï¼š{img_count}")
            print(f" - <canvas> å…ƒç´ ï¼š{canvas_count}")

            # é¡¯ç¤ºå¯èƒ½æ€§åˆ†æ
            if canvas_count > 0:
                print("âš ï¸ ç™¼ç¾ canvasï¼Œå¯èƒ½æ˜¯åœ–åƒæ¸²æŸ“çš„ slideã€‚")
            if text_count > 5 or div_count > 20:
                print("âœ… å¯èƒ½æœ‰æ–‡å­—å…§å®¹ã€‚")
            if img_count > 5 and text_count < 2:
                print("ğŸ–¼ï¸ å¾ˆå¯èƒ½æ˜¯ç´”åœ–ç‰‡å‹æŠ•å½±ç‰‡ã€‚")

        except Exception as e:
            print(f"âŒ è®€å– iframe {idx} ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
    

#æ¸¬è©¦ç”¨é©…å‹•å™¨
if __name__ == "__main__":

    async def main():
        message = """
        # ***CHOMP***

        **CHOMP's current flagship front end feels like a quiz game.** Under the hood, it's a new kind of information market primitive we call *vibe markets*.

        Users bet on what they believe and what they think others believe, and CHOMP generates the best answers that reveals what the crowd *really* thinks. The data these markets create powers a new form of more trustworthy content, one created via manipulation-resistance and honest participation; properties guaranteed by the market design.

        Builders are already using CHOMP's market primitive as an oracle to resolve their own markets. As it scales, CHOMP becomes the engine for coordinating humans, businesses, and agents to produce trustworthy opinionated information that reveals hidden beliefsâ€”powering a more trustworthy internet.

        The current front end has seen:

        - 40K+ all-time users
        - 1.6M+ total bets made
        - 72% Average MoM Growth in Q1 2025

        Built by a team with deep expertise across **Web2, Web3, and high-frequency markets**. Ex-Orca, IDEO, Mediacom, Deloitte, and more.

        CHOMP won the Colosseum Hackathon and Bonkathon. The first third-party product built on top of CHOMP launches this June.

        ## Start here

        ---

        [Why CHOMP? ](https://www.notion.so/Why-CHOMP-1bbdfb1cb436811db3a0fbd9c1010c1d?pvs=21)

        [Why us? ](https://www.notion.so/Why-us-1acdfb1cb4368007b9d5d78c5be28a9a?pvs=21)

        [Fundraising Details](https://www.notion.so/Fundraising-Details-1addfb1cb43680da916cde9c2bc05578?pvs=21)

        [Metrics To Date](https://www.notion.so/Metrics-To-Date-fffdfb1cb43681ecb692e6d4c37105b6?pvs=21)

        [Revenue Model](https://www.notion.so/Revenue-Model-1cadfb1cb43680d4bccdda7fdad45668?pvs=21)


        """
        
        reader = DeckBrowser()
        try:
            await reader.initialize()
            results = await reader.process_input(message)
            if results:
                print(json.dumps(results[0], ensure_ascii=False, indent=2))
            else:
                print("âš ï¸ æ²’æœ‰æ“·å–åˆ°ä»»ä½•çµæœ")
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            await reader.close()
            # ç¢ºä¿æ‰€æœ‰è³‡æºéƒ½è¢«é‡‹æ”¾
            import gc
            gc.collect()

    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("Bot stopped by user.")
    except Exception as e:
        print(f"Fatal error: {e}")
    finally:
        # ç¢ºä¿æ‰€æœ‰è³‡æºéƒ½è¢«é‡‹æ”¾
        import gc
        gc.collect()
