import os
from io import BytesIO  # ç§»åˆ°æœ€ä¸Šé¢
import logging
from dotenv import load_dotenv
from utils.path_helper import PathHelper
import re
import asyncio
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page
from typing import Optional, List, Dict, Literal, Any
import random
from PIL import Image
import pytesseract
import requests
from openai import AsyncOpenAI
import json
import fitz  # PyMuPDF for PDF
import tempfile
from pptx import Presentation

# Load environment variables
load_dotenv()

# Pytesseract Path
pytesseract.pytesseract.tesseract_cmd = os.getenv('TESSERACT')


# é…ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)
# 5. é…ç½® Tesseract è·¯å¾‘
tesseract_path = os.getenv('TESSERACT')

# ç¢ºä¿ logger æœ‰ handler
if not logger.handlers:
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

logger.info(f"Final Tesseract path: {pytesseract.pytesseract.tesseract_cmd}")

logger.setLevel(logging.INFO)

class DeckBrowser:
    
    def __init__(self):
        """Initialize the DeckBrowser."""
        self.browser = None
        self.logger = logging.getLogger(__name__)
        self.email = os.getenv("DOCSEND_EMAIL")  # æ›¿æ›ç‚ºæ‚¨çš„é›»å­éƒµä»¶
        self.path_helper = PathHelper()

    #æ±ºå®šæµç¨‹
    SourceType = Literal["docsend", "attachment", "gdrive", "website", "unknown"]
    
    @staticmethod
    def detect_source_type(message: str, attachments: Optional[list] = None) -> "DeckBrowser.SourceType":
        if "docsend.com" in message.lower():
            return "docsend"
        elif attachments and any(
            att.get("name", "").lower().endswith(('.pdf', '.pptx', '.ppt'))
            for att in attachments if isinstance(att, dict)
        ):
            return "attachment"
        elif re.search(r"https://(?:drive|docs)\.google\.com/(?:file/d/|presentation/)[\w\-/]+", message):
            return "gdrive"
        elif re.search(r"https?://[^\s\)\"]+", message):  # åŒ¹é…ä»»ä½•ç¶²å€
            return "website"
        else:
            return "unknown"
    
    async def process_input(self, message: str, attachments: Optional[list] = None):
        source_type = self.detect_source_type(message, attachments)

        if source_type == "docsend":
            self.logger.info(f"é–‹å§‹è™•ç† Docsend")
            return await self.run_docsend_analysis(message)
        elif source_type == "attachment":
            self.logger.info(f"é–‹å§‹è™•ç† Attachment")
            return await self.run_file_analysis(attachments)
        elif source_type == "gdrive":
            self.logger.info(f"é–‹å§‹è™•ç† Google Drive")
            return await self.run_gdrive_analysis(message)
        elif source_type == "website":
            self.logger.info("ğŸ”— åµæ¸¬ç‚ºä¸€èˆ¬ç¶²ç«™ï¼Œé–‹å§‹æ“·å–ç¶²é å…§å®¹é€²è¡Œåˆ†æ")
            return await self.run_generic_link_analysis(message)
        else:
            self.logger.info(f"Unkown")
            # æ–°å¢ï¼šå…è¨±ç´”æ–‡å­—ç›´æ¥ä¸Ÿçµ¦ GPT
            self.logger.info("æœªåµæ¸¬åˆ°é€£çµæˆ–é™„ä»¶ï¼Œç›´æ¥åˆ†æç´”æ–‡å­—å…§å®¹")
            summary = await summarize_pitch_deck(message, message)  # å‚³å…¥ message å…©æ¬¡ï¼Œä¸€æ¬¡ä½œç‚ºå…§å®¹ï¼Œä¸€æ¬¡ç”¨æ–¼æå–å…¬å¸åç¨±
            return [summary] if summary else [{"error": "âŒ ç´”æ–‡å­—åˆ†æå¤±æ•—"}]

    async def run_gdrive_analysis(self, message: str) -> Dict[str, Any]:
        self.logger.info(f"ğŸ“¥ é–‹å§‹è™•ç† Google Drive é€£çµ: {message}")

        gdrive_match = re.search(r"https://drive\.google\.com/file/d/([\w-]+)", message)
        if not gdrive_match:
            gdrive_match = re.search(r"id=([\w-]+)", message)

        if not gdrive_match:
            return {"error": "âŒ ç„¡æ³•å¾è¨Šæ¯ä¸­æ“·å– Google Drive æª”æ¡ˆ IDã€‚è«‹ç¢ºèªé€£çµæ ¼å¼ã€‚"}

        file_id = gdrive_match.group(1)
        export_url = f"https://drive.google.com/uc?export=download&id={file_id}"

        try:
            response = requests.get(export_url)
            response.raise_for_status()

            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(response.content)
                tmp_path = tmp.name

            self.logger.info(f"ğŸ“„ æˆåŠŸä¸‹è¼‰ PDF æª”æ¡ˆè‡³ {tmp_path}ï¼Œé–‹å§‹åŸ·è¡Œåˆ†æ...")

            # ä½¿ç”¨ Playwright æ‰“é–‹ PDF æ–‡ä»¶
            await self.initialize()
            page = await self._get_page(f"file://{tmp_path}")
            
            try:
                # ç­‰å¾… PDF åŠ è¼‰
                await page.wait_for_load_state('networkidle', timeout=30000)
                
                # æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰å…§å®¹
                self.logger.info("é–‹å§‹æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰å…§å®¹")
                last_height = await page.evaluate('document.body.scrollHeight')
                while True:
                    # æ»¾å‹•åˆ°åº•éƒ¨
                    await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                    # ç­‰å¾…æ–°å…§å®¹åŠ è¼‰
                    await page.wait_for_timeout(2000)
                    # è¨ˆç®—æ–°çš„æ»¾å‹•é«˜åº¦
                    new_height = await page.evaluate('document.body.scrollHeight')
                    if new_height == last_height:
                        break
                    last_height = new_height
                    self.logger.info("ç¹¼çºŒæ»¾å‹•ä»¥åŠ è¼‰æ›´å¤šå…§å®¹")
                
                # æ»¾å‹•å›é ‚éƒ¨
                await page.evaluate('window.scrollTo(0, 0)')
                await page.wait_for_timeout(1000)
                
                # æˆªåœ–ä»¥ä¾¿èª¿è©¦
                debug_screenshot_name = f"gdrive_debug_{random.randint(1000, 9999)}.png"
                debug_screenshot_path = str(self.path_helper.get("tmp", debug_screenshot_name))
                self.path_helper.ensure_dir("tmp")
                await page.screenshot(path=debug_screenshot_path)
                self.logger.info(f"ä¿å­˜é é¢æˆªåœ–è‡³: {debug_screenshot_path}")
                
                # æå–å…§å®¹
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')
                
                # æå–æ–‡æœ¬
                text_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div'])
                extracted_text = []
                
                for elem in text_elements:
                    text = elem.get_text().strip()
                    if text and len(text) > 10:
                        extracted_text.append(text)
                
                if not extracted_text:
                    # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ–‡æœ¬ï¼Œå˜—è©¦ OCR
                    self.logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°æ–‡æœ¬å…§å®¹ï¼Œå˜—è©¦ OCR")
                    img_tags = soup.find_all('img')
                    image_urls = [img['src'] for img in img_tags if img.get('src')]
                    
                    if image_urls:
                        ocr_text = await ocr_images_from_urls(image_urls)
                        if ocr_text:
                            summary = await summarize_pitch_deck(ocr_text, message)
                            await page.close()
                            await self.close()
                            return summary
                
                # æ ¼å¼åŒ–å…§å®¹
                formatted_content = "\n\n".join(extracted_text)
                summary = await summarize_pitch_deck(formatted_content, message)
                
                await page.close()
                await self.close()
                return summary
                
            except Exception as e:
                self.logger.error(f"è™•ç† PDF æ™‚å‡ºéŒ¯: {e}")
                await page.close()
                await self.close()
                return {"error": f"âŒ è™•ç† PDF å¤±æ•—: {str(e)}"}
                
        except Exception as e:
            self.logger.error(f"âŒ ä¸‹è¼‰æˆ–è™•ç† Google Drive æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            return {"error": f"âŒ Google Drive æª”æ¡ˆè™•ç†å¤±æ•—ï¼š{str(e)}"}
            
    async def run_file_analysis(self, attachments: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """
        è™•ç† PDF/PPTX é™„ä»¶ï¼šåŸ·è¡Œ OCR æˆ–çµæ§‹åŒ–æ‘˜è¦
        """
        results = []

        for file in attachments:
            path = file.get("path")
            name = file.get("name", "unnamed")
            suffix = self.path_helper.get(name).suffix.lower()

            self.logger.info(f"ğŸ“‚ é–‹å§‹åˆ†æé™„ä»¶: {name}")

            extracted_text = ""

            try:
                # Check file size before processing
                file_path_obj = self.path_helper.get(path)
                if not file_path_obj.exists() or file_path_obj.stat().st_size < 1024:
                    self.logger.error(f"âŒ æª”æ¡ˆ {name} ({path}) å¤ªå°æˆ–ä¸å­˜åœ¨ï¼Œå¯èƒ½ä¸‹è¼‰å¤±æ•—ã€‚")
                    results.append({"error": f"âŒ æª”æ¡ˆ {name} ä¸‹è¼‰å¤±æ•—æˆ–ä¸æ˜¯æœ‰æ•ˆçš„æª”æ¡ˆã€‚"})
                    continue

                if suffix == ".pdf":
                    doc = fitz.open(str(file_path_obj))
                    for page in doc:
                        extracted_text += page.get_text("text") + "\n"
                    doc.close()

                elif suffix == ".pptx":
                    try:
                        prs = Presentation(str(file_path_obj))
                    except Exception as e:
                        self.logger.error(f"âŒ ç„¡æ³•é–‹å•Ÿ PPTX æª”æ¡ˆ {name}: {type(e).__name__}: {e}")
                        results.append({"error": f"âŒ ç„¡æ³•é–‹å•Ÿ PPTX æª”æ¡ˆ {name}: {type(e).__name__}: {e}"})
                        continue
                    for i, slide in enumerate(prs.slides):
                        for shape in slide.shapes:
                            if hasattr(shape, "text"):
                                extracted_text += f"[Slide {i+1}]\n{shape.text}\n"

                if not extracted_text.strip():
                    self.logger.warning(f"âš ï¸ {name} æ²’æœ‰æ“·å–åˆ°æ–‡å­—ï¼Œé–‹å§‹åŸ·è¡Œ OCR åœ–ç‰‡è¾¨è­˜")

                    image_urls = []
                    if suffix == ".pdf":
                        doc = fitz.open(str(file_path_obj))
                        for page_num, page in enumerate(doc):
                            pix = page.get_pixmap(dpi=200)
                            img_path_obj = self.path_helper.get(f"{file_path_obj}_page_{page_num}.png")
                            pix.save(str(img_path_obj))
                            image_urls.append(f"file://{img_path_obj}")
                        doc.close()

                    elif suffix == ".pptx":
                        self.logger.warning("âŒ å°šæœªå¯¦ä½œ PPTX é é¢è½‰åœ–ç‰‡çš„ OCR fallback")
                        continue

                    if image_urls:
                        ocr_text = await ocr_images_from_urls(image_urls)
                        if ocr_text:
                            summary = await summarize_pitch_deck(ocr_text, name)
                            if summary:
                                results.append(summary)
                                continue

                else:
                    summary = await summarize_pitch_deck(extracted_text, name)
                    if summary:
                        results.append(summary)

            except Exception as e:
                self.logger.error(f"âŒ åˆ†ææª”æ¡ˆ {name} ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                results.append({"error": f"âŒ åˆ†æå¤±æ•—: {name}"})

        return results if results else [{"error": "âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•é™„ä»¶å…§å®¹"}]

    #ä¸»è¦ Docsend é©…å‹•å¼
    async def run_docsend_analysis(self, message: str) -> Dict[str, Any]:
        """
        å°è£å®Œæ•´æµç¨‹ï¼šåˆå§‹åŒ– -> æ“·å– URL -> æŠ½å–å…§å®¹ -> é—œé–‰ç€è¦½å™¨ -> å›å‚³çµæœ
        """
        await self.initialize()
        
        results = []  # List to store JSON results
        urls = await self.extract_docsend_links(message)
        for url in urls:
            content = await self.read_docsend_document(url)
            if isinstance(content, dict):
                results.append(content)
            elif isinstance(content, str):
                summarized = await summarize_pitch_deck(content, message)
                if summarized:
                    results.append(summarized)

        await self.close()
        
        return results if results else [{"error": "âŒ æ²’æœ‰æˆåŠŸæ“·å–ä»»ä½• DocSend æ–‡æª”å…§å®¹"}]

    async def initialize(self):
        """Initialize the browser instance asynchronously."""
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(headless=False)
            self.logger.info("Browser initialized successfully")
        except Exception as e:
            self.logger.error(f"Failed to initialize browser: {str(e)}")
            raise

    async def close(self):
        """é—œé–‰ç€è¦½å™¨"""
        try:
            if self.browser:
                await self.browser.close()
                self.browser = None
            if hasattr(self, 'playwright'):
                await self.playwright.stop()
        except Exception as e:
            self.logger.error(f"Error closing browser: {e}")
        finally:
            # ç¢ºä¿æ‰€æœ‰è³‡æºéƒ½è¢«é‡‹æ”¾
            import gc
            gc.collect()

    async def extract_docsend_links(self, text: str) -> List[str]:
        """å¾æ–‡æœ¬ä¸­æå– DocSend é€£çµ"""
        docsend_pattern = r'https?://(?:www\.)?docsend\.com/[^\s)"}]+'
        return re.findall(docsend_pattern, text)
    
    async def _get_page(self, url: str) -> Page:
        """Get a new page with configured User-Agent and other settings."""
        page = await self.browser.new_page()
        await page.set_viewport_size({'width': 1920, 'height': 1080})
        await page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'sec-ch-ua': '"Chromium";v="122", "Not(A:Brand";v="24", "Google Chrome";v="122"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"'
        })
        
        return page

    async def read_docsend_document(self, url: str) -> Optional[str]:
        """è®€å– DocSend æ–‡æª”çš„å…§å®¹"""
        try:
            self.logger.info(f"æ­£åœ¨å¾ DocSend é€£çµè®€å–å…§å®¹: {url}")
            
            # å‰µå»ºæ–°é é¢
            page = await self._get_page(url)
            
            # è¨ªå• DocSend é é¢
            response = await page.goto(url, wait_until='networkidle', timeout=30000)
            
            self.logger.info(f"è¨ªå•ç‹€æ…‹ç¢¼: {response.status}")
            
            if response.status == 403:
                self.logger.error("è¨ªå•è¢«æ‹’çµ• (403 Forbidden)")
                await page.close()
                return None
            
            # éš¨æ©Ÿç­‰å¾… 2-5 ç§’
            await asyncio.sleep(random.uniform(2, 5))
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦å¡«å¯«é›»å­éƒµä»¶
            email_input = await page.query_selector('input[type="email"]')
            if email_input:
                self.logger.info("éœ€è¦å¡«å¯«é›»å­éƒµä»¶æ‰èƒ½è¨ªå•æ–‡æª”")
                await page.type('input[type="email"]', self.email, delay=random.uniform(100, 200))
                try:
                    self.logger.info("å˜—è©¦é»æ“Šæäº¤æŒ‰éˆ•")
                    await page.locator('button:has-text("Continue")').wait_for(state='visible', timeout=1000)
                    await page.locator('button:has-text("Continue")').click(timeout=1000)
                except Exception as e:
                    self.logger.warning(f"æäº¤æŒ‰éˆ•é»æ“Šå¤±æ•—: {e}")
                await page.wait_for_load_state('networkidle', timeout=30000)

            # æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰å…§å®¹
            self.logger.info("é–‹å§‹æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰å…§å®¹")
            last_height = await page.evaluate('document.body.scrollHeight')
            while True:
                # æ»¾å‹•åˆ°åº•éƒ¨
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                # ç­‰å¾…æ–°å…§å®¹åŠ è¼‰
                await page.wait_for_timeout(2000)
                # è¨ˆç®—æ–°çš„æ»¾å‹•é«˜åº¦
                new_height = await page.evaluate('document.body.scrollHeight')
                if new_height == last_height:
                    break
                last_height = new_height
                self.logger.info("ç¹¼çºŒæ»¾å‹•ä»¥åŠ è¼‰æ›´å¤šå…§å®¹")
            
            # æ»¾å‹•å›é ‚éƒ¨
            await page.evaluate('window.scrollTo(0, 0)')
            await page.wait_for_timeout(1000)
            
            # å˜—è©¦é»æ“Šä¸‹ä¸€é æŒ‰éˆ•åŠ è¼‰æ›´å¤šå…§å®¹
            self.logger.info("å˜—è©¦é€šéé»æ“Šä¸‹ä¸€é æŒ‰éˆ•åŠ è¼‰æ›´å¤šå…§å®¹")
            try:
                next_buttons = await page.query_selector_all('button[aria-label*="next"], button[aria-label*="Next"], button[title*="next"], button[title*="Next"]')
                for button in next_buttons:
                    for _ in range(10):  # æœ€å¤šé»æ“Š10æ¬¡
                        try:
                            await button.click()
                            await page.wait_for_timeout(1000)  # ç­‰å¾…æ–°é é¢åŠ è¼‰
                        except:
                            break
            except Exception as e:
                self.logger.warning(f"é»æ“Šä¸‹ä¸€é æŒ‰éˆ•æ™‚å‡ºéŒ¯: {e}")
            
            # å†æ¬¡æ»¾å‹•ä»¥ç¢ºä¿æ‰€æœ‰å…§å®¹éƒ½å·²åŠ è¼‰
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            await page.wait_for_timeout(2000)
            
            # æˆªåœ–ä»¥ä¾¿èª¿è©¦
            debug_screenshot_name = f"docsend_debug_{random.randint(1000, 9999)}.png"
            debug_screenshot_path = str(self.path_helper.get("tmp", debug_screenshot_name))
            self.path_helper.ensure_dir("tmp")
            await page.screenshot(path=debug_screenshot_path)
            self.logger.info(f"ä¿å­˜é é¢æˆªåœ–è‡³: {debug_screenshot_path}")
            
            # Debug å°‡æ•´é  HTML å„²å­˜ä¸‹ä¾†
            html = await page.content()
            debug_html_path = self.path_helper.get("debug_docsend.html")
            with debug_html_path.open("w", encoding="utf-8") as f:
                f.write(html)

            # ç­‰å¾…æ–‡æª”å…§å®¹åŠ è¼‰
            target_frame = None
            content_selectors = ['.document-content', '.page', '.viewer-content', '.ds-viewer-container']

            for frame in page.frames:
                frame_url = frame.url or ""
                if "docsend.com/view" in frame_url and "marketing.docsend.com" not in frame_url:
                    for selector in content_selectors:
                        try:
                            await frame.wait_for_selector(selector, timeout=5000)
                            self.logger.info(f"åœ¨ iframe ä¸­æ‰¾åˆ°å…§å®¹: {selector}")
                            target_frame = frame
                            break
                        except:
                            continue
                if target_frame:
                    break

            if not target_frame:
                self.logger.warning("âš ï¸ æœªèƒ½æ‰¾åˆ°å«æ–‡å­— selectorï¼Œé–‹å§‹ debug æ‰€æœ‰ iframe...")
                await debug_all_iframes(page)

                # å˜—è©¦å¾æ‰€æœ‰ iframe ä¸­æ“·å–åœ–ç‰‡åš OCR
                for frame in page.frames:
                    try:
                        frame_html = await frame.content()
                        soup = BeautifulSoup(frame_html, "html.parser")
                        img_tags = soup.find_all("img")
                        image_urls = [img["src"] for img in img_tags if img.get("src")]
                        
                        if image_urls:
                            self.logger.info(f"âœ… åœ¨ iframe ä¸­æ‰¾åˆ°åœ–ç‰‡ï¼šå…± {len(image_urls)} å¼µï¼Œæº–å‚™åŸ·è¡Œ OCR")
                            ocr_raw_text = await ocr_images_from_urls(image_urls)
                            if ocr_raw_text:
                                summarized_text = await summarize_pitch_deck(ocr_raw_text, url)
                                await page.close()
                                return summarized_text
                    except Exception as e:
                        self.logger.warning(f"è®€å– iframe å…§å®¹å¤±æ•—ï¼š{e}")

                self.logger.error("âŒ æ‰€æœ‰ iframe å‡æœªèƒ½æä¾›å…§å®¹æˆ–åœ–ç‰‡")
                await page.close()
                return None
            
            # å¾ iframe æŠ“ HTML
            html = await target_frame.content()
            self.logger.info(f"ç²å–çš„HTMLå…§å®¹é•·åº¦: {len(html)} å­—ç¬¦")
            
            # è§£æ HTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # æå–æ–‡æª”æ¨™é¡Œ
            title_elem = soup.find('title')
            if not title_elem:
                self.logger.info("iframe ä¸­æœªæ‰¾åˆ° <title>ï¼Œå˜—è©¦å¾ä¸»é æŠ“å– title")
                main_html = await page.content()
                main_soup = BeautifulSoup(main_html, 'html.parser')
                title_elem = main_soup.find('title')

            title = title_elem.text.strip() if title_elem else "DocSend Document"
            self.logger.info(f"æå–çš„æ–‡æª”æ¨™é¡Œ: {title}")
            
            # æå–æ–‡æª”å…§å®¹
            text_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div'])
            self.logger.info(f"æ‰¾åˆ° {len(text_elements)} å€‹æ–‡æœ¬å…ƒç´ ")
            
            extracted_text = []
            
            for elem in text_elements:
                text = elem.get_text().strip()
                if text and len(text) > 10:  # åªä¿ç•™æœ‰æ„ç¾©çš„æ–‡æœ¬
                    extracted_text.append(text)
            
            self.logger.info(f"ç¯©é¸å¾Œæå–äº† {len(extracted_text)} å€‹æœ‰æ„ç¾©çš„æ–‡æœ¬æ®µè½")
            extracted_text = "\n\n".join(extracted_text)
            
            # é—œé–‰é é¢
            await page.close()
            
            if not extracted_text or len(extracted_text) < 100:
                self.logger.warning("âš ï¸ æ–‡å­—å…§å®¹éå°‘ï¼Œå˜—è©¦ OCR åœ–ç‰‡ä¸¦ç”¨ GPT æ‘˜è¦")
                img_tags = soup.find_all('img')
                image_urls = [img['src'] for img in img_tags if img.get('src')]

                if image_urls:
                    ocr_raw_text = await ocr_images_from_urls(image_urls)
                    if ocr_raw_text:
                        summarized_text = await summarize_pitch_deck(ocr_raw_text, url)
                        extracted_text = summarized_text
            
            # è¨˜éŒ„å…§å®¹æ‘˜è¦
            content_preview = extracted_text[:100] + "..." if len(extracted_text) > 100 else extracted_text
            self.logger.info(f"æå–æ–‡æœ¬é è¦½: {content_preview}")
            self.logger.info(f"ç¸½æå–æ–‡æœ¬é•·åº¦: {len(extracted_text)} å­—ç¬¦")
            
            # æ ¼å¼åŒ–æå–çš„å…§å®¹
            formatted_content = f"--- DocSend æ–‡æª”: {title} ---\n\n{extracted_text}\n\n--- DocSend æ–‡æª”çµæŸ ---"
            
            self.logger.info(f"æˆåŠŸå¾ DocSend é€£çµæå–å…§å®¹: {url}")
            return formatted_content
        
        except Exception as e:
            self.logger.error(f"è®€å– DocSend æ–‡æª”æ™‚å‡ºéŒ¯: {str(e)}", exc_info=True)
            if 'page' in locals():
                await page.close()
            return None

    async def is_pitch_deck(self, page) -> bool:
        """åˆ¤æ–·ç¶²é æ˜¯å¦ç‚º Pitch Deck"""
        try:
            # 0. é¦–å…ˆæª¢æŸ¥æ˜¯å¦ç‚ºç¤¾ç¾¤ç¶²ç«™
            url = page.url.lower()
            social_media_domains = ['x.com', 'twitter.com', 'facebook.com', 'linkedin.com', 'instagram.com']
            if any(domain in url for domain in social_media_domains):
                return False

            # 1. æª¢æŸ¥ URL é—œéµå­—
            deck_keywords = ['pitch', 'deck', 'presentation', 'slides', 'investor', 'fundraising']
            if any(keyword in url for keyword in deck_keywords):
                return True

            # 2. æª¢æŸ¥é é¢æ¨™é¡Œ
            title = await page.title()
            if title and any(keyword in title.lower() for keyword in deck_keywords):
                return True

            # 3. æª¢æŸ¥é é¢å…§å®¹ç‰¹å¾µ
            content = await page.content()
            soup = BeautifulSoup(content, "html.parser")
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡ç›¸é—œçš„å…ƒç´ 
            slide_indicators = [
                # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡å®¹å™¨
                bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['slide', 'deck', 'presentation']))),
                # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡å°èˆª
                bool(soup.find('nav', class_=lambda x: x and any(word in str(x).lower() for word in ['slide', 'deck', 'presentation']))),
                # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡è¨ˆæ•¸å™¨
                bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['counter', 'progress', 'slide-number']))),
                # æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡æ§åˆ¶æŒ‰éˆ•
                bool(soup.find('button', class_=lambda x: x and any(word in str(x).lower() for word in ['next', 'prev', 'slide']))),
            ]
            
            if any(slide_indicators):
                return True

            # 4. æª¢æŸ¥é é¢çµæ§‹
            # è¨ˆç®—é é¢ä¸­çš„åœ–ç‰‡æ•¸é‡
            images = await page.query_selector_all('img')
            # è¨ˆç®—é é¢ä¸­çš„æ–‡æœ¬å¡Šæ•¸é‡
            text_blocks = await page.query_selector_all('p, h1, h2, h3, h4, h5, h6')
            
            # å¦‚æœåœ–ç‰‡æ•¸é‡è¼ƒå¤šä¸”æ–‡æœ¬å¡Šè¼ƒå°‘ï¼Œå¯èƒ½æ˜¯æŠ•å½±ç‰‡
            # ä½†éœ€è¦ç¢ºä¿ä¸æ˜¯ç¤¾ç¾¤ç¶²ç«™çš„çµæ§‹
            if len(images) > 5 and len(text_blocks) < 10:
                # æª¢æŸ¥æ˜¯å¦æœ‰ç¤¾ç¾¤ç¶²ç«™ç‰¹æœ‰çš„å…ƒç´ 
                social_indicators = [
                    bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['tweet', 'post', 'status', 'feed', 'timeline']))),
                    bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['profile', 'avatar', 'user-info']))),
                    bool(soup.find('div', class_=lambda x: x and any(word in str(x).lower() for word in ['like', 'share', 'comment', 'retweet']))),
                ]
                if not any(social_indicators):
                    return True

            # 5. æª¢æŸ¥æ˜¯å¦æœ‰æŠ•å½±ç‰‡ç›¸é—œçš„ JavaScript
            scripts = await page.query_selector_all('script')
            for script in scripts:
                content = await script.text_content()
                if any(keyword in content.lower() for keyword in ['slideshow', 'presentation', 'deck']):
                    return True

            return False

        except Exception as e:
            self.logger.error(f"Error checking if page is pitch deck: {e}")
            return False

    async def run_generic_link_analysis(self, message: str) -> List[Dict[str, Any]]:
        """åˆ†æä¸€èˆ¬ç¶²å€ï¼ˆåŒ…æ‹¬å…¬å¸å®˜ç¶²ï¼‰"""
        urls = re.findall(r'https?://[^\s\)\"]+', message)
        results = []
        
        async with async_playwright() as pw:
            browser = await pw.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            )
            
            for url in urls:
                page = await context.new_page()
                try:
                    self.logger.info(f"ğŸŒ é–‹å§‹åˆ†æç¶²å€: {url}")
                    
                    # è¨ªå•ç¶²é 
                    await page.goto(url, wait_until='domcontentloaded', timeout=30000)
                    
                    # ç­‰å¾…é é¢åŠ è¼‰å®Œæˆ
                    await page.wait_for_load_state('networkidle', timeout=10000)
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚º Pitch Deck
                    is_deck = await self.is_pitch_deck(page)
                    if is_deck:
                        self.logger.info("âœ… æª¢æ¸¬åˆ° Pitch Deckï¼Œä½¿ç”¨ç‰¹æ®Šè™•ç†æ–¹å¼")
                        # å¦‚æœæ˜¯ Pitch Deckï¼Œä½¿ç”¨ç‰¹æ®Šçš„è™•ç†æ–¹å¼
                        content = await self.process_pitch_deck_page(page)
                        if content:
                            summary = await summarize_pitch_deck(content, message)
                            if summary:
                                results.append(summary)
                                continue
                    
                    # å¦‚æœä¸æ˜¯ Pitch Deck æˆ–è™•ç†å¤±æ•—ï¼Œä½¿ç”¨ä¸€èˆ¬ç¶²é è™•ç†æ–¹å¼
                    html = await page.content()
                    soup = BeautifulSoup(html, "html.parser")
                    
                    # æå–é—œéµä¿¡æ¯
                    extracted_data = {
                        "title": soup.title.string if soup.title else "",
                        "meta_description": "",
                        "main_content": [],
                        "company_info": {}
                    }
                    
                    # æå– meta description
                    meta_desc = soup.find('meta', attrs={'name': 'description'})
                    if meta_desc:
                        extracted_data["meta_description"] = meta_desc.get('content', '')
                    
                    # æå–ä¸»è¦å…§å®¹
                    main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=lambda x: x and ('content' in x.lower() or 'main' in x.lower()))
                    
                    if main_content:
                        for element in main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):
                            text = element.get_text(strip=True)
                            if text and len(text) > 10:
                                extracted_data["main_content"].append(text)
                    else:
                        for element in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):
                            text = element.get_text(strip=True)
                            if text and len(text) > 10:
                                extracted_data["main_content"].append(text)
                    
                    # æå–å…¬å¸ä¿¡æ¯
                    for meta in soup.find_all('meta'):
                        if meta.get('property') == 'og:site_name':
                            extracted_data["company_info"]["name"] = meta.get('content', '')
                    
                    footer = soup.find('footer')
                    if footer:
                        footer_text = footer.get_text(strip=True)
                        if footer_text:
                            extracted_data["company_info"]["footer"] = footer_text
                    
                    # å°‡æå–çš„å…§å®¹è½‰æ›ç‚ºæ–‡æœ¬
                    combined_text = f"""
                    Title: {extracted_data['title']}
                    Description: {extracted_data['meta_description']}
                    
                    Main Content:
                    {' '.join(extracted_data['main_content'])}
                    
                    Company Info:
                    {json.dumps(extracted_data['company_info'], indent=2)}
                    """
                    
                    # ä½¿ç”¨ GPT åˆ†æå…§å®¹
                    if combined_text.strip():
                        summary = await summarize_pitch_deck(combined_text, message)
                        if summary:
                            results.append(summary)
                            continue
                    
                    # å¦‚æœæ–‡å­—åˆ†æå¤±æ•—ï¼Œå˜—è©¦ OCR åœ–ç‰‡
                    self.logger.warning(f"âš ï¸ {url} æ–‡å­—åˆ†æå¤±æ•—ï¼Œå˜—è©¦ OCR åœ–ç‰‡")
                    img_tags = soup.find_all("img")
                    image_urls = [img.get("src") for img in img_tags if img.get("src")]
                    
                    if image_urls:
                        ocr_text = await ocr_images_from_urls(image_urls)
                        if ocr_text.strip():
                            summary = await summarize_pitch_deck(ocr_text, message)
                            if summary:
                                results.append(summary)
                                continue
                    
                    results.append({"url": url, "error": "âŒ ç„¡æ³•æå–æœ‰æ•ˆå…§å®¹"})
                    
                except Exception as e:
                    self.logger.error(f"âŒ åˆ†æ {url} å¤±æ•—ï¼š{e}")
                    results.append({"url": url, "error": f"âŒ åˆ†æå¤±æ•—: {e}"})
                finally:
                    await page.close()
            
            await context.close()
            await browser.close()
        
        return results

    async def process_pitch_deck_page(self, page) -> Optional[str]:
        """è™•ç† Pitch Deck é é¢"""
        try:
            self.logger.info("é–‹å§‹è™•ç† Pitch Deck é é¢")
            
            # ç­‰å¾…é é¢åŠ è¼‰å®Œæˆ
            await page.wait_for_load_state('networkidle', timeout=30000)
            
            # æˆªåœ–ä»¥ä¾¿èª¿è©¦
            debug_screenshot_name = f"pitch_deck_debug_{random.randint(1000, 9999)}.png"
            debug_screenshot_path = str(self.path_helper.get("tmp", debug_screenshot_name))
            self.path_helper.ensure_dir("tmp")
            await page.screenshot(path=debug_screenshot_path)
            self.logger.info(f"ä¿å­˜é é¢æˆªåœ–è‡³: {debug_screenshot_path}")
            
            # Debug å°‡æ•´é  HTML å„²å­˜ä¸‹ä¾†
            html = await page.content()
            debug_html_path = self.path_helper.get("debug_pitch_deck.html")
            with debug_html_path.open("w", encoding="utf-8") as f:
                f.write(html)
            
            # æå–é é¢æ¨™é¡Œ
            title = await page.title()
            self.logger.info(f"æå–çš„é é¢æ¨™é¡Œ: {title}")
            
            # 1. å˜—è©¦æå–æŠ•å½±ç‰‡å…§å®¹
            slides = []
            
            # ç­‰å¾…æŠ•å½±ç‰‡å®¹å™¨åŠ è¼‰
            try:
                # æ“´å±•é¸æ“‡å™¨ä»¥åŒ¹é…æ›´å¤šå¯èƒ½çš„æŠ•å½±ç‰‡å®¹å™¨
                await page.wait_for_selector('div[class*="slide"], div[class*="deck"], div[class*="presentation"], div[class*="page"], div[class*="slide-container"], div[class*="slide-wrapper"]', timeout=5000)
            except Exception as e:
                self.logger.warning(f"ç­‰å¾…æŠ•å½±ç‰‡å®¹å™¨è¶…æ™‚: {e}")
            
            # æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰æŠ•å½±ç‰‡
            self.logger.info("é–‹å§‹æ»¾å‹•é é¢ä»¥åŠ è¼‰æ‰€æœ‰æŠ•å½±ç‰‡")
            last_height = await page.evaluate('document.body.scrollHeight')
            while True:
                # æ»¾å‹•åˆ°åº•éƒ¨
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                # ç­‰å¾…æ–°å…§å®¹åŠ è¼‰
                await page.wait_for_timeout(2000)
                # è¨ˆç®—æ–°çš„æ»¾å‹•é«˜åº¦
                new_height = await page.evaluate('document.body.scrollHeight')
                if new_height == last_height:
                    break
                last_height = new_height
                self.logger.info("ç¹¼çºŒæ»¾å‹•ä»¥åŠ è¼‰æ›´å¤šæŠ•å½±ç‰‡")
            
            # æ»¾å‹•å›é ‚éƒ¨
            await page.evaluate('window.scrollTo(0, 0)')
            await page.wait_for_timeout(1000)
            
            # ç²å–æ‰€æœ‰æŠ•å½±ç‰‡
            slide_elements = await page.query_selector_all('div[class*="slide"], div[class*="deck"], div[class*="presentation"], div[class*="page"], div[class*="slide-container"], div[class*="slide-wrapper"]')
            self.logger.info(f"æ‰¾åˆ° {len(slide_elements)} å€‹æŠ•å½±ç‰‡å…ƒç´ ")
            
            # å¦‚æœæ‰¾ä¸åˆ°æŠ•å½±ç‰‡ï¼Œå˜—è©¦å…¶ä»–é¸æ“‡å™¨
            if not slide_elements:
                self.logger.info("å˜—è©¦ä½¿ç”¨å‚™ç”¨é¸æ“‡å™¨")
                # å˜—è©¦æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æŠ•å½±ç‰‡å®¹å™¨
                slide_elements = await page.query_selector_all('div[role="slide"], div[data-slide], div[data-page], div[data-index]')
                self.logger.info(f"ä½¿ç”¨å‚™ç”¨é¸æ“‡å™¨æ‰¾åˆ° {len(slide_elements)} å€‹æŠ•å½±ç‰‡å…ƒç´ ")
            
            # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œå˜—è©¦æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æŠ•å½±ç‰‡å…§å®¹çš„ div
            if not slide_elements:
                self.logger.info("å˜—è©¦æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æŠ•å½±ç‰‡å…§å®¹çš„ div")
                # ç²å–é é¢ä¸­çš„æ‰€æœ‰ div
                all_divs = await page.query_selector_all('div')
                # éæ¿¾å‡ºå¯èƒ½åŒ…å«æŠ•å½±ç‰‡å…§å®¹çš„ div
                slide_elements = []
                for div in all_divs:
                    try:
                        # æª¢æŸ¥ div æ˜¯å¦åŒ…å«æŠ•å½±ç‰‡ç›¸é—œçš„å…§å®¹
                        text = await div.text_content()
                        if text and len(text.strip()) > 50:  # å‡è¨­æŠ•å½±ç‰‡å…§å®¹é€šå¸¸è¼ƒé•·
                            slide_elements.append(div)
                    except:
                        continue
                self.logger.info(f"é€šéå…§å®¹åˆ†ææ‰¾åˆ° {len(slide_elements)} å€‹å¯èƒ½çš„æŠ•å½±ç‰‡")
            
            # å¦‚æœæ‰¾åˆ°çš„æŠ•å½±ç‰‡æ•¸é‡æ˜é¡¯å°‘æ–¼é æœŸï¼Œå˜—è©¦é»æ“Šä¸‹ä¸€é æŒ‰éˆ•
            if len(slide_elements) < 10:  # å‡è¨­è‡³å°‘æœ‰10é 
                self.logger.info("å˜—è©¦é€šéé»æ“Šä¸‹ä¸€é æŒ‰éˆ•åŠ è¼‰æ›´å¤šæŠ•å½±ç‰‡")
                try:
                    # å˜—è©¦é»æ“Šä¸‹ä¸€é æŒ‰éˆ•
                    next_buttons = await page.query_selector_all('button[aria-label*="next"], button[aria-label*="Next"], button[title*="next"], button[title*="Next"]')
                    for button in next_buttons:
                        for _ in range(10):  # æœ€å¤šé»æ“Š10æ¬¡
                            try:
                                await button.click()
                                await page.wait_for_timeout(1000)  # ç­‰å¾…æ–°é é¢åŠ è¼‰
                            except:
                                break
                except Exception as e:
                    self.logger.warning(f"é»æ“Šä¸‹ä¸€é æŒ‰éˆ•æ™‚å‡ºéŒ¯: {e}")
                
                # é‡æ–°ç²å–æŠ•å½±ç‰‡
                slide_elements = await page.query_selector_all('div[class*="slide"], div[class*="deck"], div[class*="presentation"], div[class*="page"], div[class*="slide-container"], div[class*="slide-wrapper"]')
                self.logger.info(f"é»æ“Šä¸‹ä¸€é å¾Œæ‰¾åˆ° {len(slide_elements)} å€‹æŠ•å½±ç‰‡å…ƒç´ ")
            
            for i, slide in enumerate(slide_elements):
                try:
                    # æå–æŠ•å½±ç‰‡æ–‡æœ¬
                    text = await slide.text_content()
                    if text.strip():
                        slides.append(f"[Slide {i+1}]\n{text.strip()}")
                        self.logger.info(f"æˆåŠŸæå–ç¬¬ {i+1} å¼µæŠ•å½±ç‰‡çš„æ–‡æœ¬")
                    
                    # æå–æŠ•å½±ç‰‡åœ–ç‰‡
                    images = await slide.query_selector_all('img')
                    self.logger.info(f"ç¬¬ {i+1} å¼µæŠ•å½±ç‰‡ä¸­æ‰¾åˆ° {len(images)} å¼µåœ–ç‰‡")
                    
                    for img in images:
                        src = await img.get_attribute('src')
                        if src:
                            try:
                                # ä¸‹è¼‰åœ–ç‰‡ä¸¦é€²è¡Œ OCR
                                response = requests.get(src)
                                img = Image.open(BytesIO(response.content))
                                ocr_text = pytesseract.image_to_string(img)
                                if ocr_text.strip():
                                    slides.append(f"[Slide {i+1} Image]\n{ocr_text.strip()}")
                                    self.logger.info(f"æˆåŠŸå°ç¬¬ {i+1} å¼µæŠ•å½±ç‰‡çš„åœ–ç‰‡é€²è¡Œ OCR")
                            except Exception as e:
                                self.logger.warning(f"OCR å¤±æ•—: {e}")
                except Exception as e:
                    self.logger.warning(f"è™•ç†ç¬¬ {i+1} å¼µæŠ•å½±ç‰‡æ™‚å‡ºéŒ¯: {e}")
            
            if slides:
                formatted_content = f"--- Pitch Deck: {title} ---\n\n" + "\n\n".join(slides) + "\n\n--- Pitch Deck çµæŸ ---"
                self.logger.info("æˆåŠŸæå–æŠ•å½±ç‰‡å…§å®¹")
                return formatted_content
            
            # 2. å¦‚æœç„¡æ³•æå–æŠ•å½±ç‰‡ï¼Œå˜—è©¦æå–æ•´å€‹é é¢å…§å®¹
            self.logger.info("ç„¡æ³•æå–æŠ•å½±ç‰‡å…§å®¹ï¼Œå˜—è©¦æå–æ•´å€‹é é¢")
            content = await page.content()
            soup = BeautifulSoup(content, "html.parser")
            
            # æå–æ‰€æœ‰æ–‡æœ¬å…§å®¹
            texts = []
            for element in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div']):
                text = element.get_text(strip=True)
                if text and len(text) > 10:
                    texts.append(text)
            
            if texts:
                formatted_content = f"--- Pitch Deck: {title} ---\n\n" + "\n\n".join(texts) + "\n\n--- Pitch Deck çµæŸ ---"
                self.logger.info("æˆåŠŸæå–é é¢æ–‡æœ¬å…§å®¹")
                return formatted_content
            
            # 3. å¦‚æœæ–‡å­—æå–å¤±æ•—ï¼Œå˜—è©¦ OCR æ‰€æœ‰åœ–ç‰‡
            self.logger.warning("æ–‡å­—æå–å¤±æ•—ï¼Œå˜—è©¦ OCR æ‰€æœ‰åœ–ç‰‡")
            img_tags = soup.find_all("img")
            image_urls = [img.get("src") for img in img_tags if img.get("src")]
            
            if image_urls:
                self.logger.info(f"æ‰¾åˆ° {len(image_urls)} å¼µåœ–ç‰‡ï¼Œé–‹å§‹ OCR")
                ocr_text = await ocr_images_from_urls(image_urls)
                if ocr_text.strip():
                    formatted_content = f"--- Pitch Deck: {title} ---\n\n{ocr_text}\n\n--- Pitch Deck çµæŸ ---"
                    self.logger.info("æˆåŠŸé€šé OCR æå–å…§å®¹")
                    return formatted_content
            
            self.logger.error("âŒ ç„¡æ³•æå–ä»»ä½•æœ‰æ•ˆå…§å®¹")
            return None
            
        except Exception as e:
            self.logger.error(f"è™•ç† Pitch Deck é é¢æ™‚å‡ºéŒ¯: {str(e)}", exc_info=True)
            return None

#GPT ç¸½çµ        
openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

async def ocr_images_from_urls(image_urls: List[str]) -> str:
    """ä¸‹è¼‰åœ–ç‰‡ä¸¦åŸ·è¡Œ OCR"""
    from pytesseract import pytesseract
    results = []

    for i, url in enumerate(image_urls):
        try:
            # è™•ç† base64 åœ–ç‰‡
            if url.startswith('data:image/'):
                try:
                    import base64
                    # å–å¾— base64 ç·¨ç¢¼éƒ¨åˆ†
                    header, encoded = url.split(",", 1)
                    img_data = base64.b64decode(encoded)
                    img = Image.open(BytesIO(img_data))
                except Exception as e:
                    logger.warning(f"âŒ ç„¡æ³•è§£æ base64 åœ–ç‰‡: {str(e)}")
                    continue
            # è™•ç†ä¸€èˆ¬ URL
            elif url.startswith("http"):
                response = requests.get(url)
                img = Image.open(BytesIO(response.content))
            else:
                logger.warning(f"âŒ ä¸æ”¯æ´çš„åœ–ç‰‡ URL æ ¼å¼: {url[:100]}...")
                continue

            # ä½¿ç”¨ UTF-8 ç·¨ç¢¼è™•ç†æ–‡å­—
            text = pytesseract.image_to_string(img, lang='eng')
            if text and text.strip():
                # ç¢ºä¿æ–‡å­—ä½¿ç”¨ UTF-8 ç·¨ç¢¼
                text_encoded = text.encode('utf-8', errors='ignore').decode('utf-8')
                results.append(f"[Slide {i+1}]\n{text_encoded}")
                
        except Exception as e:
            logger.warning(f"âŒ è®€å–åœ–ç‰‡å¤±æ•—: {str(e)}", exc_info=True)

    return "\n\n".join(results)

async def extract_company_name_from_message(message: str) -> Optional[str]:
    """å¾æ¶ˆæ¯ä¸­æå–å…¬å¸åç¨±"""
    try:
        # 1. æª¢æŸ¥æ˜¯å¦æœ‰æ˜ç¢ºçš„å…¬å¸åç¨±æ¨™è¨˜
        company_patterns = [
            r'(?:^|\n|\s)(?:company|project|startup):\s*([^\n]+)',
            r'(?:^|\n|\s)(?:company|project|startup)\s*name:\s*([^\n]+)',
            r'(?:^|\n|\s)(?:company|project|startup)\s*name\s*is\s*([^\n]+)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:a|an)\s*(?:company|startup|project)',
            r'(?:^|\n|\s)(?:about|introducing|presenting)\s+([A-Z][A-Za-z0-9\s&]+)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:also|working|building)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:governance|related)',
        ]
        
        for pattern in company_patterns:
            match = re.search(pattern, message, re.IGNORECASE | re.MULTILINE)
            if match:
                company_name = match.group(1).strip()
                # æ¸…ç†å…¬å¸åç¨±
                company_name = re.sub(r'\s+', ' ', company_name)  # ç§»é™¤å¤šé¤˜ç©ºæ ¼
                company_name = re.sub(r'[^\w\s&-]', '', company_name)  # åªä¿ç•™å­—æ¯ã€æ•¸å­—ã€ç©ºæ ¼ã€&å’Œ-
                # å¦‚æœå…¬å¸åç¨±å¤ªé•·ï¼Œå¯èƒ½ä¸æ˜¯æ­£ç¢ºçš„åŒ¹é…
                if len(company_name.split()) <= 3:  # å‡è¨­å…¬å¸åç¨±ä¸è¶…é3å€‹å–®è©
                    return company_name
        
        # 2. æª¢æŸ¥æ˜¯å¦æœ‰ "about" æˆ– "introducing" å¾Œé¢çš„åç¨±
        about_patterns = [
            r'(?:^|\n|\s)(?:about|introducing|presenting)\s+([A-Z][A-Za-z0-9\s&]+)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:a|an)\s*(?:company|startup|project)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:also|working|building)',
            r'(?:^|\n|\s)([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:governance|related)',
        ]
        
        for pattern in about_patterns:
            match = re.search(pattern, message, re.IGNORECASE | re.MULTILINE)
            if match:
                company_name = match.group(1).strip()
                company_name = re.sub(r'\s+', ' ', company_name)
                company_name = re.sub(r'[^\w\s&-]', '', company_name)
                if len(company_name.split()) <= 3:
                    return company_name
        
        # 3. æª¢æŸ¥æ˜¯å¦æœ‰ "Blurb" æˆ– "Description" å¾Œé¢çš„åç¨±
        blurb_patterns = [
            r'(?:^|\n|\s)(?:blurb|description):\s*([^\n]+)',
            r'(?:^|\n|\s)(?:blurb|description)\s*is\s*([^\n]+)',
        ]
        
        for pattern in blurb_patterns:
            match = re.search(pattern, message, re.IGNORECASE | re.MULTILINE)
            if match:
                text = match.group(1).strip()
                # å˜—è©¦å¾æè¿°ä¸­æå–å…¬å¸åç¨±
                company_match = re.search(r'([A-Z][A-Za-z0-9\s&]+)\s*(?:is|are)\s*(?:a|an)\s*(?:company|startup|project)', text)
                if company_match:
                    company_name = company_match.group(1).strip()
                    company_name = re.sub(r'\s+', ' ', company_name)
                    company_name = re.sub(r'[^\w\s&-]', '', company_name)
                    if len(company_name.split()) <= 3:
                        return company_name
        
        # 4. ç‰¹æ®Šæƒ…æ³ï¼šç›´æ¥æŸ¥æ‰¾å¤§å¯«é–‹é ­çš„å–®è©
        words = message.split()
        for i, word in enumerate(words):
            if word[0].isupper() and len(word) > 1 and word.isalpha():
                # æª¢æŸ¥é€™å€‹è©æ˜¯å¦å¯èƒ½æ˜¯å…¬å¸åç¨±
                if i > 0 and words[i-1].lower() in ['about', 'introducing', 'presenting', 'company', 'project', 'startup']:
                    return word
                if i < len(words)-1 and words[i+1].lower() in ['is', 'are', 'working', 'building']:
                    return word
        
        return None
    except Exception as e:
        logger.error(f"æå–å…¬å¸åç¨±æ™‚å‡ºéŒ¯: {e}")
        return None

async def summarize_pitch_deck(ocr_text: str, message: str = "") -> Dict:
    """ç”¨ GPT æ‘˜è¦ Pitch Deck OCR æ–‡å­—ç‚ºçµæ§‹åŒ–å¤§ç¶±ä¸¦å›å‚³ dict"""
    # ç¢ºä¿è¼¸å…¥æ–‡å­—æ˜¯ UTF-8 ç·¨ç¢¼
    if isinstance(ocr_text, str):
        ocr_text = ocr_text.encode('utf-8', errors='ignore').decode('utf-8')

    # é¦–å…ˆå˜—è©¦å¾æ¶ˆæ¯ä¸­æå–å…¬å¸åç¨±
    company_name = None
    if message:
        company_name = await extract_company_name_from_message(message)
        if company_name:
            logger.info(f"å¾æ¶ˆæ¯ä¸­æå–åˆ°å…¬å¸åç¨±: {company_name}")

    # ä½¿ç”¨è‹±æ–‡ prompt é¿å…ç·¨ç¢¼å•é¡Œ
    prompt = f"""
Based on the following Pitch Deck content, create a structured summary.
{f'Company name is already known to be: {company_name}' if company_name else ''}
Return in the following JSON format:
{{
  "company": "{company_name if company_name else 'company_name'}",
  "problem": "problem_statement",
  "solution": "solution_statement",
  "business_model": "how they do business",
  "financials": "financials_summary",
  "market": "what's the target market and it's description",
  "funding_team": "founding_team and their background",
}}

Pitch Deck Content:
{ocr_text}
"""

    try:
        completion = await openai_client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": "You are a professional analyst helping investors organize Pitch Deck information."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"}
        )
        raw_output = json.loads(completion.choices[0].message.content)
        
        # å¦‚æœå¾æ¶ˆæ¯ä¸­æå–åˆ°äº†å…¬å¸åç¨±ï¼Œç¢ºä¿ä½¿ç”¨é€™å€‹åç¨±
        if company_name:
            raw_output["company"] = company_name
            
        logger.info(f"æˆåŠŸæå–Deckä¿¡æ¯")       
        return raw_output
    except Exception as e:
        logger.error(f"GPT è™•ç†å¤±æ•—ï¼š{str(e)}")
        return {
            "error": f"åˆ†æå¤±æ•—: {str(e)}",
            "company": company_name if company_name else "æœªçŸ¥å…¬å¸",
            "summary": "åˆ†æå¤±æ•—"
        }
    

async def debug_all_iframes(page):
    """
    å„²å­˜æ‰€æœ‰ iframe çš„ HTML ä¸¦åˆ—å‡ºé—œéµè³‡è¨Šï¼ˆæ–‡å­—ã€åœ–ç‰‡ã€canvas æ•¸é‡ï¼‰ã€‚
    """
    frames = page.frames
    print(f"\nğŸ” æª¢æ¸¬åˆ° {len(frames)} å€‹ iframeã€‚")

    for idx, frame in enumerate(frames):
        print(f"\nğŸ“¦ è§£æ iframe [{idx}] URL: {frame.url}")

        try:
            # å„²å­˜ HTML
            html = await frame.content()
            from utils.path_helper import PathHelper as _PH
            _ph = _PH()
            file_path = _ph.get(f"debug_frame_{idx}.html")
            with file_path.open("w", encoding="utf-8") as f:
                f.write(html)
            print(f"ğŸ“ å·²å„²å­˜ HTML åˆ° {file_path}")

            # æŠ“ä¸»è¦å…ƒç´ æ•¸é‡
            text_count = await frame.eval_on_selector_all('p', 'els => els.length')
            div_count = await frame.eval_on_selector_all('div', 'els => els.length')
            img_count = await frame.eval_on_selector_all('img', 'els => els.length')
            canvas_count = await frame.eval_on_selector_all('canvas', 'els => els.length')

            print(f"ğŸ“Š å…§å®¹çµ±è¨ˆï¼š")
            print(f" - <p> æ¨™ç±¤ï¼š{text_count}")
            print(f" - <div> æ¨™ç±¤ï¼š{div_count}")
            print(f" - <img> åœ–ç‰‡ï¼š{img_count}")
            print(f" - <canvas> å…ƒç´ ï¼š{canvas_count}")

            # é¡¯ç¤ºå¯èƒ½æ€§åˆ†æ
            if canvas_count > 0:
                print("âš ï¸ ç™¼ç¾ canvasï¼Œå¯èƒ½æ˜¯åœ–åƒæ¸²æŸ“çš„ slideã€‚")
            if text_count > 5 or div_count > 20:
                print("âœ… å¯èƒ½æœ‰æ–‡å­—å…§å®¹ã€‚")
            if img_count > 5 and text_count < 2:
                print("ğŸ–¼ï¸ å¾ˆå¯èƒ½æ˜¯ç´”åœ–ç‰‡å‹æŠ•å½±ç‰‡ã€‚")

        except Exception as e:
            print(f"âŒ è®€å– iframe {idx} ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
    

#æ¸¬è©¦ç”¨é©…å‹•å™¨
if __name__ == "__main__":

    async def main():
        message = """
        Butter is also governance related project and working with Uniswap and Optimism foundation for bringing Futarchy (prediction market style govnernance framework) on web3
        If you're interested in Butter feel free to lmk. Happy to connect with you and the team :
        - Blurb: Butter is a governance project which is building Conditional Funding Markets with Uniswap and Optimism foundation.
        - X post: https://x.com/butterygg/status/1910444872903123210
        """
        
        reader = DeckBrowser()
        try:
            await reader.initialize()
            results = await reader.process_input(message)
            if results:
                print(json.dumps(results[0], ensure_ascii=False, indent=2))
            else:
                print("âš ï¸ æ²’æœ‰æ“·å–åˆ°ä»»ä½•çµæœ")
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        finally:
            await reader.close()
            # ç¢ºä¿æ‰€æœ‰è³‡æºéƒ½è¢«é‡‹æ”¾
            import gc
            gc.collect()

    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("Bot stopped by user.")
    except Exception as e:
        print(f"Fatal error: {e}")
    finally:
        # ç¢ºä¿æ‰€æœ‰è³‡æºéƒ½è¢«é‡‹æ”¾
        import gc
        gc.collect()
