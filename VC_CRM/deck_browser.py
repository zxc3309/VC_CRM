import os
import logging
from dotenv import load_dotenv
import re
import asyncio
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page
from typing import Optional, List, Dict, Literal, Any
import random
from PIL import Image
import pytesseract
import requests
from io import BytesIO
from openai import AsyncOpenAI
import json

# Load environment variables
load_dotenv()

# Pytesseract Path
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# é…ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)


class DeckBrowser:
    """DocSend æ–‡æª”è®€å–å™¨"""
    
    def __init__(self):
        """Initialize the DeckBrowser."""
        self.browser = None
        self.logger = logging.getLogger(__name__)
        self.email = os.getenv("DOCSEND_EMAIL")  # æ›¿æ›ç‚ºæ‚¨çš„é›»å­éƒµä»¶

    #æ±ºå®šæµç¨‹
    SourceType = Literal["docsend", "attachment", "gdrive", "unknown"]
    
    def detect_source_type(message: str, attachments: Optional[list] = None) -> SourceType:
        """åµæ¸¬è¨Šæ¯ä¾†æºå‹æ…‹ï¼ˆdocsend / é™„ä»¶ / gdriveï¼‰"""
        if "docsend.com" in message.lower():
            return "docsend"
        elif attachments and any(f.lower().endswith(('.pdf', '.pptx', '.ppt')) for f in attachments):
            return "attachment"
        elif re.search(r"https://drive\.google\.com/\S+", message):
            return "gdrive"
        else:
            return "unknown"
    
    #æ±ºå®šæŒ‡å®šåˆ†æé …ç›®    
    async def process_input(self, message: str, attachments: Optional[list] = None):
        source_type = self.detect_source_type(message, attachments)
        
        if source_type == "docsend":
            return await self.run_docsend_analysis(message)
        elif source_type == "attachment":
            return await run_file_analysis(attachments)
        elif source_type == "gdrive":
            return await run_gdrive_analysis(message)
        else:
            raise ValueError("âš ï¸ ç„¡æ³•åˆ¤æ–·è¼¸å…¥è³‡æ–™ä¾†æºã€‚è«‹ç¢ºèªæ˜¯å¦åŒ…å«åˆæ³•é€£çµæˆ–é™„ä»¶ã€‚")
    
    #ä¸»è¦ Docsend é©…å‹•å¼
    async def run_docsend_analysis(self, message: str) -> Dict[str, Any]:
        """
        å°è£å®Œæ•´æµç¨‹ï¼šåˆå§‹åŒ– -> æ“·å– URL -> æŠ½å–å…§å®¹ -> é—œé–‰ç€è¦½å™¨ -> å›å‚³çµæœ
        """
        await self.initialize()
        
        results = []  # List to store JSON results
        urls = await self.extract_docsend_links(message)
        for url in urls:
            content = await self.read_docsend_document(url)
            if isinstance(content, dict):
                results.append(content)
            elif isinstance(content, str):
                summarized = await summarize_pitch_deck(content)
                if summarized:
                    results.append(summarized)

        await self.close()
        
        return results if results else [{"error": "âŒ æ²’æœ‰æˆåŠŸæ“·å–ä»»ä½• DocSend æ–‡æª”å…§å®¹"}]

    async def initialize(self):
        """Initialize the browser instance asynchronously."""
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(headless=False)
            self.logger.info("Browser initialized successfully")
        except Exception as e:
            self.logger.error(f"Failed to initialize browser: {str(e)}")
            raise

    async def close(self):
        """é—œé–‰ç€è¦½å™¨"""
        if self.browser:
            await self.browser.close()
            self.browser = None
        if hasattr(self, 'playwright'):
            await self.playwright.stop()
    
    async def extract_docsend_links(self, text: str) -> List[str]:
        """å¾æ–‡æœ¬ä¸­æå– DocSend é€£çµ"""
        docsend_pattern = r'https?://(?:www\.)?docsend\.com/[^\s)"}]+'
        return re.findall(docsend_pattern, text)
    
    async def _get_page(self, url: str) -> Page:
        """Get a new page with configured User-Agent and other settings."""
        page = await self.browser.new_page()
        await page.set_viewport_size({'width': 1920, 'height': 1080})
        await page.set_extra_http_headers({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'sec-ch-ua': '"Chromium";v="122", "Not(A:Brand";v="24", "Google Chrome";v="122"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"macOS"'
        })
        
        return page

    async def read_docsend_document(self, url: str) -> Optional[str]:
        """è®€å– DocSend æ–‡æª”çš„å…§å®¹"""
        try:
            self.logger.info(f"æ­£åœ¨å¾ DocSend é€£çµè®€å–å…§å®¹: {url}")
            
            # å‰µå»ºæ–°é é¢
            page = await self._get_page(url)
            
            # è¨ªå• DocSend é é¢
            response = await page.goto(url, wait_until='networkidle', timeout=30000)
            
            self.logger.info(f"è¨ªå•ç‹€æ…‹ç¢¼: {response.status}")
            
            if response.status == 403:
                self.logger.error("è¨ªå•è¢«æ‹’çµ• (403 Forbidden)")
                await page.close()
                return None
            
            # éš¨æ©Ÿç­‰å¾… 2-5 ç§’
            await asyncio.sleep(random.uniform(2, 5))
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦å¡«å¯«é›»å­éƒµä»¶
            email_input = await page.query_selector('input[type="email"]')
            if email_input:
                self.logger.info("éœ€è¦å¡«å¯«é›»å­éƒµä»¶æ‰èƒ½è¨ªå•æ–‡æª”")
                await page.type('input[type="email"]', self.email, delay=random.uniform(100, 200))
                try:
                    self.logger.info("å˜—è©¦é»æ“Šæäº¤æŒ‰éˆ•")
                    await page.locator('button:has-text("Continue")').wait_for(state='visible', timeout=1000)
                    await page.locator('button:has-text("Continue")').click(timeout=1000)
                except Exception as e:
                    self.logger.warning(f"æäº¤æŒ‰éˆ•é»æ“Šå¤±æ•—: {e}")
                await page.wait_for_load_state('networkidle', timeout=30000)
                
            # å˜—è©¦åˆ¤æ–·æ˜¯å¦æˆåŠŸç™»å…¥ DocSend æ–‡ä»¶ï¼ˆåŠ å…¥æ›´å¤š selector fallbackï¼‰
            success_selectors = [
                '.document-content',
                '.viewer-content',
                '.page',
                '.ds-viewer-container',
            ]

            successfully_entered = False
            for selector in success_selectors:
                try:
                    if await page.query_selector(selector):
                        self.logger.info(f"âœ… æˆåŠŸé€²å…¥ DocSend æ–‡ä»¶ï¼Œåµæ¸¬åˆ° selector: {selector}")
                        successfully_entered = True
                        break
                except Exception as e:
                    self.logger.warning(f"âŒ æª¢æŸ¥ selector {selector} ç™¼ç”ŸéŒ¯èª¤: {e}")

            if not successfully_entered:
                self.logger.warning("âš ï¸ æ²’æœ‰åµæ¸¬åˆ°ä»»ä½• DocSend æ–‡ä»¶å…§å®¹çš„ selectorï¼Œå¯èƒ½ç™»å…¥å¤±æ•—ã€‚")

            # å†æ¬¡éš¨æ©Ÿç­‰å¾…ï¼ˆè®“ç¶²é æœ‰æ©Ÿæœƒç¹¼çºŒè¼‰å…¥ï¼‰
            await asyncio.sleep(random.uniform(2, 5))
            
            # æˆªåœ–ä»¥ä¾¿èª¿è©¦
            debug_screenshot = f"/tmp/docsend_debug_{random.randint(1000, 9999)}.png"
            await page.screenshot(path=debug_screenshot)
            self.logger.info(f"ä¿å­˜é é¢æˆªåœ–è‡³: {debug_screenshot}")
            
            # Debug å°‡æ•´é  HTML å„²å­˜ä¸‹ä¾†
            html = await page.content()
            with open("debug_docsend.html", "w", encoding="utf-8") as f:
                f.write(html)

            # Debug é¡¯ç¤ºç›®å‰é é¢ä¸Šçš„æ‰€æœ‰ iframeï¼ˆè‹¥æœ‰ï¼‰
            for frame in page.frames:
                print(f"[iframe] name: {frame.name}, url: {frame.url}")
            
            # ç­‰å¾…æ–‡æª”å…§å®¹åŠ è¼‰
            # å˜—è©¦å¾æ‰€æœ‰ iframe ä¸­æ‰¾åˆ°å…§å®¹
            target_frame = None
            content_selectors = ['.document-content', '.page', '.viewer-content', '.ds-viewer-container']

            for frame in page.frames:
                frame_url = frame.url or ""
                # æ ¹æ“š URL åˆ¤æ–·æ˜¯ä¸» DocSend iframeï¼Œè€Œä¸æ˜¯ dropbox æˆ– intercom ç­‰é›œè¨Š
                if "docsend.com/view" in frame_url and "marketing.docsend.com" not in frame_url:
                    for selector in content_selectors:
                        try:
                            await frame.wait_for_selector(selector, timeout=5000)
                            self.logger.info(f"åœ¨ iframe ä¸­æ‰¾åˆ°å…§å®¹: {selector}")
                            target_frame = frame
                            break
                        except:
                            continue
                if target_frame:
                    break

                if not target_frame:
                    self.logger.warning("âš ï¸ æœªèƒ½æ‰¾åˆ°å«æ–‡å­— selectorï¼Œé–‹å§‹ debug æ‰€æœ‰ iframe...")
                    await debug_all_iframes(page)

                    # å˜—è©¦å¾æ‰€æœ‰ iframe ä¸­æ“·å–åœ–ç‰‡åš OCR
                    for frame in page.frames:
                        try:
                            frame_html = await frame.content()
                            soup = BeautifulSoup(frame_html, "html.parser")
                            img_tags = soup.find_all("img")
                            image_urls = [img["src"] for img in img_tags if img.get("src")]
                            
                            if image_urls:
                                self.logger.info(f"âœ… åœ¨ iframe ä¸­æ‰¾åˆ°åœ–ç‰‡ï¼šå…± {len(image_urls)} å¼µï¼Œæº–å‚™åŸ·è¡Œ OCR")
                                ocr_raw_text = await ocr_images_from_urls(image_urls)
                                if ocr_raw_text:
                                    summarized_text = await summarize_pitch_deck(ocr_raw_text)
                                    await page.close()
                                    return summarized_text
                        except Exception as e:
                            self.logger.warning(f"è®€å– iframe å…§å®¹å¤±æ•—ï¼š{e}")

                    self.logger.error("âŒ æ‰€æœ‰ iframe å‡æœªèƒ½æä¾›å…§å®¹æˆ–åœ–ç‰‡")
                    await page.close()
                    return None
                
            # å¾ iframe æŠ“ HTML
            html = await target_frame.content()
            self.logger.info(f"ç²å–çš„HTMLå…§å®¹é•·åº¦: {len(html)} å­—ç¬¦")
            
            # è§£æ HTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # æå–æ–‡æª”æ¨™é¡Œ
            title_elem = soup.find('title')

            # fallbackï¼šå¦‚æœ iframe æ²’æœ‰ titleï¼Œå˜—è©¦å¾ä¸»é  page æ‹¿
            if not title_elem:
                self.logger.info("iframe ä¸­æœªæ‰¾åˆ° <title>ï¼Œå˜—è©¦å¾ä¸»é æŠ“å– title")
                main_html = await page.content()
                main_soup = BeautifulSoup(main_html, 'html.parser')
                title_elem = main_soup.find('title')

            # æœ€çµ‚å–å¾— title
            title = title_elem.text.strip() if title_elem else "DocSend Document"
            self.logger.info(f"æå–çš„æ–‡æª”æ¨™é¡Œ: {title}")
            
            # æå–æ–‡æª”å…§å®¹
            text_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div'])
            self.logger.info(f"æ‰¾åˆ° {len(text_elements)} å€‹æ–‡æœ¬å…ƒç´ ")
            
            extracted_text = []
            
            for elem in text_elements:
                text = elem.get_text().strip()
                if text and len(text) > 10:  # åªä¿ç•™æœ‰æ„ç¾©çš„æ–‡æœ¬
                    extracted_text.append(text)
            
            self.logger.info(f"ç¯©é¸å¾Œæå–äº† {len(extracted_text)} å€‹æœ‰æ„ç¾©çš„æ–‡æœ¬æ®µè½")
            extracted_text = "\n\n".join(extracted_text)
            
            # é—œé–‰é é¢
            await page.close()
            
            if not extracted_text or len(extracted_text) < 100:
                self.logger.warning("âš ï¸ æ–‡å­—å…§å®¹éå°‘ï¼Œå˜—è©¦ OCR åœ–ç‰‡ä¸¦ç”¨ GPT æ‘˜è¦")
                img_tags = soup.find_all('img')
                image_urls = [img['src'] for img in img_tags if img.get('src')]

                if image_urls:
                    ocr_raw_text = await ocr_images_from_urls(image_urls)
                    if ocr_raw_text:
                        summarized_text = await summarize_pitch_deck(ocr_raw_text)
                        extracted_text = summarized_text
            
            # è¨˜éŒ„å…§å®¹æ‘˜è¦
            content_preview = extracted_text[:100] + "..." if len(extracted_text) > 100 else extracted_text
            self.logger.info(f"æå–æ–‡æœ¬é è¦½: {content_preview}")
            self.logger.info(f"ç¸½æå–æ–‡æœ¬é•·åº¦: {len(extracted_text)} å­—ç¬¦")
            
            # æ ¼å¼åŒ–æå–çš„å…§å®¹
            formatted_content = f"--- DocSend æ–‡æª”: {title} ---\n\n{extracted_text}\n\n--- DocSend æ–‡æª”çµæŸ ---"
            
            self.logger.info(f"æˆåŠŸå¾ DocSend é€£çµæå–å…§å®¹: {url}")
            return formatted_content
        
        except Exception as e:
            self.logger.error(f"è®€å– DocSend æ–‡æª”æ™‚å‡ºéŒ¯: {str(e)}", exc_info=True)
            if 'page' in locals():
                await page.close()
            return None

        

#GPT ç¸½çµ        
openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

async def ocr_images_from_urls(image_urls: List[str]) -> str:
    """ä¸‹è¼‰åœ–ç‰‡ä¸¦åŸ·è¡Œ OCR"""
    from pytesseract import pytesseract

    results = []

    for i, url in enumerate(image_urls):
        try:
            # åŸºæœ¬é©—è­‰ URL æ ¼å¼
            if not url.startswith("http"):
                logger.warning(f"âŒ ç„¡æ•ˆçš„åœ–ç‰‡ URL: {url}")
                continue

            response = requests.get(url)
            img = Image.open(BytesIO(response.content))
            text = pytesseract.image_to_string(img)
            if text.strip():
                results.append(f"[Slide {i+1}]\n{text.strip()}")
        except Exception as e:
            logger.warning(f"âŒ è®€å–åœ–ç‰‡ {url} å¤±æ•—: {e}")

    return "\n\n".join(results)

async def summarize_pitch_deck(ocr_text: str) -> Dict:
    """ç”¨ GPT æ‘˜è¦ Pitch Deck OCR æ–‡å­—ç‚ºçµæ§‹åŒ–å¤§ç¶±ä¸¦å›å‚³ dict"""
    prompt = f"""
è«‹æ ¹æ“šä»¥ä¸‹ Pitch Deck OCR å…§å®¹ï¼Œæ•´ç†æˆä¸€ä»½çµæ§‹åŒ–çš„å¤§ç¶±æ‘˜è¦ã€‚
è¿”å›ä»¥ä¸‹ JSON æ ¼å¼:
{{
  "company": "company_name",
  "problem": "problem_statement",
  "solution": "solution_statement",
  "business_model": "how they do business",
  "financials": "financials_summary",
  "market": "what's the target market and it's description",
  "funding_team": "founding_team and their background",
}}
è¦å‰‡ï¼š


Pitch Deck OCR:
{ocr_text}
"""

    try:
        completion = await openai_client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å¹«åŠ©æŠ•è³‡äººæ•´ç† Pitch Deck çš„å°ˆæ¥­åˆ†æå¸«"},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"}
        )
        raw_output = json.loads(completion.choices[0].message.content)
        logger.info(f"æˆåŠŸæå–Deckä¿¡æ¯")       
        return raw_output
    except json.JSONDecodeError as e:
        logger.error(f"GPT å›å‚³é JSON æ ¼å¼ï¼š{e}")
        return None  # Do not return raw_text to avoid adding it to the output
    

async def debug_all_iframes(page):
    """
    å„²å­˜æ‰€æœ‰ iframe çš„ HTML ä¸¦åˆ—å‡ºé—œéµè³‡è¨Šï¼ˆæ–‡å­—ã€åœ–ç‰‡ã€canvas æ•¸é‡ï¼‰ã€‚
    """
    frames = page.frames
    print(f"\nğŸ” æª¢æ¸¬åˆ° {len(frames)} å€‹ iframeã€‚")

    for idx, frame in enumerate(frames):
        print(f"\nğŸ“¦ è§£æ iframe [{idx}] URL: {frame.url}")

        try:
            # å„²å­˜ HTML
            html = await frame.content()
            file_path = f"debug_frame_{idx}.html"
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(html)
            print(f"ğŸ“ å·²å„²å­˜ HTML åˆ° {file_path}")

            # æŠ“ä¸»è¦å…ƒç´ æ•¸é‡
            text_count = await frame.eval_on_selector_all('p', 'els => els.length')
            div_count = await frame.eval_on_selector_all('div', 'els => els.length')
            img_count = await frame.eval_on_selector_all('img', 'els => els.length')
            canvas_count = await frame.eval_on_selector_all('canvas', 'els => els.length')

            print(f"ğŸ“Š å…§å®¹çµ±è¨ˆï¼š")
            print(f" - <p> æ¨™ç±¤ï¼š{text_count}")
            print(f" - <div> æ¨™ç±¤ï¼š{div_count}")
            print(f" - <img> åœ–ç‰‡ï¼š{img_count}")
            print(f" - <canvas> å…ƒç´ ï¼š{canvas_count}")

            # é¡¯ç¤ºå¯èƒ½æ€§åˆ†æ
            if canvas_count > 0:
                print("âš ï¸ ç™¼ç¾ canvasï¼Œå¯èƒ½æ˜¯åœ–åƒæ¸²æŸ“çš„ slideã€‚")
            if text_count > 5 or div_count > 20:
                print("âœ… å¯èƒ½æœ‰æ–‡å­—å…§å®¹ã€‚")
            if img_count > 5 and text_count < 2:
                print("ğŸ–¼ï¸ å¾ˆå¯èƒ½æ˜¯ç´”åœ–ç‰‡å‹æŠ•å½±ç‰‡ã€‚")

        except Exception as e:
            print(f"âŒ è®€å– iframe {idx} ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
    

#æ¸¬è©¦ç”¨é©…å‹•å™¨
if __name__ == "__main__":
    import sys

    async def main():
        message = """
        Global Sovereign Exchange (GSX): The First Compliant On/Off Ramp for India's Crypto Boom

        Pitch Deck: https://docsend.com/view/dpr7kr5uayvxekst/d/wbyrdmea55xp2ryk

        Indiaâ€™s crypto market has 500M+ of potential users, but no compliant on/off ramp. Exchanges rely on an unregulated P2P system riddled with chargebacks and frozen accounts.

        GSX is fixing this. GSX is Indiaâ€™s first FIU-registered fiat-to-crypto on/off ramp, with a live digital wallet, cross-chain DEX aggregator, and 500+ active P2P merchants moving $500K+ daily.

        Partnerships
        -Top 3 global exchange (100M+ users) â€“ Integrating GSX for fiat-to-crypto on-ramping in India.
        -Circle â€“ Expanding stablecoin adoption in India & high-inflation regions.
        -Near Protocol â€“ Partnering on India ecosystem growth.
        -Major Indian financial group â€“ Collaborating on crypto lending & insurance.

        Team
        CEO Alex Tai
        - Former Senior Director at Virgin Group for 20 years
        - One of four executives to report directly to Richard Branson
        - He Co-Founded Virgin Galactic and led as COO
        - A fighter jet pilot and holds 11 world records
        - Founded a Blockchain consultancy that worked with BCG to bring blockchain to Fortune500 companies

        Raising Now: $1.5M (SAFE + Token Warrant)
        """
        
        reader = DeckBrowser()
        await reader.initialize()
        for url in await reader.extract_docsend_links(message):
            content = await reader.read_docsend_document(url)
        await reader.close()
        print(content)

    asyncio.run(main())
